{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FeraMaks/test/blob/main/NN_ShaftTraectory_2_6_cnn_withSigm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGwCWPzQ4PLb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "from numpy.core.memmap import uint8\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY4YjbjkzSO1"
      },
      "outputs": [],
      "source": [
        "#device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y-xHoV-4gUq",
        "outputId": "ed943893-bcd7-4902-8d69-c1d63dcd72d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwSUjvtElsBR"
      },
      "source": [
        "##Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAjFXQpKhmGO"
      },
      "source": [
        "Эллиптические графики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF6Vtt8afTY1",
        "outputId": "5491e02f-4511-4c2f-c7cc-2c5fcfc06756"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Советую воспользоваться циклом for, чтобы не писать множество однотипных строк кода\n",
        "\n",
        "data1 = pd.read_excel(open('/content/drive/MyDrive/work/0/001_19-12-03_1728_009-50_000.xlsx', 'rb'))\n",
        "data1 = pd.DataFrame(data1)\n",
        "data1 = pd.DataFrame.to_numpy(data1)\n",
        "data1 = np.rot90(data1)\n",
        "data1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L52sW0YSfRF",
        "outputId": "d0a9012b-ef3d-4dd1-f8b4-c935c101c259"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data2 = pd.read_excel(open('/content/drive/MyDrive/work/0/001_19-12-05_1704_020_000.xlsx', 'rb'))\n",
        "data2 = pd.DataFrame(data2)\n",
        "data2 = pd.DataFrame.to_numpy(data2)\n",
        "data2 = np.rot90(data2)\n",
        "data2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_yRhrbSS6E1",
        "outputId": "6f0d8c23-ebbd-48ce-a048-f1470c7ce869"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data3 = pd.read_excel(open('/content/drive/MyDrive/work/0/001_19-12-05_1704_021_000.xlsx', 'rb'))\n",
        "data3 = pd.DataFrame(data3)\n",
        "data3 = pd.DataFrame.to_numpy(data3)\n",
        "data3 = np.rot90(data3)\n",
        "data3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q-ams-_S6nc",
        "outputId": "72b431a9-4dc9-424e-877d-6d11405c38e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data4 = pd.read_excel(open('/content/drive/MyDrive/work/0/001_19-12-05_1744_017-330_000.xlsx', 'rb'))\n",
        "data4 = pd.DataFrame(data4)\n",
        "data4 = pd.DataFrame.to_numpy(data4)\n",
        "data4 = np.rot90(data4)\n",
        "data4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWGTp6oqh4Qu",
        "outputId": "402d4d2d-3da1-423e-f10f-5b8f82295f72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data5 = pd.read_excel(open('/content/drive/MyDrive/work/0/001_19-12-05_1746_021_000.xlsx', 'rb'))\n",
        "data5 = pd.DataFrame(data5)\n",
        "data5 = pd.DataFrame.to_numpy(data5)\n",
        "data5 = np.rot90(data5)\n",
        "data5.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H5FHn6T6j7M",
        "outputId": "a0e14deb-f16c-4d88-ece0-ec427e1a3430"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data6 = pd.read_excel(open('/content/drive/MyDrive/work/0/001_19-12-05_1748_027-360_000.xlsx', 'rb'))\n",
        "data6 = pd.DataFrame(data6)\n",
        "data6 = pd.DataFrame.to_numpy(data6)\n",
        "data6 = np.rot90(data6)\n",
        "data6.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IjuHB2l6riE",
        "outputId": "498cd363-5317-435b-bc86-92bc5c5c5d04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data7 = pd.read_excel(open('/content/drive/MyDrive/work/0/001_19-12-05_1801_062-480_000.xlsx', 'rb'))\n",
        "data7 = pd.DataFrame(data7)\n",
        "data7 = pd.DataFrame.to_numpy(data7)\n",
        "data7 = np.rot90(data7)\n",
        "data7.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noFCqRg_6r_G"
      },
      "outputs": [],
      "source": [
        "#data8 = pd.read_excel(open('/content/drive/MyDrive/work/0/100_000.xlsx', 'rb'))\n",
        "#data8 = pd.DataFrame(data8)\n",
        "#data8 = pd.DataFrame.to_numpy(data8)\n",
        "#data8 = np.rot90(data8)\n",
        "#data8.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok2gYcpTR2Kw"
      },
      "outputs": [],
      "source": [
        "#data9 = pd.read_excel(open('/content/drive/MyDrive/work/0/110_000.xlsx', 'rb'))\n",
        "#data9 = pd.DataFrame(data9)\n",
        "#data9 = pd.DataFrame.to_numpy(data9)\n",
        "#data9 = np.rot90(data9)\n",
        "#data9.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPMqcRGXR13w"
      },
      "outputs": [],
      "source": [
        "#data10 = pd.read_excel(open('/content/drive/MyDrive/work/0/120_000.xlsx', 'rb'))\n",
        "#data10 = pd.DataFrame(data10)\n",
        "#data10 = pd.DataFrame.to_numpy(data10)\n",
        "#data10 = np.rot90(data10)\n",
        "#data10.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbx4WrcjiCah"
      },
      "source": [
        "Метастабильные графики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mClCPuEIiFif",
        "outputId": "6879b9f7-d157-4207-f614-eb9381c6bd40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "data11 = pd.read_excel(open('/content/drive/MyDrive/work/2/001_19-12-05_1625_001_222.xlsx', 'rb'))\n",
        "data11 = pd.DataFrame(data11)\n",
        "data11 = pd.DataFrame.to_numpy(data11)\n",
        "data11 = np.rot90(data11)\n",
        "data11.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAAJt6GkixD1",
        "outputId": "76ba3247-1ceb-4cdb-881f-52b8bcb4f28e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "data12 = pd.read_excel(open('/content/drive/MyDrive/work/2/001_19-12-05_1625_002-10_222.xlsx', 'rb'))\n",
        "data12 = pd.DataFrame(data12)\n",
        "data12 = pd.DataFrame.to_numpy(data12)\n",
        "data12 = np.rot90(data12)\n",
        "data12.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G20rPhrNiw92",
        "outputId": "40bff8ad-e2d2-4cc5-d665-bb445986ce5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "data13 = pd.read_excel(open('/content/drive/MyDrive/work/2/001_19-12-05_1625_003_222.xlsx', 'rb'))\n",
        "data13 = pd.DataFrame(data13)\n",
        "data13 = pd.DataFrame.to_numpy(data13)\n",
        "data13 = np.rot90(data13)\n",
        "data13.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydrgHeI0iw1e",
        "outputId": "d10e48a2-4e28-43e7-cc69-ec6149b1d4de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "data14 = pd.read_excel(open('/content/drive/MyDrive/work/2/001_19-12-05_1737_016_222.xlsx', 'rb'))\n",
        "data14 = pd.DataFrame(data14)\n",
        "data14 = pd.DataFrame.to_numpy(data14)\n",
        "data14 = np.rot90(data14)\n",
        "data14.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy0M0CEqiwVG",
        "outputId": "1df54e88-6f06-4184-88fe-c0dbf97f4b5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "data15 = pd.read_excel(open('/content/drive/MyDrive/work/2/001_19-12-05_1739_005-290_222.xlsx', 'rb'))\n",
        "data15 = pd.DataFrame(data15)\n",
        "data15 = pd.DataFrame.to_numpy(data15)\n",
        "data15 = np.rot90(data15)\n",
        "data15.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vGXDVlf77Cr",
        "outputId": "0b575759-09b8-4e30-ceba-85ca4f2d39ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "data16 = pd.read_excel(open('/content/drive/MyDrive/work/2/001_19-12-05_1739_006_222.xlsx', 'rb'))\n",
        "data16 = pd.DataFrame(data16)\n",
        "data16 = pd.DataFrame.to_numpy(data16)\n",
        "data16 = np.rot90(data16)\n",
        "data16.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C1BldYj767b",
        "outputId": "436b0864-e673-4b35-d83b-90b47c92b7b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data17 = pd.read_excel(open('/content/drive/MyDrive/work/2/21_222.xlsx', 'rb'))\n",
        "data17 = pd.DataFrame(data17)\n",
        "data17 = pd.DataFrame.to_numpy(data17)\n",
        "data17 = np.rot90(data17)\n",
        "data17.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsamCy_H760b"
      },
      "outputs": [],
      "source": [
        "#data18 = pd.read_excel(open('/content/drive/MyDrive/work/2/22_222.xlsx', 'rb'))\n",
        "#data18 = pd.DataFrame(data18)\n",
        "#data18 = pd.DataFrame.to_numpy(data18)\n",
        "#data18 = np.rot90(data18)\n",
        "#data18.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF1Ked0dSGV5"
      },
      "outputs": [],
      "source": [
        "#data19 = pd.read_excel(open('/content/drive/MyDrive/work/2/23_222.xlsx', 'rb'))\n",
        "#data19 = pd.DataFrame(data19)\n",
        "#data19 = pd.DataFrame.to_numpy(data19)\n",
        "#data19 = np.rot90(data19)\n",
        "#data19.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL7pjf0pSHyp"
      },
      "outputs": [],
      "source": [
        "#data20 = pd.read_excel(open('/content/drive/MyDrive/work/2/260_222.xlsx', 'rb'))\n",
        "#data20 = pd.DataFrame(data20)\n",
        "#data20 = pd.DataFrame.to_numpy(data20)\n",
        "#data20 = np.rot90(data20)\n",
        "#data20.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Графики с самопересечением"
      ],
      "metadata": {
        "id": "NFHnaNADsAHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data31 = pd.read_excel(open('/content/drive/MyDrive/work/1/11_111.xlsx', 'rb'))\n",
        "data31 = pd.DataFrame(data31)\n",
        "data31 = pd.DataFrame.to_numpy(data31)\n",
        "data31 = np.rot90(data31)\n",
        "data31.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN4iXRnAr-JC",
        "outputId": "fefdb013-77e4-41ca-8cfe-cd12cb2f62c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data32 = pd.read_excel(open('/content/drive/MyDrive/work/1/12_111.xlsx', 'rb'))\n",
        "data32 = pd.DataFrame(data32)\n",
        "data32 = pd.DataFrame.to_numpy(data32)\n",
        "data32 = np.rot90(data32)\n",
        "data32.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdd-j2tosD9A",
        "outputId": "16ce60e9-8b34-41e4-89a0-da7027013606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data33 = pd.read_excel(open('/content/drive/MyDrive/work/1/13_111.xlsx', 'rb'))\n",
        "data33 = pd.DataFrame(data33)\n",
        "data33 = pd.DataFrame.to_numpy(data33)\n",
        "data33 = np.rot90(data33)\n",
        "data33.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg0e_f9usEfK",
        "outputId": "b95d1e9e-a32f-490a-99dd-38cb97b48e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data34 = pd.read_excel(open('/content/drive/MyDrive/work/1/14_111.xlsx', 'rb'))\n",
        "data34 = pd.DataFrame(data34)\n",
        "data34 = pd.DataFrame.to_numpy(data34)\n",
        "data34 = np.rot90(data34)\n",
        "data34.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbfAdSfjsFBA",
        "outputId": "b5fc3213-2931-444a-9011-732a0ff95523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data35 = pd.read_excel(open('/content/drive/MyDrive/work/1/15_111.xlsx', 'rb'))\n",
        "data35 = pd.DataFrame(data35)\n",
        "data35 = pd.DataFrame.to_numpy(data35)\n",
        "data35 = np.rot90(data35)\n",
        "data35.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7UCL1QNsFeY",
        "outputId": "ad426747-8b07-420c-ec89-dcd54678f75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data36 = pd.read_excel(open('/content/drive/MyDrive/work/1/17_111.xlsx', 'rb'))\n",
        "data36 = pd.DataFrame(data36)\n",
        "data36 = pd.DataFrame.to_numpy(data36)\n",
        "data36 = np.rot90(data36)\n",
        "data36.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ1sU-YEsGFw",
        "outputId": "ea787c93-bd82-4599-a916-cfc7874c5915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data37 = pd.read_excel(open('/content/drive/MyDrive/work/1/18_111.xlsx', 'rb'))\n",
        "data37 = pd.DataFrame(data37)\n",
        "data37 = pd.DataFrame.to_numpy(data37)\n",
        "data37 = np.rot90(data37)\n",
        "data37.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxSenAaisF3w",
        "outputId": "73a63c43-b1bc-4cde-dd1b-b0628c320075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7K7nOb0_UaO"
      },
      "source": [
        "Графики на валидацию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG9A-kDN_Xix",
        "outputId": "0bfcfc35-0033-4900-f9d0-387c1e577280"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "data21 = pd.read_excel(open('/content/drive/MyDrive/work/0/0330.xlsx', 'rb'))\n",
        "data21 = pd.DataFrame(data21)\n",
        "data21 = pd.DataFrame.to_numpy(data21)\n",
        "data21 = np.rot90(data21)\n",
        "data21.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geDAwyCA_fZm",
        "outputId": "2b30115f-ff9d-46fb-a152-62ea04dafaef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "data22 = pd.read_excel(open('/content/drive/MyDrive/work/0/0350.xlsx', 'rb'))\n",
        "data22 = pd.DataFrame(data22)\n",
        "data22 = pd.DataFrame.to_numpy(data22)\n",
        "data22 = np.rot90(data22)\n",
        "data22.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DybOwpd6_fUW",
        "outputId": "229a7e1d-44ed-4862-d2ae-0d0ce9bb80bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "data23 = pd.read_excel(open('/content/drive/MyDrive/work/2/330.xlsx', 'rb'))\n",
        "data23 = pd.DataFrame(data23)\n",
        "data23 = pd.DataFrame.to_numpy(data23)\n",
        "data23 = np.rot90(data23)\n",
        "data23.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFBzsr_G_fOw",
        "outputId": "d7e72a4d-70cd-47eb-bf8d-5daabee79051"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "data24 = pd.read_excel(open('/content/drive/MyDrive/work/2/350.xlsx', 'rb'))\n",
        "data24 = pd.DataFrame(data24)\n",
        "data24 = pd.DataFrame.to_numpy(data24)\n",
        "data24 = np.rot90(data24)\n",
        "data24.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data25 = pd.read_excel(open('/content/drive/MyDrive/work/1/val_111.xlsx', 'rb'))\n",
        "data25 = pd.DataFrame(data25)\n",
        "data25 = pd.DataFrame.to_numpy(data25)\n",
        "data25 = np.rot90(data25)\n",
        "data25.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9Gjb8oArjaC",
        "outputId": "5ada7021-5261-4748-e6f6-94a5b2a47d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data26= pd.read_excel(open('/content/drive/MyDrive/work/1/16_111.xlsx', 'rb'))\n",
        "data26 = pd.DataFrame(data26)\n",
        "data26 = pd.DataFrame.to_numpy(data26)\n",
        "data26 = np.rot90(data26)\n",
        "data26.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFZclNqCrjwY",
        "outputId": "f0201913-a9d9-40ef-b0c9-cbf8db22bfe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4vWtCezUnKu"
      },
      "source": [
        "##Подготовка датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "EsmFXtlaHS4W"
      },
      "outputs": [],
      "source": [
        "#parameters\n",
        "data_train_number = 21 # количество файлов с графиками на обучение\n",
        "data_test_number = 6 # количество файлов с графиками на валидацию\n",
        "num_samples = 20 # количество сэмплов в одном графике\n",
        "point_number = 1000 # количество точек в одном графике\n",
        "num_graphycs = 3 # количество графиков\n",
        "num_grahp0 = 3 # кол-во эллиптических графиков\n",
        "num_grahp1 = 3 # кол-во графиков с самопересечением\n",
        "num_grahp2 = 3 # кол-во метастабильных графиков\n",
        "inputs = []\n",
        "inputs_test = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KVeDoDjCZdN"
      },
      "source": [
        "Data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reImqeAHTlb1",
        "outputId": "576a9e9c-6e73-4cb2-b5c4-df9eb8c3c8cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        ...,\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.]]), (1260, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ],
      "source": [
        "y0_train = np.zeros((num_grahp0*num_samples*7,1))\n",
        "y1_train = np.ones((num_grahp1*num_samples*7,1))\n",
        "y2_train = np.ones((num_grahp2*num_samples*7,1))\n",
        "y2_train = y2_train*2\n",
        "y_train = np.concatenate((y0_train, y1_train, y2_train))\n",
        "#for i in range (5):\n",
        "#  y = np.concatenate((y, y))\n",
        "y_train, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AftsfVRiWjIm",
        "outputId": "552b1468-1eae-4a9f-f08b-733170dd4346"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(126, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ],
      "source": [
        "data_x_train = np.concatenate((data1,data2,data3,data4,data5,data6,data7,data31,data32,data33,data34,data35,data36,data37,data11,data12,data13,data14,data15,data16,data17))\n",
        "data_x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB7L1axkHaLO",
        "outputId": "a59565a9-ef38-4f51-f3bf-16099fe0d442"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1260, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ],
      "source": [
        "for j in range (data_train_number*num_graphycs):\n",
        "  for i in range (num_samples):\n",
        "    inputs.append(np.concatenate((data_x_train[j*2,i*point_number:i*point_number+point_number], data_x_train[j*2+1,i*point_number:i*point_number+point_number])))\n",
        "inputs = np.concatenate(inputs, axis=0)\n",
        "x_train = np.reshape(inputs, (data_train_number*num_graphycs*num_samples,point_number*2))\n",
        "# Тип данных  в нейросетях - float32. Можно и float64, но тогда модель нужно делать с таким типом данных.\n",
        "#x = x.astype('float32')\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLdTD4KqXvnO",
        "outputId": "7a3d426e-2f56-4cc0-d41f-82710c90444f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.      ,  10.799518,  11.305789, ...,  -9.683737,  -9.999936,\n",
              "       -10.315881])"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ],
      "source": [
        "data_train = np.concatenate((y_train,x_train), axis=1)\n",
        "data_train[0,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "-7Ih4dhZXseF"
      },
      "outputs": [],
      "source": [
        "data_train = sorted(data_train, key=lambda x: random.random())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vci-eSMravRP",
        "outputId": "c592bd94-3b36-44f4-d090-c72ae831edff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1260, 2001)"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ],
      "source": [
        "data_train = np.asarray(data_train)\n",
        "data_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnlOpWLcYhdF",
        "outputId": "7e28e338-a284-47e3-c448-77037a89d771"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1260, 2000), (1260,))"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ],
      "source": [
        "x_train, y_train = data_train[:,1:], data_train[:,0]\n",
        "x_train = (x_train - x_train.mean()) / x_train.std()\n",
        "x_train = x_train.astype('float32')\n",
        "y_train = y_train.astype('int64')\n",
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqFr9ZjLDSJu"
      },
      "source": [
        "Data for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6q-ep4YDkxO",
        "outputId": "20d6d8ce-ecb4-4692-af0a-b456b2c8df47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.],\n",
              "        [2.]]), (360, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ],
      "source": [
        "y0_test = np.zeros((num_grahp0*num_samples*2,1))\n",
        "y1_test = np.ones((num_grahp1*num_samples*2,1))\n",
        "y2_test = np.ones((num_grahp2*num_samples*2,1))\n",
        "y2_test = y2_test*2\n",
        "y_test = np.concatenate((y0_test, y1_test, y2_test))\n",
        "#for i in range (5):\n",
        "#  y = np.concatenate((y, y))\n",
        "y_test, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76Y6UryJDUAf",
        "outputId": "6262e020-602a-4d0d-8194-247c8c1ad155"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ],
      "source": [
        "data_x_test = np.concatenate((data21,data22,data25,data26,data23,data24))\n",
        "data_x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0JcUSuvDfgN",
        "outputId": "28c935ce-6e2b-4ec0-ae06-82992a2d12da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(360, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ],
      "source": [
        "for j in range (data_test_number*num_graphycs):\n",
        "  for i in range (num_samples):\n",
        "    inputs_test.append(np.concatenate((data_x_test[j*2,i*point_number:i*point_number+point_number], data_x_test[j*2+1,i*point_number:i*point_number+point_number])))\n",
        "inputs_test = np.concatenate(inputs_test, axis=0)\n",
        "x_test = np.reshape(inputs_test, (data_test_number*num_graphycs*num_samples,point_number*2))\n",
        "# Тип данных  в нейросетях - float32. Можно и float64, но тогда модель нужно делать с таким типом данных.\n",
        "#x = x.astype('float32')\n",
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24682Z_YESjF",
        "outputId": "700f0fdf-dbc3-4cc3-973f-58219dcdcb41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0.        , -696.13182   , -688.57759125, ..., -601.781265  ,\n",
              "       -598.2432525 , -594.76867875])"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ],
      "source": [
        "data_test = np.concatenate((y_test,x_test), axis=1)\n",
        "data_test[0,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "1UPeWIt0Eamt"
      },
      "outputs": [],
      "source": [
        "data_test = sorted(data_test, key=lambda x: random.random())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_O3KrWUEdDg",
        "outputId": "a0715d6b-5ccf-4c1f-e1bb-4f5423dc20e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(360, 2001)"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ],
      "source": [
        "data_test = np.asarray(data_test)\n",
        "data_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1ihuS2pEjIV",
        "outputId": "b295dada-830e-4a80-efcd-0d2a61e97afc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((360, 2000), (360,))"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ],
      "source": [
        "x_test, y_test = data_test[:,1:], data_test[:,0]\n",
        "x_test = (x_test - x_test.mean()) / x_test.std()\n",
        "x_test = x_test.astype('float32')\n",
        "y_test = y_test.astype('int64')\n",
        "x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "wS05TpUwkmdp"
      },
      "outputs": [],
      "source": [
        "# Индексация классов начиается с нуля. То есть первый класс - 0, второй класс - 1. Тип данных разметки - int64.\n",
        "# Размерность у разметки должна быть одна"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "HitKnVqI2YkK",
        "outputId": "0e5e977e-bedd-4f25-b9b3-a2f316599799"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range (6):\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.title(y_train[i])\n",
        "  plt.plot(x_train[i,:999], x_train[i,1001:2000], color='tab:blue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "XURkjZuT6yyi"
      },
      "outputs": [],
      "source": [
        "# Тут и далее уже очень много ошибок, покажу просто как правильно с комментариями.\n",
        "\n",
        "class ShaftDataset(Dataset):\n",
        "    def __init__(self, x, y, mode):\n",
        "        # Проверка выборки\n",
        "        if mode == 'train':\n",
        "            self.x = x_train\n",
        "            self.y = y_train\n",
        "        elif mode == 'val':\n",
        "            self.x = x_test\n",
        "            self.y = y_test\n",
        "            \n",
        "        # Разбиение сделано по вашему примеру, но обращаю внимание,\n",
        "        # что так в тренировочной выборке всего один пример со 2 классом. А в тестовой - только 2 класс/\n",
        "        # Поэтому нужно как то примеры скомпоновать по-другому\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "ITBdf8iT7D8x"
      },
      "outputs": [],
      "source": [
        "# Создаем два отдельных датасета и их будем подавать в Loader.\n",
        "# Изначально вы пытались сделать два среза по одному датасету и подавили в Loader. Так работать не будет\n",
        "\n",
        "train_dataset = ShaftDataset(x_train, y_train, 'train')\n",
        "test_dataset = ShaftDataset(x_test, y_test, 'val')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgohZFvDgJmR",
        "outputId": "79599adb-7b4d-4ca0-d72a-2f20bb9c6b88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.24441516, -0.24281083, -0.24117884, ..., -0.50478244,\n",
              "        -0.49985856, -0.49529323], dtype=float32), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG0rieAzwscd"
      },
      "source": [
        "##*Hyper* parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "Ow5VwGMIwrym"
      },
      "outputs": [],
      "source": [
        "input_size = x_train.shape[1]\n",
        "hidden_size = 40 # Попробуйте для начала поменьше\n",
        "num_classes = 3\n",
        "num_epochs = 2000\n",
        "batch_size = 20 # Bath size не может быть больше размера датасета (у вас 30 примеров)\n",
        "out_c = 40\n",
        "out_c1 = 80\n",
        "out_c2 = 160\n",
        "lr = 0.00005 # learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "KxDbXIaVquNd"
      },
      "outputs": [],
      "source": [
        "# Loader генерирует тензоры следующего размера -\n",
        "# (batch_size (размер батча - кол-во примеров в батче), m (кол-во признаков))\n",
        "# Таким образом, размерностей 2, а не 4\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCYSzuDxtJx1",
        "outputId": "7b71ba09-2301-4e18-f335-ce3bf8688825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1260, 2000) (1260,)\n",
            "(360, 2000) (360,)\n"
          ]
        }
      ],
      "source": [
        "# Размеры данных в датасетах\n",
        "\n",
        "features, labels = train_dataset.x, train_dataset.y\n",
        "print(features.shape, labels.shape)\n",
        "features, labels = test_dataset.x, test_dataset.y\n",
        "print(features.shape, labels.shape)\n",
        "#samples = torch.from_numpy(samples)\n",
        "#labels = torch.from_numpy(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsQ4OoSKxrmR"
      },
      "source": [
        "#Создание нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "psneQ6gguTQO"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    #4000*1 > 2000#2\n",
        "    self.l1_1 = nn.Sequential(nn.Conv1d(batch_size, out_c, 3, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(1998))\n",
        "    self.l1_2 = nn.Sequential(nn.Conv1d(out_c, out_c, 3, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(1996))\n",
        "    self.l1_3 = nn.Sequential(nn.Conv1d(out_c, out_c, 3, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(1994))\n",
        "\n",
        "    self.l2_1 = nn.Sequential(nn.Conv1d(out_c, out_c1, 3, stride=2, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(996))\n",
        "    self.l2_2 = nn.Sequential(nn.Conv1d(out_c1, out_c1, 3, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(994))\n",
        "    self.l2_3 = nn.Sequential(nn.Conv1d(out_c1, out_c1, 3, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(992))\n",
        "\n",
        "    self.l3_1 = nn.Sequential(nn.Conv1d(out_c1, out_c2, 3, stride=2, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(495))\n",
        "    self.l3_2 = nn.Sequential(nn.Conv1d(out_c2, out_c2, 3, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(493))\n",
        "    self.l3_3 = nn.Sequential(nn.Conv1d(out_c2, batch_size, 3, bias=False),\n",
        "                            nn.ReLU(True),\n",
        "                            nn.BatchNorm1d(491))\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    \n",
        "    self.l1 = nn.Linear(491, 32)\n",
        "    self.drop = nn.Dropout(p=0.5)\n",
        "    self.sigmoid = nn.Sigmoid() \n",
        "    self.l2 = nn.Linear(32, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1_1(x)\n",
        "    out = self.l1_2(out)\n",
        "    out = self.l1_3(out)\n",
        "\n",
        "    out = self.l2_1(out)\n",
        "    out = self.l2_2(out)\n",
        "    out = self.l2_3(out)\n",
        "\n",
        "    out = self.l3_1(out)\n",
        "    out = self.l3_2(out)\n",
        "    out = self.l3_3(out)\n",
        "\n",
        "    out = self.flatten(out)\n",
        "    out = self.l1(out)\n",
        "    out = self.drop(out)\n",
        "    out = self.sigmoid(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Модель также перемещаем в нужное место\n",
        "model = model.to(device)\n",
        "# loss and optimizer\n",
        "L = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhe6kfOPx3YG"
      },
      "source": [
        "Loss and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-JRvLNRyRIl"
      },
      "source": [
        "#Training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NN_ver = '2_6_cnn'\n",
        "history = {'Train loss': [],\n",
        "            'Val loss': [],\n",
        "            'Train {}'.format(NN_ver): [],\n",
        "            'Val {}'.format(NN_ver): []}"
      ],
      "metadata": {
        "id": "wr4dTyThNRKH"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcrFEtk1yS3N",
        "outputId": "34f62805-1074-48e5-9962-5ea6c8a1dc35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "epoch 334 / 2000, step 32/63, loss = 0.9475\n",
            "epoch 334 / 2000, step 48/63, loss = 1.0539\n",
            "epoch 335 / 2000, step 16/63, loss = 0.8798\n",
            "epoch 335 / 2000, step 32/63, loss = 1.0057\n",
            "epoch 335 / 2000, step 48/63, loss = 0.9518\n",
            "epoch 336 / 2000, step 16/63, loss = 0.9221\n",
            "epoch 336 / 2000, step 32/63, loss = 0.9754\n",
            "epoch 336 / 2000, step 48/63, loss = 1.0907\n",
            "epoch 337 / 2000, step 16/63, loss = 0.9310\n",
            "epoch 337 / 2000, step 32/63, loss = 0.9656\n",
            "epoch 337 / 2000, step 48/63, loss = 0.9764\n",
            "epoch 338 / 2000, step 16/63, loss = 1.0524\n",
            "epoch 338 / 2000, step 32/63, loss = 1.0137\n",
            "epoch 338 / 2000, step 48/63, loss = 0.8253\n",
            "epoch 339 / 2000, step 16/63, loss = 1.0127\n",
            "epoch 339 / 2000, step 32/63, loss = 1.0549\n",
            "epoch 339 / 2000, step 48/63, loss = 1.1119\n",
            "epoch 340 / 2000, step 16/63, loss = 0.8821\n",
            "epoch 340 / 2000, step 32/63, loss = 0.9787\n",
            "epoch 340 / 2000, step 48/63, loss = 0.9794\n",
            "epoch 341 / 2000, step 16/63, loss = 1.0476\n",
            "epoch 341 / 2000, step 32/63, loss = 0.8665\n",
            "epoch 341 / 2000, step 48/63, loss = 1.1149\n",
            "epoch 342 / 2000, step 16/63, loss = 0.8019\n",
            "epoch 342 / 2000, step 32/63, loss = 1.0025\n",
            "epoch 342 / 2000, step 48/63, loss = 0.8667\n",
            "epoch 343 / 2000, step 16/63, loss = 0.8554\n",
            "epoch 343 / 2000, step 32/63, loss = 0.9456\n",
            "epoch 343 / 2000, step 48/63, loss = 0.9729\n",
            "epoch 344 / 2000, step 16/63, loss = 0.9876\n",
            "epoch 344 / 2000, step 32/63, loss = 0.8027\n",
            "epoch 344 / 2000, step 48/63, loss = 0.9949\n",
            "epoch 345 / 2000, step 16/63, loss = 0.8562\n",
            "epoch 345 / 2000, step 32/63, loss = 1.0475\n",
            "epoch 345 / 2000, step 48/63, loss = 1.0361\n",
            "epoch 346 / 2000, step 16/63, loss = 0.9024\n",
            "epoch 346 / 2000, step 32/63, loss = 0.9053\n",
            "epoch 346 / 2000, step 48/63, loss = 0.8378\n",
            "epoch 347 / 2000, step 16/63, loss = 1.1168\n",
            "epoch 347 / 2000, step 32/63, loss = 1.0200\n",
            "epoch 347 / 2000, step 48/63, loss = 0.8057\n",
            "epoch 348 / 2000, step 16/63, loss = 0.8850\n",
            "epoch 348 / 2000, step 32/63, loss = 0.9767\n",
            "epoch 348 / 2000, step 48/63, loss = 0.9365\n",
            "epoch 349 / 2000, step 16/63, loss = 0.9364\n",
            "epoch 349 / 2000, step 32/63, loss = 1.0429\n",
            "epoch 349 / 2000, step 48/63, loss = 0.9717\n",
            "epoch 350 / 2000, step 16/63, loss = 0.8509\n",
            "epoch 350 / 2000, step 32/63, loss = 0.9185\n",
            "epoch 350 / 2000, step 48/63, loss = 1.1088\n",
            "epoch 351 / 2000, step 16/63, loss = 0.9309\n",
            "epoch 351 / 2000, step 32/63, loss = 0.9521\n",
            "epoch 351 / 2000, step 48/63, loss = 0.9845\n",
            "epoch 352 / 2000, step 16/63, loss = 0.9840\n",
            "epoch 352 / 2000, step 32/63, loss = 0.9379\n",
            "epoch 352 / 2000, step 48/63, loss = 0.8681\n",
            "epoch 353 / 2000, step 16/63, loss = 0.9847\n",
            "epoch 353 / 2000, step 32/63, loss = 0.8740\n",
            "epoch 353 / 2000, step 48/63, loss = 0.8922\n",
            "epoch 354 / 2000, step 16/63, loss = 0.9065\n",
            "epoch 354 / 2000, step 32/63, loss = 0.9943\n",
            "epoch 354 / 2000, step 48/63, loss = 1.0111\n",
            "epoch 355 / 2000, step 16/63, loss = 0.9947\n",
            "epoch 355 / 2000, step 32/63, loss = 1.0224\n",
            "epoch 355 / 2000, step 48/63, loss = 0.9500\n",
            "epoch 356 / 2000, step 16/63, loss = 0.7434\n",
            "epoch 356 / 2000, step 32/63, loss = 0.9794\n",
            "epoch 356 / 2000, step 48/63, loss = 1.0066\n",
            "epoch 357 / 2000, step 16/63, loss = 1.0069\n",
            "epoch 357 / 2000, step 32/63, loss = 0.8770\n",
            "epoch 357 / 2000, step 48/63, loss = 0.9682\n",
            "epoch 358 / 2000, step 16/63, loss = 0.9930\n",
            "epoch 358 / 2000, step 32/63, loss = 0.8182\n",
            "epoch 358 / 2000, step 48/63, loss = 0.9059\n",
            "epoch 359 / 2000, step 16/63, loss = 1.0037\n",
            "epoch 359 / 2000, step 32/63, loss = 0.9133\n",
            "epoch 359 / 2000, step 48/63, loss = 0.8901\n",
            "epoch 360 / 2000, step 16/63, loss = 0.9407\n",
            "epoch 360 / 2000, step 32/63, loss = 0.9300\n",
            "epoch 360 / 2000, step 48/63, loss = 0.8148\n",
            "epoch 361 / 2000, step 16/63, loss = 1.0782\n",
            "epoch 361 / 2000, step 32/63, loss = 1.0383\n",
            "epoch 361 / 2000, step 48/63, loss = 1.0744\n",
            "epoch 362 / 2000, step 16/63, loss = 1.0026\n",
            "epoch 362 / 2000, step 32/63, loss = 0.8602\n",
            "epoch 362 / 2000, step 48/63, loss = 0.9683\n",
            "epoch 363 / 2000, step 16/63, loss = 0.8411\n",
            "epoch 363 / 2000, step 32/63, loss = 1.1015\n",
            "epoch 363 / 2000, step 48/63, loss = 1.0714\n",
            "epoch 364 / 2000, step 16/63, loss = 1.0047\n",
            "epoch 364 / 2000, step 32/63, loss = 0.9845\n",
            "epoch 364 / 2000, step 48/63, loss = 0.9750\n",
            "epoch 365 / 2000, step 16/63, loss = 0.8814\n",
            "epoch 365 / 2000, step 32/63, loss = 1.0852\n",
            "epoch 365 / 2000, step 48/63, loss = 1.0313\n",
            "epoch 366 / 2000, step 16/63, loss = 0.8726\n",
            "epoch 366 / 2000, step 32/63, loss = 0.9284\n",
            "epoch 366 / 2000, step 48/63, loss = 0.8905\n",
            "epoch 367 / 2000, step 16/63, loss = 1.0561\n",
            "epoch 367 / 2000, step 32/63, loss = 0.9799\n",
            "epoch 367 / 2000, step 48/63, loss = 0.7631\n",
            "epoch 368 / 2000, step 16/63, loss = 0.8887\n",
            "epoch 368 / 2000, step 32/63, loss = 0.9815\n",
            "epoch 368 / 2000, step 48/63, loss = 1.0114\n",
            "epoch 369 / 2000, step 16/63, loss = 0.9389\n",
            "epoch 369 / 2000, step 32/63, loss = 1.0617\n",
            "epoch 369 / 2000, step 48/63, loss = 0.9031\n",
            "epoch 370 / 2000, step 16/63, loss = 0.8366\n",
            "epoch 370 / 2000, step 32/63, loss = 1.1175\n",
            "epoch 370 / 2000, step 48/63, loss = 0.8855\n",
            "epoch 371 / 2000, step 16/63, loss = 0.8366\n",
            "epoch 371 / 2000, step 32/63, loss = 1.0344\n",
            "epoch 371 / 2000, step 48/63, loss = 0.9053\n",
            "epoch 372 / 2000, step 16/63, loss = 0.8808\n",
            "epoch 372 / 2000, step 32/63, loss = 0.9925\n",
            "epoch 372 / 2000, step 48/63, loss = 0.9461\n",
            "epoch 373 / 2000, step 16/63, loss = 1.0134\n",
            "epoch 373 / 2000, step 32/63, loss = 0.9327\n",
            "epoch 373 / 2000, step 48/63, loss = 1.0204\n",
            "epoch 374 / 2000, step 16/63, loss = 0.9603\n",
            "epoch 374 / 2000, step 32/63, loss = 0.9340\n",
            "epoch 374 / 2000, step 48/63, loss = 0.9881\n",
            "epoch 375 / 2000, step 16/63, loss = 0.8714\n",
            "epoch 375 / 2000, step 32/63, loss = 0.9045\n",
            "epoch 375 / 2000, step 48/63, loss = 1.0121\n",
            "epoch 376 / 2000, step 16/63, loss = 0.8725\n",
            "epoch 376 / 2000, step 32/63, loss = 0.9306\n",
            "epoch 376 / 2000, step 48/63, loss = 0.9883\n",
            "epoch 377 / 2000, step 16/63, loss = 1.0064\n",
            "epoch 377 / 2000, step 32/63, loss = 0.9576\n",
            "epoch 377 / 2000, step 48/63, loss = 0.9249\n",
            "epoch 378 / 2000, step 16/63, loss = 1.0028\n",
            "epoch 378 / 2000, step 32/63, loss = 0.9270\n",
            "epoch 378 / 2000, step 48/63, loss = 0.8464\n",
            "epoch 379 / 2000, step 16/63, loss = 1.1096\n",
            "epoch 379 / 2000, step 32/63, loss = 1.0539\n",
            "epoch 379 / 2000, step 48/63, loss = 0.9469\n",
            "epoch 380 / 2000, step 16/63, loss = 0.8554\n",
            "epoch 380 / 2000, step 32/63, loss = 0.9778\n",
            "epoch 380 / 2000, step 48/63, loss = 0.9209\n",
            "epoch 381 / 2000, step 16/63, loss = 1.1177\n",
            "epoch 381 / 2000, step 32/63, loss = 0.8330\n",
            "epoch 381 / 2000, step 48/63, loss = 1.0578\n",
            "epoch 382 / 2000, step 16/63, loss = 1.0766\n",
            "epoch 382 / 2000, step 32/63, loss = 0.9887\n",
            "epoch 382 / 2000, step 48/63, loss = 0.8715\n",
            "epoch 383 / 2000, step 16/63, loss = 1.0200\n",
            "epoch 383 / 2000, step 32/63, loss = 0.9613\n",
            "epoch 383 / 2000, step 48/63, loss = 1.0656\n",
            "epoch 384 / 2000, step 16/63, loss = 0.8534\n",
            "epoch 384 / 2000, step 32/63, loss = 0.9690\n",
            "epoch 384 / 2000, step 48/63, loss = 1.1485\n",
            "epoch 385 / 2000, step 16/63, loss = 0.9325\n",
            "epoch 385 / 2000, step 32/63, loss = 0.9393\n",
            "epoch 385 / 2000, step 48/63, loss = 0.9551\n",
            "epoch 386 / 2000, step 16/63, loss = 0.8980\n",
            "epoch 386 / 2000, step 32/63, loss = 1.0253\n",
            "epoch 386 / 2000, step 48/63, loss = 0.9576\n",
            "epoch 387 / 2000, step 16/63, loss = 0.7944\n",
            "epoch 387 / 2000, step 32/63, loss = 0.9833\n",
            "epoch 387 / 2000, step 48/63, loss = 0.7912\n",
            "epoch 388 / 2000, step 16/63, loss = 0.8530\n",
            "epoch 388 / 2000, step 32/63, loss = 0.8091\n",
            "epoch 388 / 2000, step 48/63, loss = 0.7955\n",
            "epoch 389 / 2000, step 16/63, loss = 1.0126\n",
            "epoch 389 / 2000, step 32/63, loss = 0.9458\n",
            "epoch 389 / 2000, step 48/63, loss = 0.9919\n",
            "epoch 390 / 2000, step 16/63, loss = 1.1282\n",
            "epoch 390 / 2000, step 32/63, loss = 1.0960\n",
            "epoch 390 / 2000, step 48/63, loss = 1.0045\n",
            "epoch 391 / 2000, step 16/63, loss = 0.8822\n",
            "epoch 391 / 2000, step 32/63, loss = 0.9044\n",
            "epoch 391 / 2000, step 48/63, loss = 1.0585\n",
            "epoch 392 / 2000, step 16/63, loss = 0.9576\n",
            "epoch 392 / 2000, step 32/63, loss = 1.0128\n",
            "epoch 392 / 2000, step 48/63, loss = 1.0795\n",
            "epoch 393 / 2000, step 16/63, loss = 1.0564\n",
            "epoch 393 / 2000, step 32/63, loss = 0.9597\n",
            "epoch 393 / 2000, step 48/63, loss = 1.0788\n",
            "epoch 394 / 2000, step 16/63, loss = 1.0359\n",
            "epoch 394 / 2000, step 32/63, loss = 0.9271\n",
            "epoch 394 / 2000, step 48/63, loss = 0.7756\n",
            "epoch 395 / 2000, step 16/63, loss = 0.9901\n",
            "epoch 395 / 2000, step 32/63, loss = 1.0319\n",
            "epoch 395 / 2000, step 48/63, loss = 0.9214\n",
            "epoch 396 / 2000, step 16/63, loss = 0.8398\n",
            "epoch 396 / 2000, step 32/63, loss = 1.0095\n",
            "epoch 396 / 2000, step 48/63, loss = 1.0004\n",
            "epoch 397 / 2000, step 16/63, loss = 0.9470\n",
            "epoch 397 / 2000, step 32/63, loss = 0.9049\n",
            "epoch 397 / 2000, step 48/63, loss = 0.8226\n",
            "epoch 398 / 2000, step 16/63, loss = 0.9757\n",
            "epoch 398 / 2000, step 32/63, loss = 0.9809\n",
            "epoch 398 / 2000, step 48/63, loss = 0.7998\n",
            "epoch 399 / 2000, step 16/63, loss = 1.0304\n",
            "epoch 399 / 2000, step 32/63, loss = 1.0962\n",
            "epoch 399 / 2000, step 48/63, loss = 0.9158\n",
            "epoch 400 / 2000, step 16/63, loss = 0.9050\n",
            "epoch 400 / 2000, step 32/63, loss = 0.9135\n",
            "epoch 400 / 2000, step 48/63, loss = 0.8651\n",
            "epoch 401 / 2000, step 16/63, loss = 0.8900\n",
            "epoch 401 / 2000, step 32/63, loss = 0.8645\n",
            "epoch 401 / 2000, step 48/63, loss = 0.9438\n",
            "epoch 402 / 2000, step 16/63, loss = 0.8413\n",
            "epoch 402 / 2000, step 32/63, loss = 0.8430\n",
            "epoch 402 / 2000, step 48/63, loss = 0.9386\n",
            "epoch 403 / 2000, step 16/63, loss = 1.0269\n",
            "epoch 403 / 2000, step 32/63, loss = 0.9028\n",
            "epoch 403 / 2000, step 48/63, loss = 0.8761\n",
            "epoch 404 / 2000, step 16/63, loss = 1.0171\n",
            "epoch 404 / 2000, step 32/63, loss = 0.7789\n",
            "epoch 404 / 2000, step 48/63, loss = 0.9602\n",
            "epoch 405 / 2000, step 16/63, loss = 1.0998\n",
            "epoch 405 / 2000, step 32/63, loss = 0.8388\n",
            "epoch 405 / 2000, step 48/63, loss = 0.8193\n",
            "epoch 406 / 2000, step 16/63, loss = 1.0161\n",
            "epoch 406 / 2000, step 32/63, loss = 0.7869\n",
            "epoch 406 / 2000, step 48/63, loss = 0.9465\n",
            "epoch 407 / 2000, step 16/63, loss = 0.8881\n",
            "epoch 407 / 2000, step 32/63, loss = 0.8451\n",
            "epoch 407 / 2000, step 48/63, loss = 0.7833\n",
            "epoch 408 / 2000, step 16/63, loss = 0.8956\n",
            "epoch 408 / 2000, step 32/63, loss = 0.7824\n",
            "epoch 408 / 2000, step 48/63, loss = 1.0196\n",
            "epoch 409 / 2000, step 16/63, loss = 0.9064\n",
            "epoch 409 / 2000, step 32/63, loss = 0.8203\n",
            "epoch 409 / 2000, step 48/63, loss = 0.9864\n",
            "epoch 410 / 2000, step 16/63, loss = 0.9114\n",
            "epoch 410 / 2000, step 32/63, loss = 0.9242\n",
            "epoch 410 / 2000, step 48/63, loss = 0.9410\n",
            "epoch 411 / 2000, step 16/63, loss = 0.9491\n",
            "epoch 411 / 2000, step 32/63, loss = 0.8613\n",
            "epoch 411 / 2000, step 48/63, loss = 0.9458\n",
            "epoch 412 / 2000, step 16/63, loss = 0.8279\n",
            "epoch 412 / 2000, step 32/63, loss = 1.0530\n",
            "epoch 412 / 2000, step 48/63, loss = 1.0041\n",
            "epoch 413 / 2000, step 16/63, loss = 0.8695\n",
            "epoch 413 / 2000, step 32/63, loss = 0.9539\n",
            "epoch 413 / 2000, step 48/63, loss = 0.8716\n",
            "epoch 414 / 2000, step 16/63, loss = 0.8669\n",
            "epoch 414 / 2000, step 32/63, loss = 0.8514\n",
            "epoch 414 / 2000, step 48/63, loss = 0.8198\n",
            "epoch 415 / 2000, step 16/63, loss = 0.8191\n",
            "epoch 415 / 2000, step 32/63, loss = 1.0072\n",
            "epoch 415 / 2000, step 48/63, loss = 0.9380\n",
            "epoch 416 / 2000, step 16/63, loss = 0.8720\n",
            "epoch 416 / 2000, step 32/63, loss = 1.0586\n",
            "epoch 416 / 2000, step 48/63, loss = 1.0929\n",
            "epoch 417 / 2000, step 16/63, loss = 0.9516\n",
            "epoch 417 / 2000, step 32/63, loss = 0.7702\n",
            "epoch 417 / 2000, step 48/63, loss = 0.9693\n",
            "epoch 418 / 2000, step 16/63, loss = 0.9826\n",
            "epoch 418 / 2000, step 32/63, loss = 0.9630\n",
            "epoch 418 / 2000, step 48/63, loss = 0.9564\n",
            "epoch 419 / 2000, step 16/63, loss = 0.9474\n",
            "epoch 419 / 2000, step 32/63, loss = 0.8007\n",
            "epoch 419 / 2000, step 48/63, loss = 0.9102\n",
            "epoch 420 / 2000, step 16/63, loss = 0.9688\n",
            "epoch 420 / 2000, step 32/63, loss = 1.0090\n",
            "epoch 420 / 2000, step 48/63, loss = 1.0172\n",
            "epoch 421 / 2000, step 16/63, loss = 1.1138\n",
            "epoch 421 / 2000, step 32/63, loss = 0.9299\n",
            "epoch 421 / 2000, step 48/63, loss = 0.9026\n",
            "epoch 422 / 2000, step 16/63, loss = 0.9026\n",
            "epoch 422 / 2000, step 32/63, loss = 0.9617\n",
            "epoch 422 / 2000, step 48/63, loss = 0.9982\n",
            "epoch 423 / 2000, step 16/63, loss = 0.8137\n",
            "epoch 423 / 2000, step 32/63, loss = 0.9550\n",
            "epoch 423 / 2000, step 48/63, loss = 1.1312\n",
            "epoch 424 / 2000, step 16/63, loss = 1.0457\n",
            "epoch 424 / 2000, step 32/63, loss = 0.7883\n",
            "epoch 424 / 2000, step 48/63, loss = 0.9118\n",
            "epoch 425 / 2000, step 16/63, loss = 0.9091\n",
            "epoch 425 / 2000, step 32/63, loss = 1.0972\n",
            "epoch 425 / 2000, step 48/63, loss = 0.9206\n",
            "epoch 426 / 2000, step 16/63, loss = 0.8351\n",
            "epoch 426 / 2000, step 32/63, loss = 0.9463\n",
            "epoch 426 / 2000, step 48/63, loss = 1.0642\n",
            "epoch 427 / 2000, step 16/63, loss = 0.8115\n",
            "epoch 427 / 2000, step 32/63, loss = 0.8556\n",
            "epoch 427 / 2000, step 48/63, loss = 0.9712\n",
            "epoch 428 / 2000, step 16/63, loss = 0.8270\n",
            "epoch 428 / 2000, step 32/63, loss = 0.8187\n",
            "epoch 428 / 2000, step 48/63, loss = 0.6581\n",
            "epoch 429 / 2000, step 16/63, loss = 0.9468\n",
            "epoch 429 / 2000, step 32/63, loss = 0.9658\n",
            "epoch 429 / 2000, step 48/63, loss = 0.9528\n",
            "epoch 430 / 2000, step 16/63, loss = 0.9058\n",
            "epoch 430 / 2000, step 32/63, loss = 0.8675\n",
            "epoch 430 / 2000, step 48/63, loss = 0.9262\n",
            "epoch 431 / 2000, step 16/63, loss = 1.0390\n",
            "epoch 431 / 2000, step 32/63, loss = 0.9269\n",
            "epoch 431 / 2000, step 48/63, loss = 0.9027\n",
            "epoch 432 / 2000, step 16/63, loss = 1.0494\n",
            "epoch 432 / 2000, step 32/63, loss = 0.9308\n",
            "epoch 432 / 2000, step 48/63, loss = 0.7609\n",
            "epoch 433 / 2000, step 16/63, loss = 0.9518\n",
            "epoch 433 / 2000, step 32/63, loss = 0.8484\n",
            "epoch 433 / 2000, step 48/63, loss = 0.7385\n",
            "epoch 434 / 2000, step 16/63, loss = 1.0035\n",
            "epoch 434 / 2000, step 32/63, loss = 0.8758\n",
            "epoch 434 / 2000, step 48/63, loss = 0.9489\n",
            "epoch 435 / 2000, step 16/63, loss = 0.8745\n",
            "epoch 435 / 2000, step 32/63, loss = 0.9012\n",
            "epoch 435 / 2000, step 48/63, loss = 0.8363\n",
            "epoch 436 / 2000, step 16/63, loss = 0.9897\n",
            "epoch 436 / 2000, step 32/63, loss = 0.8489\n",
            "epoch 436 / 2000, step 48/63, loss = 0.9991\n",
            "epoch 437 / 2000, step 16/63, loss = 0.8131\n",
            "epoch 437 / 2000, step 32/63, loss = 1.0306\n",
            "epoch 437 / 2000, step 48/63, loss = 0.8616\n",
            "epoch 438 / 2000, step 16/63, loss = 0.7972\n",
            "epoch 438 / 2000, step 32/63, loss = 1.0680\n",
            "epoch 438 / 2000, step 48/63, loss = 0.8640\n",
            "epoch 439 / 2000, step 16/63, loss = 1.0558\n",
            "epoch 439 / 2000, step 32/63, loss = 0.8533\n",
            "epoch 439 / 2000, step 48/63, loss = 0.9186\n",
            "epoch 440 / 2000, step 16/63, loss = 0.9113\n",
            "epoch 440 / 2000, step 32/63, loss = 0.9876\n",
            "epoch 440 / 2000, step 48/63, loss = 0.8999\n",
            "epoch 441 / 2000, step 16/63, loss = 0.8817\n",
            "epoch 441 / 2000, step 32/63, loss = 0.8118\n",
            "epoch 441 / 2000, step 48/63, loss = 0.8913\n",
            "epoch 442 / 2000, step 16/63, loss = 0.9543\n",
            "epoch 442 / 2000, step 32/63, loss = 0.8173\n",
            "epoch 442 / 2000, step 48/63, loss = 1.0370\n",
            "epoch 443 / 2000, step 16/63, loss = 0.9011\n",
            "epoch 443 / 2000, step 32/63, loss = 0.9342\n",
            "epoch 443 / 2000, step 48/63, loss = 0.8467\n",
            "epoch 444 / 2000, step 16/63, loss = 0.8551\n",
            "epoch 444 / 2000, step 32/63, loss = 0.8736\n",
            "epoch 444 / 2000, step 48/63, loss = 0.9276\n",
            "epoch 445 / 2000, step 16/63, loss = 0.7036\n",
            "epoch 445 / 2000, step 32/63, loss = 1.0385\n",
            "epoch 445 / 2000, step 48/63, loss = 0.8883\n",
            "epoch 446 / 2000, step 16/63, loss = 0.9278\n",
            "epoch 446 / 2000, step 32/63, loss = 0.8017\n",
            "epoch 446 / 2000, step 48/63, loss = 0.9757\n",
            "epoch 447 / 2000, step 16/63, loss = 0.8660\n",
            "epoch 447 / 2000, step 32/63, loss = 0.9870\n",
            "epoch 447 / 2000, step 48/63, loss = 0.8626\n",
            "epoch 448 / 2000, step 16/63, loss = 0.8947\n",
            "epoch 448 / 2000, step 32/63, loss = 1.0084\n",
            "epoch 448 / 2000, step 48/63, loss = 1.0437\n",
            "epoch 449 / 2000, step 16/63, loss = 0.9754\n",
            "epoch 449 / 2000, step 32/63, loss = 0.8318\n",
            "epoch 449 / 2000, step 48/63, loss = 0.9007\n",
            "epoch 450 / 2000, step 16/63, loss = 0.9257\n",
            "epoch 450 / 2000, step 32/63, loss = 0.8639\n",
            "epoch 450 / 2000, step 48/63, loss = 0.9332\n",
            "epoch 451 / 2000, step 16/63, loss = 0.8593\n",
            "epoch 451 / 2000, step 32/63, loss = 0.8998\n",
            "epoch 451 / 2000, step 48/63, loss = 0.8462\n",
            "epoch 452 / 2000, step 16/63, loss = 0.8584\n",
            "epoch 452 / 2000, step 32/63, loss = 0.9924\n",
            "epoch 452 / 2000, step 48/63, loss = 0.9174\n",
            "epoch 453 / 2000, step 16/63, loss = 0.8299\n",
            "epoch 453 / 2000, step 32/63, loss = 0.9211\n",
            "epoch 453 / 2000, step 48/63, loss = 0.7406\n",
            "epoch 454 / 2000, step 16/63, loss = 0.9140\n",
            "epoch 454 / 2000, step 32/63, loss = 0.8709\n",
            "epoch 454 / 2000, step 48/63, loss = 0.9392\n",
            "epoch 455 / 2000, step 16/63, loss = 1.1136\n",
            "epoch 455 / 2000, step 32/63, loss = 0.7949\n",
            "epoch 455 / 2000, step 48/63, loss = 0.8649\n",
            "epoch 456 / 2000, step 16/63, loss = 0.9468\n",
            "epoch 456 / 2000, step 32/63, loss = 0.9375\n",
            "epoch 456 / 2000, step 48/63, loss = 0.9116\n",
            "epoch 457 / 2000, step 16/63, loss = 0.7338\n",
            "epoch 457 / 2000, step 32/63, loss = 0.9355\n",
            "epoch 457 / 2000, step 48/63, loss = 0.8200\n",
            "epoch 458 / 2000, step 16/63, loss = 0.7807\n",
            "epoch 458 / 2000, step 32/63, loss = 0.9541\n",
            "epoch 458 / 2000, step 48/63, loss = 0.8098\n",
            "epoch 459 / 2000, step 16/63, loss = 1.0443\n",
            "epoch 459 / 2000, step 32/63, loss = 0.8427\n",
            "epoch 459 / 2000, step 48/63, loss = 0.7518\n",
            "epoch 460 / 2000, step 16/63, loss = 0.8959\n",
            "epoch 460 / 2000, step 32/63, loss = 0.9510\n",
            "epoch 460 / 2000, step 48/63, loss = 0.8988\n",
            "epoch 461 / 2000, step 16/63, loss = 0.8937\n",
            "epoch 461 / 2000, step 32/63, loss = 0.7961\n",
            "epoch 461 / 2000, step 48/63, loss = 0.8940\n",
            "epoch 462 / 2000, step 16/63, loss = 0.8175\n",
            "epoch 462 / 2000, step 32/63, loss = 1.1081\n",
            "epoch 462 / 2000, step 48/63, loss = 0.8449\n",
            "epoch 463 / 2000, step 16/63, loss = 0.9119\n",
            "epoch 463 / 2000, step 32/63, loss = 1.0591\n",
            "epoch 463 / 2000, step 48/63, loss = 0.9178\n",
            "epoch 464 / 2000, step 16/63, loss = 0.8529\n",
            "epoch 464 / 2000, step 32/63, loss = 1.0816\n",
            "epoch 464 / 2000, step 48/63, loss = 1.0035\n",
            "epoch 465 / 2000, step 16/63, loss = 0.8221\n",
            "epoch 465 / 2000, step 32/63, loss = 0.8592\n",
            "epoch 465 / 2000, step 48/63, loss = 1.0451\n",
            "epoch 466 / 2000, step 16/63, loss = 0.8250\n",
            "epoch 466 / 2000, step 32/63, loss = 0.9956\n",
            "epoch 466 / 2000, step 48/63, loss = 0.8929\n",
            "epoch 467 / 2000, step 16/63, loss = 0.8015\n",
            "epoch 467 / 2000, step 32/63, loss = 0.7869\n",
            "epoch 467 / 2000, step 48/63, loss = 0.7128\n",
            "epoch 468 / 2000, step 16/63, loss = 0.9044\n",
            "epoch 468 / 2000, step 32/63, loss = 0.8578\n",
            "epoch 468 / 2000, step 48/63, loss = 0.8278\n",
            "epoch 469 / 2000, step 16/63, loss = 0.9674\n",
            "epoch 469 / 2000, step 32/63, loss = 0.7864\n",
            "epoch 469 / 2000, step 48/63, loss = 0.8560\n",
            "epoch 470 / 2000, step 16/63, loss = 1.0690\n",
            "epoch 470 / 2000, step 32/63, loss = 0.8193\n",
            "epoch 470 / 2000, step 48/63, loss = 1.0696\n",
            "epoch 471 / 2000, step 16/63, loss = 0.9642\n",
            "epoch 471 / 2000, step 32/63, loss = 0.9436\n",
            "epoch 471 / 2000, step 48/63, loss = 0.8836\n",
            "epoch 472 / 2000, step 16/63, loss = 0.9376\n",
            "epoch 472 / 2000, step 32/63, loss = 1.1346\n",
            "epoch 472 / 2000, step 48/63, loss = 0.8322\n",
            "epoch 473 / 2000, step 16/63, loss = 1.0453\n",
            "epoch 473 / 2000, step 32/63, loss = 0.8600\n",
            "epoch 473 / 2000, step 48/63, loss = 1.1056\n",
            "epoch 474 / 2000, step 16/63, loss = 0.8902\n",
            "epoch 474 / 2000, step 32/63, loss = 0.7682\n",
            "epoch 474 / 2000, step 48/63, loss = 0.9245\n",
            "epoch 475 / 2000, step 16/63, loss = 0.7852\n",
            "epoch 475 / 2000, step 32/63, loss = 0.8402\n",
            "epoch 475 / 2000, step 48/63, loss = 0.9204\n",
            "epoch 476 / 2000, step 16/63, loss = 0.8802\n",
            "epoch 476 / 2000, step 32/63, loss = 0.9428\n",
            "epoch 476 / 2000, step 48/63, loss = 0.7666\n",
            "epoch 477 / 2000, step 16/63, loss = 0.9937\n",
            "epoch 477 / 2000, step 32/63, loss = 0.9500\n",
            "epoch 477 / 2000, step 48/63, loss = 0.8016\n",
            "epoch 478 / 2000, step 16/63, loss = 0.8556\n",
            "epoch 478 / 2000, step 32/63, loss = 0.8413\n",
            "epoch 478 / 2000, step 48/63, loss = 1.0282\n",
            "epoch 479 / 2000, step 16/63, loss = 0.9329\n",
            "epoch 479 / 2000, step 32/63, loss = 0.9976\n",
            "epoch 479 / 2000, step 48/63, loss = 0.8572\n",
            "epoch 480 / 2000, step 16/63, loss = 1.0207\n",
            "epoch 480 / 2000, step 32/63, loss = 1.0591\n",
            "epoch 480 / 2000, step 48/63, loss = 0.9333\n",
            "epoch 481 / 2000, step 16/63, loss = 0.8015\n",
            "epoch 481 / 2000, step 32/63, loss = 0.7654\n",
            "epoch 481 / 2000, step 48/63, loss = 0.9513\n",
            "epoch 482 / 2000, step 16/63, loss = 1.0381\n",
            "epoch 482 / 2000, step 32/63, loss = 0.9674\n",
            "epoch 482 / 2000, step 48/63, loss = 1.0091\n",
            "epoch 483 / 2000, step 16/63, loss = 0.8464\n",
            "epoch 483 / 2000, step 32/63, loss = 0.7428\n",
            "epoch 483 / 2000, step 48/63, loss = 0.9852\n",
            "epoch 484 / 2000, step 16/63, loss = 0.9821\n",
            "epoch 484 / 2000, step 32/63, loss = 0.8592\n",
            "epoch 484 / 2000, step 48/63, loss = 1.0074\n",
            "epoch 485 / 2000, step 16/63, loss = 1.0490\n",
            "epoch 485 / 2000, step 32/63, loss = 0.8528\n",
            "epoch 485 / 2000, step 48/63, loss = 0.9955\n",
            "epoch 486 / 2000, step 16/63, loss = 0.8460\n",
            "epoch 486 / 2000, step 32/63, loss = 0.7411\n",
            "epoch 486 / 2000, step 48/63, loss = 0.9313\n",
            "epoch 487 / 2000, step 16/63, loss = 0.9513\n",
            "epoch 487 / 2000, step 32/63, loss = 0.9689\n",
            "epoch 487 / 2000, step 48/63, loss = 0.9889\n",
            "epoch 488 / 2000, step 16/63, loss = 0.9941\n",
            "epoch 488 / 2000, step 32/63, loss = 0.9293\n",
            "epoch 488 / 2000, step 48/63, loss = 0.8718\n",
            "epoch 489 / 2000, step 16/63, loss = 0.8558\n",
            "epoch 489 / 2000, step 32/63, loss = 0.8417\n",
            "epoch 489 / 2000, step 48/63, loss = 0.9718\n",
            "epoch 490 / 2000, step 16/63, loss = 0.8409\n",
            "epoch 490 / 2000, step 32/63, loss = 1.1468\n",
            "epoch 490 / 2000, step 48/63, loss = 0.9698\n",
            "epoch 491 / 2000, step 16/63, loss = 0.7589\n",
            "epoch 491 / 2000, step 32/63, loss = 1.1447\n",
            "epoch 491 / 2000, step 48/63, loss = 0.7891\n",
            "epoch 492 / 2000, step 16/63, loss = 0.8951\n",
            "epoch 492 / 2000, step 32/63, loss = 0.7721\n",
            "epoch 492 / 2000, step 48/63, loss = 0.8301\n",
            "epoch 493 / 2000, step 16/63, loss = 0.8707\n",
            "epoch 493 / 2000, step 32/63, loss = 0.7947\n",
            "epoch 493 / 2000, step 48/63, loss = 0.8975\n",
            "epoch 494 / 2000, step 16/63, loss = 0.8237\n",
            "epoch 494 / 2000, step 32/63, loss = 0.9398\n",
            "epoch 494 / 2000, step 48/63, loss = 0.8657\n",
            "epoch 495 / 2000, step 16/63, loss = 0.8385\n",
            "epoch 495 / 2000, step 32/63, loss = 0.9291\n",
            "epoch 495 / 2000, step 48/63, loss = 0.8199\n",
            "epoch 496 / 2000, step 16/63, loss = 0.9101\n",
            "epoch 496 / 2000, step 32/63, loss = 0.8396\n",
            "epoch 496 / 2000, step 48/63, loss = 0.8709\n",
            "epoch 497 / 2000, step 16/63, loss = 0.8531\n",
            "epoch 497 / 2000, step 32/63, loss = 0.8834\n",
            "epoch 497 / 2000, step 48/63, loss = 0.9327\n",
            "epoch 498 / 2000, step 16/63, loss = 0.9040\n",
            "epoch 498 / 2000, step 32/63, loss = 0.9592\n",
            "epoch 498 / 2000, step 48/63, loss = 0.8245\n",
            "epoch 499 / 2000, step 16/63, loss = 0.8728\n",
            "epoch 499 / 2000, step 32/63, loss = 0.7586\n",
            "epoch 499 / 2000, step 48/63, loss = 0.8873\n",
            "epoch 500 / 2000, step 16/63, loss = 0.8907\n",
            "epoch 500 / 2000, step 32/63, loss = 0.9316\n",
            "epoch 500 / 2000, step 48/63, loss = 1.1043\n",
            "epoch 501 / 2000, step 16/63, loss = 0.9414\n",
            "epoch 501 / 2000, step 32/63, loss = 0.7612\n",
            "epoch 501 / 2000, step 48/63, loss = 0.7224\n",
            "epoch 502 / 2000, step 16/63, loss = 0.9241\n",
            "epoch 502 / 2000, step 32/63, loss = 1.0322\n",
            "epoch 502 / 2000, step 48/63, loss = 0.7511\n",
            "epoch 503 / 2000, step 16/63, loss = 0.8172\n",
            "epoch 503 / 2000, step 32/63, loss = 0.8037\n",
            "epoch 503 / 2000, step 48/63, loss = 0.8048\n",
            "epoch 504 / 2000, step 16/63, loss = 0.9338\n",
            "epoch 504 / 2000, step 32/63, loss = 0.9173\n",
            "epoch 504 / 2000, step 48/63, loss = 0.8259\n",
            "epoch 505 / 2000, step 16/63, loss = 1.0748\n",
            "epoch 505 / 2000, step 32/63, loss = 0.9923\n",
            "epoch 505 / 2000, step 48/63, loss = 1.1466\n",
            "epoch 506 / 2000, step 16/63, loss = 1.0202\n",
            "epoch 506 / 2000, step 32/63, loss = 0.9470\n",
            "epoch 506 / 2000, step 48/63, loss = 0.8276\n",
            "epoch 507 / 2000, step 16/63, loss = 0.8666\n",
            "epoch 507 / 2000, step 32/63, loss = 0.8754\n",
            "epoch 507 / 2000, step 48/63, loss = 0.8336\n",
            "epoch 508 / 2000, step 16/63, loss = 0.8051\n",
            "epoch 508 / 2000, step 32/63, loss = 0.8402\n",
            "epoch 508 / 2000, step 48/63, loss = 0.8386\n",
            "epoch 509 / 2000, step 16/63, loss = 0.8370\n",
            "epoch 509 / 2000, step 32/63, loss = 0.8450\n",
            "epoch 509 / 2000, step 48/63, loss = 0.8650\n",
            "epoch 510 / 2000, step 16/63, loss = 0.8638\n",
            "epoch 510 / 2000, step 32/63, loss = 0.8936\n",
            "epoch 510 / 2000, step 48/63, loss = 1.0185\n",
            "epoch 511 / 2000, step 16/63, loss = 0.9211\n",
            "epoch 511 / 2000, step 32/63, loss = 0.7487\n",
            "epoch 511 / 2000, step 48/63, loss = 0.9530\n",
            "epoch 512 / 2000, step 16/63, loss = 0.9605\n",
            "epoch 512 / 2000, step 32/63, loss = 0.8396\n",
            "epoch 512 / 2000, step 48/63, loss = 0.9197\n",
            "epoch 513 / 2000, step 16/63, loss = 0.7216\n",
            "epoch 513 / 2000, step 32/63, loss = 0.7960\n",
            "epoch 513 / 2000, step 48/63, loss = 0.8332\n",
            "epoch 514 / 2000, step 16/63, loss = 1.0340\n",
            "epoch 514 / 2000, step 32/63, loss = 0.7510\n",
            "epoch 514 / 2000, step 48/63, loss = 0.8348\n",
            "epoch 515 / 2000, step 16/63, loss = 0.8278\n",
            "epoch 515 / 2000, step 32/63, loss = 0.9002\n",
            "epoch 515 / 2000, step 48/63, loss = 0.9020\n",
            "epoch 516 / 2000, step 16/63, loss = 0.9234\n",
            "epoch 516 / 2000, step 32/63, loss = 0.8956\n",
            "epoch 516 / 2000, step 48/63, loss = 0.7287\n",
            "epoch 517 / 2000, step 16/63, loss = 0.8693\n",
            "epoch 517 / 2000, step 32/63, loss = 0.8499\n",
            "epoch 517 / 2000, step 48/63, loss = 0.8960\n",
            "epoch 518 / 2000, step 16/63, loss = 0.8311\n",
            "epoch 518 / 2000, step 32/63, loss = 0.8265\n",
            "epoch 518 / 2000, step 48/63, loss = 0.8049\n",
            "epoch 519 / 2000, step 16/63, loss = 0.9632\n",
            "epoch 519 / 2000, step 32/63, loss = 0.7259\n",
            "epoch 519 / 2000, step 48/63, loss = 0.8677\n",
            "epoch 520 / 2000, step 16/63, loss = 0.9556\n",
            "epoch 520 / 2000, step 32/63, loss = 0.8055\n",
            "epoch 520 / 2000, step 48/63, loss = 0.8188\n",
            "epoch 521 / 2000, step 16/63, loss = 0.9882\n",
            "epoch 521 / 2000, step 32/63, loss = 0.7539\n",
            "epoch 521 / 2000, step 48/63, loss = 0.8382\n",
            "epoch 522 / 2000, step 16/63, loss = 0.8390\n",
            "epoch 522 / 2000, step 32/63, loss = 0.7167\n",
            "epoch 522 / 2000, step 48/63, loss = 0.6902\n",
            "epoch 523 / 2000, step 16/63, loss = 0.8257\n",
            "epoch 523 / 2000, step 32/63, loss = 0.9328\n",
            "epoch 523 / 2000, step 48/63, loss = 0.8459\n",
            "epoch 524 / 2000, step 16/63, loss = 0.8967\n",
            "epoch 524 / 2000, step 32/63, loss = 0.9765\n",
            "epoch 524 / 2000, step 48/63, loss = 0.8267\n",
            "epoch 525 / 2000, step 16/63, loss = 0.8894\n",
            "epoch 525 / 2000, step 32/63, loss = 0.8044\n",
            "epoch 525 / 2000, step 48/63, loss = 0.9322\n",
            "epoch 526 / 2000, step 16/63, loss = 0.7648\n",
            "epoch 526 / 2000, step 32/63, loss = 0.8886\n",
            "epoch 526 / 2000, step 48/63, loss = 0.8110\n",
            "epoch 527 / 2000, step 16/63, loss = 0.8793\n",
            "epoch 527 / 2000, step 32/63, loss = 0.7738\n",
            "epoch 527 / 2000, step 48/63, loss = 0.9235\n",
            "epoch 528 / 2000, step 16/63, loss = 0.8911\n",
            "epoch 528 / 2000, step 32/63, loss = 0.7779\n",
            "epoch 528 / 2000, step 48/63, loss = 0.8417\n",
            "epoch 529 / 2000, step 16/63, loss = 0.8117\n",
            "epoch 529 / 2000, step 32/63, loss = 0.9933\n",
            "epoch 529 / 2000, step 48/63, loss = 0.8467\n",
            "epoch 530 / 2000, step 16/63, loss = 0.8252\n",
            "epoch 530 / 2000, step 32/63, loss = 0.7779\n",
            "epoch 530 / 2000, step 48/63, loss = 0.9488\n",
            "epoch 531 / 2000, step 16/63, loss = 0.9061\n",
            "epoch 531 / 2000, step 32/63, loss = 0.8219\n",
            "epoch 531 / 2000, step 48/63, loss = 0.9157\n",
            "epoch 532 / 2000, step 16/63, loss = 0.9181\n",
            "epoch 532 / 2000, step 32/63, loss = 0.6918\n",
            "epoch 532 / 2000, step 48/63, loss = 0.7692\n",
            "epoch 533 / 2000, step 16/63, loss = 1.1607\n",
            "epoch 533 / 2000, step 32/63, loss = 0.8022\n",
            "epoch 533 / 2000, step 48/63, loss = 0.9997\n",
            "epoch 534 / 2000, step 16/63, loss = 1.0672\n",
            "epoch 534 / 2000, step 32/63, loss = 0.9455\n",
            "epoch 534 / 2000, step 48/63, loss = 0.9909\n",
            "epoch 535 / 2000, step 16/63, loss = 0.9316\n",
            "epoch 535 / 2000, step 32/63, loss = 0.9524\n",
            "epoch 535 / 2000, step 48/63, loss = 0.8528\n",
            "epoch 536 / 2000, step 16/63, loss = 0.6777\n",
            "epoch 536 / 2000, step 32/63, loss = 0.7638\n",
            "epoch 536 / 2000, step 48/63, loss = 0.8408\n",
            "epoch 537 / 2000, step 16/63, loss = 0.9969\n",
            "epoch 537 / 2000, step 32/63, loss = 0.9122\n",
            "epoch 537 / 2000, step 48/63, loss = 0.8789\n",
            "epoch 538 / 2000, step 16/63, loss = 1.0095\n",
            "epoch 538 / 2000, step 32/63, loss = 1.0708\n",
            "epoch 538 / 2000, step 48/63, loss = 0.8665\n",
            "epoch 539 / 2000, step 16/63, loss = 0.6991\n",
            "epoch 539 / 2000, step 32/63, loss = 0.9196\n",
            "epoch 539 / 2000, step 48/63, loss = 0.8709\n",
            "epoch 540 / 2000, step 16/63, loss = 0.7318\n",
            "epoch 540 / 2000, step 32/63, loss = 0.7553\n",
            "epoch 540 / 2000, step 48/63, loss = 1.0752\n",
            "epoch 541 / 2000, step 16/63, loss = 0.8658\n",
            "epoch 541 / 2000, step 32/63, loss = 0.8635\n",
            "epoch 541 / 2000, step 48/63, loss = 1.0425\n",
            "epoch 542 / 2000, step 16/63, loss = 0.8586\n",
            "epoch 542 / 2000, step 32/63, loss = 0.9632\n",
            "epoch 542 / 2000, step 48/63, loss = 1.0016\n",
            "epoch 543 / 2000, step 16/63, loss = 0.8514\n",
            "epoch 543 / 2000, step 32/63, loss = 1.0108\n",
            "epoch 543 / 2000, step 48/63, loss = 0.8388\n",
            "epoch 544 / 2000, step 16/63, loss = 0.9132\n",
            "epoch 544 / 2000, step 32/63, loss = 0.8815\n",
            "epoch 544 / 2000, step 48/63, loss = 0.9564\n",
            "epoch 545 / 2000, step 16/63, loss = 0.8475\n",
            "epoch 545 / 2000, step 32/63, loss = 0.6252\n",
            "epoch 545 / 2000, step 48/63, loss = 0.7961\n",
            "epoch 546 / 2000, step 16/63, loss = 0.7770\n",
            "epoch 546 / 2000, step 32/63, loss = 0.8614\n",
            "epoch 546 / 2000, step 48/63, loss = 0.7950\n",
            "epoch 547 / 2000, step 16/63, loss = 0.7758\n",
            "epoch 547 / 2000, step 32/63, loss = 0.9011\n",
            "epoch 547 / 2000, step 48/63, loss = 0.9735\n",
            "epoch 548 / 2000, step 16/63, loss = 1.0648\n",
            "epoch 548 / 2000, step 32/63, loss = 0.7977\n",
            "epoch 548 / 2000, step 48/63, loss = 0.8277\n",
            "epoch 549 / 2000, step 16/63, loss = 0.7019\n",
            "epoch 549 / 2000, step 32/63, loss = 0.8178\n",
            "epoch 549 / 2000, step 48/63, loss = 0.6812\n",
            "epoch 550 / 2000, step 16/63, loss = 0.8737\n",
            "epoch 550 / 2000, step 32/63, loss = 0.9677\n",
            "epoch 550 / 2000, step 48/63, loss = 0.9228\n",
            "epoch 551 / 2000, step 16/63, loss = 0.8531\n",
            "epoch 551 / 2000, step 32/63, loss = 0.9846\n",
            "epoch 551 / 2000, step 48/63, loss = 0.7789\n",
            "epoch 552 / 2000, step 16/63, loss = 0.9360\n",
            "epoch 552 / 2000, step 32/63, loss = 0.8556\n",
            "epoch 552 / 2000, step 48/63, loss = 1.1013\n",
            "epoch 553 / 2000, step 16/63, loss = 0.9189\n",
            "epoch 553 / 2000, step 32/63, loss = 0.7894\n",
            "epoch 553 / 2000, step 48/63, loss = 0.8945\n",
            "epoch 554 / 2000, step 16/63, loss = 1.0437\n",
            "epoch 554 / 2000, step 32/63, loss = 0.7215\n",
            "epoch 554 / 2000, step 48/63, loss = 0.6237\n",
            "epoch 555 / 2000, step 16/63, loss = 0.9868\n",
            "epoch 555 / 2000, step 32/63, loss = 0.9015\n",
            "epoch 555 / 2000, step 48/63, loss = 0.7998\n",
            "epoch 556 / 2000, step 16/63, loss = 0.9840\n",
            "epoch 556 / 2000, step 32/63, loss = 0.9149\n",
            "epoch 556 / 2000, step 48/63, loss = 0.8785\n",
            "epoch 557 / 2000, step 16/63, loss = 0.8236\n",
            "epoch 557 / 2000, step 32/63, loss = 0.9029\n",
            "epoch 557 / 2000, step 48/63, loss = 0.8476\n",
            "epoch 558 / 2000, step 16/63, loss = 0.7604\n",
            "epoch 558 / 2000, step 32/63, loss = 0.8424\n",
            "epoch 558 / 2000, step 48/63, loss = 0.9178\n",
            "epoch 559 / 2000, step 16/63, loss = 0.7828\n",
            "epoch 559 / 2000, step 32/63, loss = 1.0058\n",
            "epoch 559 / 2000, step 48/63, loss = 0.7243\n",
            "epoch 560 / 2000, step 16/63, loss = 0.7434\n",
            "epoch 560 / 2000, step 32/63, loss = 0.7728\n",
            "epoch 560 / 2000, step 48/63, loss = 1.0172\n",
            "epoch 561 / 2000, step 16/63, loss = 0.7553\n",
            "epoch 561 / 2000, step 32/63, loss = 0.9286\n",
            "epoch 561 / 2000, step 48/63, loss = 0.8437\n",
            "epoch 562 / 2000, step 16/63, loss = 0.9269\n",
            "epoch 562 / 2000, step 32/63, loss = 0.7122\n",
            "epoch 562 / 2000, step 48/63, loss = 0.8847\n",
            "epoch 563 / 2000, step 16/63, loss = 0.7640\n",
            "epoch 563 / 2000, step 32/63, loss = 0.7911\n",
            "epoch 563 / 2000, step 48/63, loss = 0.9104\n",
            "epoch 564 / 2000, step 16/63, loss = 0.8569\n",
            "epoch 564 / 2000, step 32/63, loss = 0.6862\n",
            "epoch 564 / 2000, step 48/63, loss = 0.8616\n",
            "epoch 565 / 2000, step 16/63, loss = 0.9882\n",
            "epoch 565 / 2000, step 32/63, loss = 0.7695\n",
            "epoch 565 / 2000, step 48/63, loss = 0.8685\n",
            "epoch 566 / 2000, step 16/63, loss = 0.8395\n",
            "epoch 566 / 2000, step 32/63, loss = 0.9829\n",
            "epoch 566 / 2000, step 48/63, loss = 0.8889\n",
            "epoch 567 / 2000, step 16/63, loss = 0.8576\n",
            "epoch 567 / 2000, step 32/63, loss = 1.1355\n",
            "epoch 567 / 2000, step 48/63, loss = 0.9076\n",
            "epoch 568 / 2000, step 16/63, loss = 0.7741\n",
            "epoch 568 / 2000, step 32/63, loss = 0.8984\n",
            "epoch 568 / 2000, step 48/63, loss = 0.9656\n",
            "epoch 569 / 2000, step 16/63, loss = 0.8596\n",
            "epoch 569 / 2000, step 32/63, loss = 0.7452\n",
            "epoch 569 / 2000, step 48/63, loss = 0.9118\n",
            "epoch 570 / 2000, step 16/63, loss = 0.9088\n",
            "epoch 570 / 2000, step 32/63, loss = 0.8804\n",
            "epoch 570 / 2000, step 48/63, loss = 0.8449\n",
            "epoch 571 / 2000, step 16/63, loss = 1.0249\n",
            "epoch 571 / 2000, step 32/63, loss = 0.9241\n",
            "epoch 571 / 2000, step 48/63, loss = 0.8549\n",
            "epoch 572 / 2000, step 16/63, loss = 0.8171\n",
            "epoch 572 / 2000, step 32/63, loss = 0.7387\n",
            "epoch 572 / 2000, step 48/63, loss = 0.7345\n",
            "epoch 573 / 2000, step 16/63, loss = 0.9820\n",
            "epoch 573 / 2000, step 32/63, loss = 0.8580\n",
            "epoch 573 / 2000, step 48/63, loss = 0.9227\n",
            "epoch 574 / 2000, step 16/63, loss = 0.8670\n",
            "epoch 574 / 2000, step 32/63, loss = 0.8933\n",
            "epoch 574 / 2000, step 48/63, loss = 0.8367\n",
            "epoch 575 / 2000, step 16/63, loss = 0.7863\n",
            "epoch 575 / 2000, step 32/63, loss = 0.8056\n",
            "epoch 575 / 2000, step 48/63, loss = 0.8216\n",
            "epoch 576 / 2000, step 16/63, loss = 0.9574\n",
            "epoch 576 / 2000, step 32/63, loss = 0.8528\n",
            "epoch 576 / 2000, step 48/63, loss = 0.8988\n",
            "epoch 577 / 2000, step 16/63, loss = 0.8589\n",
            "epoch 577 / 2000, step 32/63, loss = 0.9069\n",
            "epoch 577 / 2000, step 48/63, loss = 0.9343\n",
            "epoch 578 / 2000, step 16/63, loss = 0.9152\n",
            "epoch 578 / 2000, step 32/63, loss = 0.7839\n",
            "epoch 578 / 2000, step 48/63, loss = 0.9305\n",
            "epoch 579 / 2000, step 16/63, loss = 0.9879\n",
            "epoch 579 / 2000, step 32/63, loss = 0.7499\n",
            "epoch 579 / 2000, step 48/63, loss = 1.0225\n",
            "epoch 580 / 2000, step 16/63, loss = 1.1853\n",
            "epoch 580 / 2000, step 32/63, loss = 0.8464\n",
            "epoch 580 / 2000, step 48/63, loss = 0.7560\n",
            "epoch 581 / 2000, step 16/63, loss = 0.6886\n",
            "epoch 581 / 2000, step 32/63, loss = 0.7677\n",
            "epoch 581 / 2000, step 48/63, loss = 0.7679\n",
            "epoch 582 / 2000, step 16/63, loss = 0.9870\n",
            "epoch 582 / 2000, step 32/63, loss = 0.8346\n",
            "epoch 582 / 2000, step 48/63, loss = 0.8167\n",
            "epoch 583 / 2000, step 16/63, loss = 0.8463\n",
            "epoch 583 / 2000, step 32/63, loss = 0.8293\n",
            "epoch 583 / 2000, step 48/63, loss = 0.7891\n",
            "epoch 584 / 2000, step 16/63, loss = 0.8206\n",
            "epoch 584 / 2000, step 32/63, loss = 0.8574\n",
            "epoch 584 / 2000, step 48/63, loss = 0.7404\n",
            "epoch 585 / 2000, step 16/63, loss = 0.7735\n",
            "epoch 585 / 2000, step 32/63, loss = 0.8850\n",
            "epoch 585 / 2000, step 48/63, loss = 0.8469\n",
            "epoch 586 / 2000, step 16/63, loss = 0.7488\n",
            "epoch 586 / 2000, step 32/63, loss = 0.8895\n",
            "epoch 586 / 2000, step 48/63, loss = 0.7901\n",
            "epoch 587 / 2000, step 16/63, loss = 0.7356\n",
            "epoch 587 / 2000, step 32/63, loss = 0.8751\n",
            "epoch 587 / 2000, step 48/63, loss = 0.8828\n",
            "epoch 588 / 2000, step 16/63, loss = 0.6453\n",
            "epoch 588 / 2000, step 32/63, loss = 0.8799\n",
            "epoch 588 / 2000, step 48/63, loss = 0.8858\n",
            "epoch 589 / 2000, step 16/63, loss = 0.9120\n",
            "epoch 589 / 2000, step 32/63, loss = 0.7554\n",
            "epoch 589 / 2000, step 48/63, loss = 0.8361\n",
            "epoch 590 / 2000, step 16/63, loss = 0.8092\n",
            "epoch 590 / 2000, step 32/63, loss = 1.0634\n",
            "epoch 590 / 2000, step 48/63, loss = 0.7125\n",
            "epoch 591 / 2000, step 16/63, loss = 0.8376\n",
            "epoch 591 / 2000, step 32/63, loss = 0.6990\n",
            "epoch 591 / 2000, step 48/63, loss = 0.8901\n",
            "epoch 592 / 2000, step 16/63, loss = 0.7059\n",
            "epoch 592 / 2000, step 32/63, loss = 1.0708\n",
            "epoch 592 / 2000, step 48/63, loss = 0.7436\n",
            "epoch 593 / 2000, step 16/63, loss = 0.9593\n",
            "epoch 593 / 2000, step 32/63, loss = 0.8156\n",
            "epoch 593 / 2000, step 48/63, loss = 0.9038\n",
            "epoch 594 / 2000, step 16/63, loss = 0.8193\n",
            "epoch 594 / 2000, step 32/63, loss = 0.8360\n",
            "epoch 594 / 2000, step 48/63, loss = 0.7199\n",
            "epoch 595 / 2000, step 16/63, loss = 0.6938\n",
            "epoch 595 / 2000, step 32/63, loss = 0.7577\n",
            "epoch 595 / 2000, step 48/63, loss = 0.8957\n",
            "epoch 596 / 2000, step 16/63, loss = 0.8245\n",
            "epoch 596 / 2000, step 32/63, loss = 0.9235\n",
            "epoch 596 / 2000, step 48/63, loss = 0.7413\n",
            "epoch 597 / 2000, step 16/63, loss = 0.8110\n",
            "epoch 597 / 2000, step 32/63, loss = 0.6238\n",
            "epoch 597 / 2000, step 48/63, loss = 0.6804\n",
            "epoch 598 / 2000, step 16/63, loss = 1.0715\n",
            "epoch 598 / 2000, step 32/63, loss = 0.9014\n",
            "epoch 598 / 2000, step 48/63, loss = 0.7766\n",
            "epoch 599 / 2000, step 16/63, loss = 0.7711\n",
            "epoch 599 / 2000, step 32/63, loss = 0.9741\n",
            "epoch 599 / 2000, step 48/63, loss = 0.8548\n",
            "epoch 600 / 2000, step 16/63, loss = 0.7301\n",
            "epoch 600 / 2000, step 32/63, loss = 0.7027\n",
            "epoch 600 / 2000, step 48/63, loss = 0.9207\n",
            "epoch 601 / 2000, step 16/63, loss = 0.9060\n",
            "epoch 601 / 2000, step 32/63, loss = 0.9189\n",
            "epoch 601 / 2000, step 48/63, loss = 0.8637\n",
            "epoch 602 / 2000, step 16/63, loss = 0.9004\n",
            "epoch 602 / 2000, step 32/63, loss = 0.8933\n",
            "epoch 602 / 2000, step 48/63, loss = 0.8533\n",
            "epoch 603 / 2000, step 16/63, loss = 0.8318\n",
            "epoch 603 / 2000, step 32/63, loss = 0.8504\n",
            "epoch 603 / 2000, step 48/63, loss = 0.9329\n",
            "epoch 604 / 2000, step 16/63, loss = 0.6630\n",
            "epoch 604 / 2000, step 32/63, loss = 0.8328\n",
            "epoch 604 / 2000, step 48/63, loss = 0.8051\n",
            "epoch 605 / 2000, step 16/63, loss = 0.7260\n",
            "epoch 605 / 2000, step 32/63, loss = 1.0433\n",
            "epoch 605 / 2000, step 48/63, loss = 0.9330\n",
            "epoch 606 / 2000, step 16/63, loss = 0.7927\n",
            "epoch 606 / 2000, step 32/63, loss = 0.6322\n",
            "epoch 606 / 2000, step 48/63, loss = 0.7682\n",
            "epoch 607 / 2000, step 16/63, loss = 1.0147\n",
            "epoch 607 / 2000, step 32/63, loss = 0.6320\n",
            "epoch 607 / 2000, step 48/63, loss = 0.7003\n",
            "epoch 608 / 2000, step 16/63, loss = 0.8448\n",
            "epoch 608 / 2000, step 32/63, loss = 1.0067\n",
            "epoch 608 / 2000, step 48/63, loss = 0.7909\n",
            "epoch 609 / 2000, step 16/63, loss = 0.6972\n",
            "epoch 609 / 2000, step 32/63, loss = 0.7858\n",
            "epoch 609 / 2000, step 48/63, loss = 0.8449\n",
            "epoch 610 / 2000, step 16/63, loss = 0.8873\n",
            "epoch 610 / 2000, step 32/63, loss = 0.8834\n",
            "epoch 610 / 2000, step 48/63, loss = 1.0276\n",
            "epoch 611 / 2000, step 16/63, loss = 0.8891\n",
            "epoch 611 / 2000, step 32/63, loss = 0.8327\n",
            "epoch 611 / 2000, step 48/63, loss = 0.6783\n",
            "epoch 612 / 2000, step 16/63, loss = 0.6393\n",
            "epoch 612 / 2000, step 32/63, loss = 1.0070\n",
            "epoch 612 / 2000, step 48/63, loss = 0.7821\n",
            "epoch 613 / 2000, step 16/63, loss = 0.8197\n",
            "epoch 613 / 2000, step 32/63, loss = 0.9382\n",
            "epoch 613 / 2000, step 48/63, loss = 0.9546\n",
            "epoch 614 / 2000, step 16/63, loss = 0.7826\n",
            "epoch 614 / 2000, step 32/63, loss = 0.6843\n",
            "epoch 614 / 2000, step 48/63, loss = 0.9050\n",
            "epoch 615 / 2000, step 16/63, loss = 0.7735\n",
            "epoch 615 / 2000, step 32/63, loss = 0.7195\n",
            "epoch 615 / 2000, step 48/63, loss = 0.7075\n",
            "epoch 616 / 2000, step 16/63, loss = 0.8755\n",
            "epoch 616 / 2000, step 32/63, loss = 0.7882\n",
            "epoch 616 / 2000, step 48/63, loss = 0.7922\n",
            "epoch 617 / 2000, step 16/63, loss = 0.8686\n",
            "epoch 617 / 2000, step 32/63, loss = 0.6701\n",
            "epoch 617 / 2000, step 48/63, loss = 0.9402\n",
            "epoch 618 / 2000, step 16/63, loss = 0.7611\n",
            "epoch 618 / 2000, step 32/63, loss = 0.8740\n",
            "epoch 618 / 2000, step 48/63, loss = 0.7663\n",
            "epoch 619 / 2000, step 16/63, loss = 0.7833\n",
            "epoch 619 / 2000, step 32/63, loss = 0.8066\n",
            "epoch 619 / 2000, step 48/63, loss = 0.7193\n",
            "epoch 620 / 2000, step 16/63, loss = 0.7978\n",
            "epoch 620 / 2000, step 32/63, loss = 0.6793\n",
            "epoch 620 / 2000, step 48/63, loss = 0.7027\n",
            "epoch 621 / 2000, step 16/63, loss = 0.6772\n",
            "epoch 621 / 2000, step 32/63, loss = 0.7318\n",
            "epoch 621 / 2000, step 48/63, loss = 0.6610\n",
            "epoch 622 / 2000, step 16/63, loss = 0.8934\n",
            "epoch 622 / 2000, step 32/63, loss = 0.8115\n",
            "epoch 622 / 2000, step 48/63, loss = 0.8485\n",
            "epoch 623 / 2000, step 16/63, loss = 0.8152\n",
            "epoch 623 / 2000, step 32/63, loss = 0.9786\n",
            "epoch 623 / 2000, step 48/63, loss = 0.7651\n",
            "epoch 624 / 2000, step 16/63, loss = 1.0048\n",
            "epoch 624 / 2000, step 32/63, loss = 0.6144\n",
            "epoch 624 / 2000, step 48/63, loss = 0.7717\n",
            "epoch 625 / 2000, step 16/63, loss = 0.8654\n",
            "epoch 625 / 2000, step 32/63, loss = 0.8284\n",
            "epoch 625 / 2000, step 48/63, loss = 0.8608\n",
            "epoch 626 / 2000, step 16/63, loss = 0.8951\n",
            "epoch 626 / 2000, step 32/63, loss = 0.8810\n",
            "epoch 626 / 2000, step 48/63, loss = 1.0044\n",
            "epoch 627 / 2000, step 16/63, loss = 0.8831\n",
            "epoch 627 / 2000, step 32/63, loss = 0.7113\n",
            "epoch 627 / 2000, step 48/63, loss = 0.9866\n",
            "epoch 628 / 2000, step 16/63, loss = 0.7352\n",
            "epoch 628 / 2000, step 32/63, loss = 0.7570\n",
            "epoch 628 / 2000, step 48/63, loss = 0.7853\n",
            "epoch 629 / 2000, step 16/63, loss = 0.8419\n",
            "epoch 629 / 2000, step 32/63, loss = 0.8873\n",
            "epoch 629 / 2000, step 48/63, loss = 0.7157\n",
            "epoch 630 / 2000, step 16/63, loss = 0.8032\n",
            "epoch 630 / 2000, step 32/63, loss = 0.5941\n",
            "epoch 630 / 2000, step 48/63, loss = 0.7953\n",
            "epoch 631 / 2000, step 16/63, loss = 0.7345\n",
            "epoch 631 / 2000, step 32/63, loss = 0.8446\n",
            "epoch 631 / 2000, step 48/63, loss = 0.7175\n",
            "epoch 632 / 2000, step 16/63, loss = 0.8979\n",
            "epoch 632 / 2000, step 32/63, loss = 0.7985\n",
            "epoch 632 / 2000, step 48/63, loss = 0.8640\n",
            "epoch 633 / 2000, step 16/63, loss = 0.9077\n",
            "epoch 633 / 2000, step 32/63, loss = 0.8283\n",
            "epoch 633 / 2000, step 48/63, loss = 0.9476\n",
            "epoch 634 / 2000, step 16/63, loss = 0.7036\n",
            "epoch 634 / 2000, step 32/63, loss = 0.8674\n",
            "epoch 634 / 2000, step 48/63, loss = 0.9472\n",
            "epoch 635 / 2000, step 16/63, loss = 0.7184\n",
            "epoch 635 / 2000, step 32/63, loss = 0.8866\n",
            "epoch 635 / 2000, step 48/63, loss = 0.8845\n",
            "epoch 636 / 2000, step 16/63, loss = 0.8210\n",
            "epoch 636 / 2000, step 32/63, loss = 0.7463\n",
            "epoch 636 / 2000, step 48/63, loss = 0.8713\n",
            "epoch 637 / 2000, step 16/63, loss = 0.9998\n",
            "epoch 637 / 2000, step 32/63, loss = 0.6502\n",
            "epoch 637 / 2000, step 48/63, loss = 0.7278\n",
            "epoch 638 / 2000, step 16/63, loss = 0.8408\n",
            "epoch 638 / 2000, step 32/63, loss = 0.7204\n",
            "epoch 638 / 2000, step 48/63, loss = 0.9716\n",
            "epoch 639 / 2000, step 16/63, loss = 0.6439\n",
            "epoch 639 / 2000, step 32/63, loss = 0.8244\n",
            "epoch 639 / 2000, step 48/63, loss = 0.9717\n",
            "epoch 640 / 2000, step 16/63, loss = 0.8917\n",
            "epoch 640 / 2000, step 32/63, loss = 0.8190\n",
            "epoch 640 / 2000, step 48/63, loss = 0.8987\n",
            "epoch 641 / 2000, step 16/63, loss = 0.7664\n",
            "epoch 641 / 2000, step 32/63, loss = 0.7583\n",
            "epoch 641 / 2000, step 48/63, loss = 0.7553\n",
            "epoch 642 / 2000, step 16/63, loss = 0.9784\n",
            "epoch 642 / 2000, step 32/63, loss = 0.9684\n",
            "epoch 642 / 2000, step 48/63, loss = 0.7565\n",
            "epoch 643 / 2000, step 16/63, loss = 0.8619\n",
            "epoch 643 / 2000, step 32/63, loss = 0.8825\n",
            "epoch 643 / 2000, step 48/63, loss = 0.8158\n",
            "epoch 644 / 2000, step 16/63, loss = 1.0200\n",
            "epoch 644 / 2000, step 32/63, loss = 0.9065\n",
            "epoch 644 / 2000, step 48/63, loss = 0.6995\n",
            "epoch 645 / 2000, step 16/63, loss = 0.9431\n",
            "epoch 645 / 2000, step 32/63, loss = 0.7823\n",
            "epoch 645 / 2000, step 48/63, loss = 1.1767\n",
            "epoch 646 / 2000, step 16/63, loss = 0.7921\n",
            "epoch 646 / 2000, step 32/63, loss = 0.8095\n",
            "epoch 646 / 2000, step 48/63, loss = 0.7614\n",
            "epoch 647 / 2000, step 16/63, loss = 0.6454\n",
            "epoch 647 / 2000, step 32/63, loss = 0.7563\n",
            "epoch 647 / 2000, step 48/63, loss = 1.1524\n",
            "epoch 648 / 2000, step 16/63, loss = 0.7937\n",
            "epoch 648 / 2000, step 32/63, loss = 0.7813\n",
            "epoch 648 / 2000, step 48/63, loss = 1.0407\n",
            "epoch 649 / 2000, step 16/63, loss = 0.7430\n",
            "epoch 649 / 2000, step 32/63, loss = 0.6502\n",
            "epoch 649 / 2000, step 48/63, loss = 0.8761\n",
            "epoch 650 / 2000, step 16/63, loss = 0.8393\n",
            "epoch 650 / 2000, step 32/63, loss = 0.8256\n",
            "epoch 650 / 2000, step 48/63, loss = 0.8320\n",
            "epoch 651 / 2000, step 16/63, loss = 0.6687\n",
            "epoch 651 / 2000, step 32/63, loss = 0.7070\n",
            "epoch 651 / 2000, step 48/63, loss = 0.9011\n",
            "epoch 652 / 2000, step 16/63, loss = 0.7329\n",
            "epoch 652 / 2000, step 32/63, loss = 0.6987\n",
            "epoch 652 / 2000, step 48/63, loss = 0.8117\n",
            "epoch 653 / 2000, step 16/63, loss = 0.8415\n",
            "epoch 653 / 2000, step 32/63, loss = 0.6371\n",
            "epoch 653 / 2000, step 48/63, loss = 0.7579\n",
            "epoch 654 / 2000, step 16/63, loss = 0.8095\n",
            "epoch 654 / 2000, step 32/63, loss = 0.9504\n",
            "epoch 654 / 2000, step 48/63, loss = 0.7591\n",
            "epoch 655 / 2000, step 16/63, loss = 0.7555\n",
            "epoch 655 / 2000, step 32/63, loss = 0.8269\n",
            "epoch 655 / 2000, step 48/63, loss = 0.6631\n",
            "epoch 656 / 2000, step 16/63, loss = 0.6734\n",
            "epoch 656 / 2000, step 32/63, loss = 0.9611\n",
            "epoch 656 / 2000, step 48/63, loss = 0.8026\n",
            "epoch 657 / 2000, step 16/63, loss = 0.6547\n",
            "epoch 657 / 2000, step 32/63, loss = 0.7546\n",
            "epoch 657 / 2000, step 48/63, loss = 1.0094\n",
            "epoch 658 / 2000, step 16/63, loss = 0.9050\n",
            "epoch 658 / 2000, step 32/63, loss = 0.8387\n",
            "epoch 658 / 2000, step 48/63, loss = 1.0050\n",
            "epoch 659 / 2000, step 16/63, loss = 1.0249\n",
            "epoch 659 / 2000, step 32/63, loss = 0.7475\n",
            "epoch 659 / 2000, step 48/63, loss = 0.7138\n",
            "epoch 660 / 2000, step 16/63, loss = 0.7461\n",
            "epoch 660 / 2000, step 32/63, loss = 0.7396\n",
            "epoch 660 / 2000, step 48/63, loss = 0.7075\n",
            "epoch 661 / 2000, step 16/63, loss = 0.9639\n",
            "epoch 661 / 2000, step 32/63, loss = 0.7059\n",
            "epoch 661 / 2000, step 48/63, loss = 0.7774\n",
            "epoch 662 / 2000, step 16/63, loss = 0.6579\n",
            "epoch 662 / 2000, step 32/63, loss = 0.7932\n",
            "epoch 662 / 2000, step 48/63, loss = 0.6821\n",
            "epoch 663 / 2000, step 16/63, loss = 0.7174\n",
            "epoch 663 / 2000, step 32/63, loss = 0.7525\n",
            "epoch 663 / 2000, step 48/63, loss = 0.8042\n",
            "epoch 664 / 2000, step 16/63, loss = 0.7819\n",
            "epoch 664 / 2000, step 32/63, loss = 1.0806\n",
            "epoch 664 / 2000, step 48/63, loss = 0.7922\n",
            "epoch 665 / 2000, step 16/63, loss = 0.8771\n",
            "epoch 665 / 2000, step 32/63, loss = 0.9233\n",
            "epoch 665 / 2000, step 48/63, loss = 0.7900\n",
            "epoch 666 / 2000, step 16/63, loss = 0.8653\n",
            "epoch 666 / 2000, step 32/63, loss = 0.6438\n",
            "epoch 666 / 2000, step 48/63, loss = 0.7863\n",
            "epoch 667 / 2000, step 16/63, loss = 0.8478\n",
            "epoch 667 / 2000, step 32/63, loss = 0.9089\n",
            "epoch 667 / 2000, step 48/63, loss = 0.9350\n",
            "epoch 668 / 2000, step 16/63, loss = 0.6973\n",
            "epoch 668 / 2000, step 32/63, loss = 0.6986\n",
            "epoch 668 / 2000, step 48/63, loss = 0.7405\n",
            "epoch 669 / 2000, step 16/63, loss = 0.8234\n",
            "epoch 669 / 2000, step 32/63, loss = 0.6786\n",
            "epoch 669 / 2000, step 48/63, loss = 0.6795\n",
            "epoch 670 / 2000, step 16/63, loss = 0.8857\n",
            "epoch 670 / 2000, step 32/63, loss = 0.7205\n",
            "epoch 670 / 2000, step 48/63, loss = 0.8861\n",
            "epoch 671 / 2000, step 16/63, loss = 0.8722\n",
            "epoch 671 / 2000, step 32/63, loss = 0.7064\n",
            "epoch 671 / 2000, step 48/63, loss = 0.6695\n",
            "epoch 672 / 2000, step 16/63, loss = 0.9602\n",
            "epoch 672 / 2000, step 32/63, loss = 0.8039\n",
            "epoch 672 / 2000, step 48/63, loss = 0.7394\n",
            "epoch 673 / 2000, step 16/63, loss = 0.7305\n",
            "epoch 673 / 2000, step 32/63, loss = 0.7283\n",
            "epoch 673 / 2000, step 48/63, loss = 0.8329\n",
            "epoch 674 / 2000, step 16/63, loss = 1.0202\n",
            "epoch 674 / 2000, step 32/63, loss = 0.8230\n",
            "epoch 674 / 2000, step 48/63, loss = 0.8845\n",
            "epoch 675 / 2000, step 16/63, loss = 0.8011\n",
            "epoch 675 / 2000, step 32/63, loss = 0.8100\n",
            "epoch 675 / 2000, step 48/63, loss = 0.8084\n",
            "epoch 676 / 2000, step 16/63, loss = 0.6769\n",
            "epoch 676 / 2000, step 32/63, loss = 0.7463\n",
            "epoch 676 / 2000, step 48/63, loss = 0.8889\n",
            "epoch 677 / 2000, step 16/63, loss = 0.7174\n",
            "epoch 677 / 2000, step 32/63, loss = 0.8920\n",
            "epoch 677 / 2000, step 48/63, loss = 0.7092\n",
            "epoch 678 / 2000, step 16/63, loss = 0.8501\n",
            "epoch 678 / 2000, step 32/63, loss = 1.0302\n",
            "epoch 678 / 2000, step 48/63, loss = 0.8217\n",
            "epoch 679 / 2000, step 16/63, loss = 0.8203\n",
            "epoch 679 / 2000, step 32/63, loss = 0.9567\n",
            "epoch 679 / 2000, step 48/63, loss = 0.7990\n",
            "epoch 680 / 2000, step 16/63, loss = 0.8754\n",
            "epoch 680 / 2000, step 32/63, loss = 0.9043\n",
            "epoch 680 / 2000, step 48/63, loss = 0.7992\n",
            "epoch 681 / 2000, step 16/63, loss = 0.9430\n",
            "epoch 681 / 2000, step 32/63, loss = 0.7285\n",
            "epoch 681 / 2000, step 48/63, loss = 1.0239\n",
            "epoch 682 / 2000, step 16/63, loss = 0.6696\n",
            "epoch 682 / 2000, step 32/63, loss = 0.7166\n",
            "epoch 682 / 2000, step 48/63, loss = 0.7255\n",
            "epoch 683 / 2000, step 16/63, loss = 0.9541\n",
            "epoch 683 / 2000, step 32/63, loss = 0.9222\n",
            "epoch 683 / 2000, step 48/63, loss = 0.6501\n",
            "epoch 684 / 2000, step 16/63, loss = 0.6736\n",
            "epoch 684 / 2000, step 32/63, loss = 0.7769\n",
            "epoch 684 / 2000, step 48/63, loss = 0.8115\n",
            "epoch 685 / 2000, step 16/63, loss = 0.6982\n",
            "epoch 685 / 2000, step 32/63, loss = 0.7081\n",
            "epoch 685 / 2000, step 48/63, loss = 0.6106\n",
            "epoch 686 / 2000, step 16/63, loss = 0.8262\n",
            "epoch 686 / 2000, step 32/63, loss = 0.8599\n",
            "epoch 686 / 2000, step 48/63, loss = 0.6615\n",
            "epoch 687 / 2000, step 16/63, loss = 0.9392\n",
            "epoch 687 / 2000, step 32/63, loss = 0.7994\n",
            "epoch 687 / 2000, step 48/63, loss = 0.6533\n",
            "epoch 688 / 2000, step 16/63, loss = 0.6594\n",
            "epoch 688 / 2000, step 32/63, loss = 0.6992\n",
            "epoch 688 / 2000, step 48/63, loss = 0.7632\n",
            "epoch 689 / 2000, step 16/63, loss = 0.9424\n",
            "epoch 689 / 2000, step 32/63, loss = 0.7675\n",
            "epoch 689 / 2000, step 48/63, loss = 0.7376\n",
            "epoch 690 / 2000, step 16/63, loss = 0.7421\n",
            "epoch 690 / 2000, step 32/63, loss = 0.8631\n",
            "epoch 690 / 2000, step 48/63, loss = 0.7375\n",
            "epoch 691 / 2000, step 16/63, loss = 0.8795\n",
            "epoch 691 / 2000, step 32/63, loss = 0.7917\n",
            "epoch 691 / 2000, step 48/63, loss = 0.7918\n",
            "epoch 692 / 2000, step 16/63, loss = 0.6527\n",
            "epoch 692 / 2000, step 32/63, loss = 1.0314\n",
            "epoch 692 / 2000, step 48/63, loss = 0.6543\n",
            "epoch 693 / 2000, step 16/63, loss = 1.0491\n",
            "epoch 693 / 2000, step 32/63, loss = 0.6824\n",
            "epoch 693 / 2000, step 48/63, loss = 0.8962\n",
            "epoch 694 / 2000, step 16/63, loss = 0.8786\n",
            "epoch 694 / 2000, step 32/63, loss = 0.8103\n",
            "epoch 694 / 2000, step 48/63, loss = 1.0557\n",
            "epoch 695 / 2000, step 16/63, loss = 0.8665\n",
            "epoch 695 / 2000, step 32/63, loss = 0.7069\n",
            "epoch 695 / 2000, step 48/63, loss = 0.8565\n",
            "epoch 696 / 2000, step 16/63, loss = 0.8440\n",
            "epoch 696 / 2000, step 32/63, loss = 0.7242\n",
            "epoch 696 / 2000, step 48/63, loss = 0.7424\n",
            "epoch 697 / 2000, step 16/63, loss = 0.7988\n",
            "epoch 697 / 2000, step 32/63, loss = 0.7394\n",
            "epoch 697 / 2000, step 48/63, loss = 0.8408\n",
            "epoch 698 / 2000, step 16/63, loss = 0.6758\n",
            "epoch 698 / 2000, step 32/63, loss = 0.8153\n",
            "epoch 698 / 2000, step 48/63, loss = 0.7605\n",
            "epoch 699 / 2000, step 16/63, loss = 0.6023\n",
            "epoch 699 / 2000, step 32/63, loss = 0.5609\n",
            "epoch 699 / 2000, step 48/63, loss = 0.6956\n",
            "epoch 700 / 2000, step 16/63, loss = 0.7139\n",
            "epoch 700 / 2000, step 32/63, loss = 0.8568\n",
            "epoch 700 / 2000, step 48/63, loss = 0.7761\n",
            "epoch 701 / 2000, step 16/63, loss = 0.6883\n",
            "epoch 701 / 2000, step 32/63, loss = 0.6790\n",
            "epoch 701 / 2000, step 48/63, loss = 0.9596\n",
            "epoch 702 / 2000, step 16/63, loss = 0.6595\n",
            "epoch 702 / 2000, step 32/63, loss = 0.7350\n",
            "epoch 702 / 2000, step 48/63, loss = 0.6542\n",
            "epoch 703 / 2000, step 16/63, loss = 0.5822\n",
            "epoch 703 / 2000, step 32/63, loss = 0.6444\n",
            "epoch 703 / 2000, step 48/63, loss = 0.7661\n",
            "epoch 704 / 2000, step 16/63, loss = 0.8194\n",
            "epoch 704 / 2000, step 32/63, loss = 0.6773\n",
            "epoch 704 / 2000, step 48/63, loss = 0.7790\n",
            "epoch 705 / 2000, step 16/63, loss = 1.0091\n",
            "epoch 705 / 2000, step 32/63, loss = 0.8509\n",
            "epoch 705 / 2000, step 48/63, loss = 0.8547\n",
            "epoch 706 / 2000, step 16/63, loss = 0.7747\n",
            "epoch 706 / 2000, step 32/63, loss = 0.8687\n",
            "epoch 706 / 2000, step 48/63, loss = 0.9121\n",
            "epoch 707 / 2000, step 16/63, loss = 0.7235\n",
            "epoch 707 / 2000, step 32/63, loss = 0.8212\n",
            "epoch 707 / 2000, step 48/63, loss = 0.7478\n",
            "epoch 708 / 2000, step 16/63, loss = 1.0533\n",
            "epoch 708 / 2000, step 32/63, loss = 0.9666\n",
            "epoch 708 / 2000, step 48/63, loss = 0.6318\n",
            "epoch 709 / 2000, step 16/63, loss = 0.8019\n",
            "epoch 709 / 2000, step 32/63, loss = 0.8849\n",
            "epoch 709 / 2000, step 48/63, loss = 0.6973\n",
            "epoch 710 / 2000, step 16/63, loss = 0.5754\n",
            "epoch 710 / 2000, step 32/63, loss = 0.7843\n",
            "epoch 710 / 2000, step 48/63, loss = 0.6826\n",
            "epoch 711 / 2000, step 16/63, loss = 0.8175\n",
            "epoch 711 / 2000, step 32/63, loss = 0.7746\n",
            "epoch 711 / 2000, step 48/63, loss = 0.8810\n",
            "epoch 712 / 2000, step 16/63, loss = 0.6688\n",
            "epoch 712 / 2000, step 32/63, loss = 0.8121\n",
            "epoch 712 / 2000, step 48/63, loss = 1.0028\n",
            "epoch 713 / 2000, step 16/63, loss = 0.8376\n",
            "epoch 713 / 2000, step 32/63, loss = 0.7363\n",
            "epoch 713 / 2000, step 48/63, loss = 0.8020\n",
            "epoch 714 / 2000, step 16/63, loss = 0.6583\n",
            "epoch 714 / 2000, step 32/63, loss = 0.7525\n",
            "epoch 714 / 2000, step 48/63, loss = 0.8500\n",
            "epoch 715 / 2000, step 16/63, loss = 0.6708\n",
            "epoch 715 / 2000, step 32/63, loss = 0.7412\n",
            "epoch 715 / 2000, step 48/63, loss = 0.6867\n",
            "epoch 716 / 2000, step 16/63, loss = 0.6522\n",
            "epoch 716 / 2000, step 32/63, loss = 0.8464\n",
            "epoch 716 / 2000, step 48/63, loss = 0.8092\n",
            "epoch 717 / 2000, step 16/63, loss = 0.6830\n",
            "epoch 717 / 2000, step 32/63, loss = 0.8476\n",
            "epoch 717 / 2000, step 48/63, loss = 0.8782\n",
            "epoch 718 / 2000, step 16/63, loss = 0.9762\n",
            "epoch 718 / 2000, step 32/63, loss = 0.6983\n",
            "epoch 718 / 2000, step 48/63, loss = 0.7650\n",
            "epoch 719 / 2000, step 16/63, loss = 0.7994\n",
            "epoch 719 / 2000, step 32/63, loss = 0.7596\n",
            "epoch 719 / 2000, step 48/63, loss = 0.7001\n",
            "epoch 720 / 2000, step 16/63, loss = 0.7480\n",
            "epoch 720 / 2000, step 32/63, loss = 0.8059\n",
            "epoch 720 / 2000, step 48/63, loss = 0.7382\n",
            "epoch 721 / 2000, step 16/63, loss = 0.7489\n",
            "epoch 721 / 2000, step 32/63, loss = 0.7832\n",
            "epoch 721 / 2000, step 48/63, loss = 0.7558\n",
            "epoch 722 / 2000, step 16/63, loss = 0.7621\n",
            "epoch 722 / 2000, step 32/63, loss = 0.7349\n",
            "epoch 722 / 2000, step 48/63, loss = 0.6203\n",
            "epoch 723 / 2000, step 16/63, loss = 0.5442\n",
            "epoch 723 / 2000, step 32/63, loss = 0.7898\n",
            "epoch 723 / 2000, step 48/63, loss = 0.6557\n",
            "epoch 724 / 2000, step 16/63, loss = 0.8501\n",
            "epoch 724 / 2000, step 32/63, loss = 0.6707\n",
            "epoch 724 / 2000, step 48/63, loss = 0.6549\n",
            "epoch 725 / 2000, step 16/63, loss = 0.7384\n",
            "epoch 725 / 2000, step 32/63, loss = 0.7005\n",
            "epoch 725 / 2000, step 48/63, loss = 0.8323\n",
            "epoch 726 / 2000, step 16/63, loss = 0.8692\n",
            "epoch 726 / 2000, step 32/63, loss = 0.8379\n",
            "epoch 726 / 2000, step 48/63, loss = 0.6385\n",
            "epoch 727 / 2000, step 16/63, loss = 0.6997\n",
            "epoch 727 / 2000, step 32/63, loss = 0.8106\n",
            "epoch 727 / 2000, step 48/63, loss = 0.8508\n",
            "epoch 728 / 2000, step 16/63, loss = 0.6123\n",
            "epoch 728 / 2000, step 32/63, loss = 0.7837\n",
            "epoch 728 / 2000, step 48/63, loss = 0.9337\n",
            "epoch 729 / 2000, step 16/63, loss = 0.8360\n",
            "epoch 729 / 2000, step 32/63, loss = 0.6338\n",
            "epoch 729 / 2000, step 48/63, loss = 0.6145\n",
            "epoch 730 / 2000, step 16/63, loss = 0.7080\n",
            "epoch 730 / 2000, step 32/63, loss = 0.9632\n",
            "epoch 730 / 2000, step 48/63, loss = 0.7127\n",
            "epoch 731 / 2000, step 16/63, loss = 0.8267\n",
            "epoch 731 / 2000, step 32/63, loss = 0.7771\n",
            "epoch 731 / 2000, step 48/63, loss = 0.7611\n",
            "epoch 732 / 2000, step 16/63, loss = 0.8091\n",
            "epoch 732 / 2000, step 32/63, loss = 0.9948\n",
            "epoch 732 / 2000, step 48/63, loss = 0.6794\n",
            "epoch 733 / 2000, step 16/63, loss = 0.7371\n",
            "epoch 733 / 2000, step 32/63, loss = 0.8315\n",
            "epoch 733 / 2000, step 48/63, loss = 0.8912\n",
            "epoch 734 / 2000, step 16/63, loss = 0.7473\n",
            "epoch 734 / 2000, step 32/63, loss = 0.9887\n",
            "epoch 734 / 2000, step 48/63, loss = 0.7283\n",
            "epoch 735 / 2000, step 16/63, loss = 0.7854\n",
            "epoch 735 / 2000, step 32/63, loss = 0.6646\n",
            "epoch 735 / 2000, step 48/63, loss = 0.8740\n",
            "epoch 736 / 2000, step 16/63, loss = 0.7626\n",
            "epoch 736 / 2000, step 32/63, loss = 0.8258\n",
            "epoch 736 / 2000, step 48/63, loss = 0.7274\n",
            "epoch 737 / 2000, step 16/63, loss = 0.9269\n",
            "epoch 737 / 2000, step 32/63, loss = 0.6788\n",
            "epoch 737 / 2000, step 48/63, loss = 0.7244\n",
            "epoch 738 / 2000, step 16/63, loss = 0.5730\n",
            "epoch 738 / 2000, step 32/63, loss = 0.8059\n",
            "epoch 738 / 2000, step 48/63, loss = 0.7896\n",
            "epoch 739 / 2000, step 16/63, loss = 0.9072\n",
            "epoch 739 / 2000, step 32/63, loss = 0.5135\n",
            "epoch 739 / 2000, step 48/63, loss = 0.6808\n",
            "epoch 740 / 2000, step 16/63, loss = 0.6601\n",
            "epoch 740 / 2000, step 32/63, loss = 0.7147\n",
            "epoch 740 / 2000, step 48/63, loss = 0.6441\n",
            "epoch 741 / 2000, step 16/63, loss = 0.8726\n",
            "epoch 741 / 2000, step 32/63, loss = 0.8225\n",
            "epoch 741 / 2000, step 48/63, loss = 0.6310\n",
            "epoch 742 / 2000, step 16/63, loss = 0.8796\n",
            "epoch 742 / 2000, step 32/63, loss = 0.6633\n",
            "epoch 742 / 2000, step 48/63, loss = 0.7244\n",
            "epoch 743 / 2000, step 16/63, loss = 0.5697\n",
            "epoch 743 / 2000, step 32/63, loss = 0.9494\n",
            "epoch 743 / 2000, step 48/63, loss = 0.6088\n",
            "epoch 744 / 2000, step 16/63, loss = 0.7677\n",
            "epoch 744 / 2000, step 32/63, loss = 0.6353\n",
            "epoch 744 / 2000, step 48/63, loss = 0.5407\n",
            "epoch 745 / 2000, step 16/63, loss = 0.7401\n",
            "epoch 745 / 2000, step 32/63, loss = 0.6847\n",
            "epoch 745 / 2000, step 48/63, loss = 0.9558\n",
            "epoch 746 / 2000, step 16/63, loss = 0.6802\n",
            "epoch 746 / 2000, step 32/63, loss = 0.6433\n",
            "epoch 746 / 2000, step 48/63, loss = 0.7900\n",
            "epoch 747 / 2000, step 16/63, loss = 0.9033\n",
            "epoch 747 / 2000, step 32/63, loss = 0.7811\n",
            "epoch 747 / 2000, step 48/63, loss = 0.8736\n",
            "epoch 748 / 2000, step 16/63, loss = 0.7100\n",
            "epoch 748 / 2000, step 32/63, loss = 0.6809\n",
            "epoch 748 / 2000, step 48/63, loss = 0.7034\n",
            "epoch 749 / 2000, step 16/63, loss = 0.7831\n",
            "epoch 749 / 2000, step 32/63, loss = 0.6415\n",
            "epoch 749 / 2000, step 48/63, loss = 0.7028\n",
            "epoch 750 / 2000, step 16/63, loss = 0.8242\n",
            "epoch 750 / 2000, step 32/63, loss = 0.8801\n",
            "epoch 750 / 2000, step 48/63, loss = 0.7101\n",
            "epoch 751 / 2000, step 16/63, loss = 0.5838\n",
            "epoch 751 / 2000, step 32/63, loss = 0.5311\n",
            "epoch 751 / 2000, step 48/63, loss = 0.6078\n",
            "epoch 752 / 2000, step 16/63, loss = 0.8156\n",
            "epoch 752 / 2000, step 32/63, loss = 0.9064\n",
            "epoch 752 / 2000, step 48/63, loss = 0.9026\n",
            "epoch 753 / 2000, step 16/63, loss = 0.8721\n",
            "epoch 753 / 2000, step 32/63, loss = 0.6522\n",
            "epoch 753 / 2000, step 48/63, loss = 0.6476\n",
            "epoch 754 / 2000, step 16/63, loss = 0.7297\n",
            "epoch 754 / 2000, step 32/63, loss = 0.9849\n",
            "epoch 754 / 2000, step 48/63, loss = 0.8425\n",
            "epoch 755 / 2000, step 16/63, loss = 0.7275\n",
            "epoch 755 / 2000, step 32/63, loss = 0.7295\n",
            "epoch 755 / 2000, step 48/63, loss = 0.8437\n",
            "epoch 756 / 2000, step 16/63, loss = 0.6549\n",
            "epoch 756 / 2000, step 32/63, loss = 0.8317\n",
            "epoch 756 / 2000, step 48/63, loss = 0.8471\n",
            "epoch 757 / 2000, step 16/63, loss = 0.8264\n",
            "epoch 757 / 2000, step 32/63, loss = 0.7772\n",
            "epoch 757 / 2000, step 48/63, loss = 0.8012\n",
            "epoch 758 / 2000, step 16/63, loss = 0.7560\n",
            "epoch 758 / 2000, step 32/63, loss = 0.9547\n",
            "epoch 758 / 2000, step 48/63, loss = 0.7645\n",
            "epoch 759 / 2000, step 16/63, loss = 0.8166\n",
            "epoch 759 / 2000, step 32/63, loss = 0.5909\n",
            "epoch 759 / 2000, step 48/63, loss = 0.7348\n",
            "epoch 760 / 2000, step 16/63, loss = 0.6436\n",
            "epoch 760 / 2000, step 32/63, loss = 0.6780\n",
            "epoch 760 / 2000, step 48/63, loss = 0.6040\n",
            "epoch 761 / 2000, step 16/63, loss = 0.6994\n",
            "epoch 761 / 2000, step 32/63, loss = 0.5371\n",
            "epoch 761 / 2000, step 48/63, loss = 0.7578\n",
            "epoch 762 / 2000, step 16/63, loss = 0.8494\n",
            "epoch 762 / 2000, step 32/63, loss = 0.8385\n",
            "epoch 762 / 2000, step 48/63, loss = 0.6931\n",
            "epoch 763 / 2000, step 16/63, loss = 0.8239\n",
            "epoch 763 / 2000, step 32/63, loss = 0.9069\n",
            "epoch 763 / 2000, step 48/63, loss = 0.6786\n",
            "epoch 764 / 2000, step 16/63, loss = 0.9354\n",
            "epoch 764 / 2000, step 32/63, loss = 0.7263\n",
            "epoch 764 / 2000, step 48/63, loss = 0.9051\n",
            "epoch 765 / 2000, step 16/63, loss = 0.6796\n",
            "epoch 765 / 2000, step 32/63, loss = 0.7508\n",
            "epoch 765 / 2000, step 48/63, loss = 0.4469\n",
            "epoch 766 / 2000, step 16/63, loss = 0.7738\n",
            "epoch 766 / 2000, step 32/63, loss = 0.5806\n",
            "epoch 766 / 2000, step 48/63, loss = 0.8163\n",
            "epoch 767 / 2000, step 16/63, loss = 0.9790\n",
            "epoch 767 / 2000, step 32/63, loss = 0.8869\n",
            "epoch 767 / 2000, step 48/63, loss = 0.6540\n",
            "epoch 768 / 2000, step 16/63, loss = 1.0108\n",
            "epoch 768 / 2000, step 32/63, loss = 0.6784\n",
            "epoch 768 / 2000, step 48/63, loss = 0.7482\n",
            "epoch 769 / 2000, step 16/63, loss = 0.8348\n",
            "epoch 769 / 2000, step 32/63, loss = 0.5510\n",
            "epoch 769 / 2000, step 48/63, loss = 0.6929\n",
            "epoch 770 / 2000, step 16/63, loss = 0.6879\n",
            "epoch 770 / 2000, step 32/63, loss = 0.8487\n",
            "epoch 770 / 2000, step 48/63, loss = 0.8005\n",
            "epoch 771 / 2000, step 16/63, loss = 0.8442\n",
            "epoch 771 / 2000, step 32/63, loss = 0.8257\n",
            "epoch 771 / 2000, step 48/63, loss = 0.7009\n",
            "epoch 772 / 2000, step 16/63, loss = 0.8632\n",
            "epoch 772 / 2000, step 32/63, loss = 0.7547\n",
            "epoch 772 / 2000, step 48/63, loss = 0.7250\n",
            "epoch 773 / 2000, step 16/63, loss = 0.6930\n",
            "epoch 773 / 2000, step 32/63, loss = 0.8254\n",
            "epoch 773 / 2000, step 48/63, loss = 0.9833\n",
            "epoch 774 / 2000, step 16/63, loss = 0.9814\n",
            "epoch 774 / 2000, step 32/63, loss = 0.7386\n",
            "epoch 774 / 2000, step 48/63, loss = 0.8414\n",
            "epoch 775 / 2000, step 16/63, loss = 0.6988\n",
            "epoch 775 / 2000, step 32/63, loss = 0.7033\n",
            "epoch 775 / 2000, step 48/63, loss = 0.6987\n",
            "epoch 776 / 2000, step 16/63, loss = 0.6388\n",
            "epoch 776 / 2000, step 32/63, loss = 0.5075\n",
            "epoch 776 / 2000, step 48/63, loss = 0.7104\n",
            "epoch 777 / 2000, step 16/63, loss = 0.8586\n",
            "epoch 777 / 2000, step 32/63, loss = 0.9033\n",
            "epoch 777 / 2000, step 48/63, loss = 0.7787\n",
            "epoch 778 / 2000, step 16/63, loss = 1.1613\n",
            "epoch 778 / 2000, step 32/63, loss = 0.5922\n",
            "epoch 778 / 2000, step 48/63, loss = 0.8694\n",
            "epoch 779 / 2000, step 16/63, loss = 0.6130\n",
            "epoch 779 / 2000, step 32/63, loss = 0.5851\n",
            "epoch 779 / 2000, step 48/63, loss = 0.7675\n",
            "epoch 780 / 2000, step 16/63, loss = 0.6581\n",
            "epoch 780 / 2000, step 32/63, loss = 0.8149\n",
            "epoch 780 / 2000, step 48/63, loss = 1.0480\n",
            "epoch 781 / 2000, step 16/63, loss = 0.7938\n",
            "epoch 781 / 2000, step 32/63, loss = 0.9808\n",
            "epoch 781 / 2000, step 48/63, loss = 0.8150\n",
            "epoch 782 / 2000, step 16/63, loss = 0.7518\n",
            "epoch 782 / 2000, step 32/63, loss = 0.6235\n",
            "epoch 782 / 2000, step 48/63, loss = 0.6493\n",
            "epoch 783 / 2000, step 16/63, loss = 0.6293\n",
            "epoch 783 / 2000, step 32/63, loss = 0.6055\n",
            "epoch 783 / 2000, step 48/63, loss = 0.7476\n",
            "epoch 784 / 2000, step 16/63, loss = 0.8186\n",
            "epoch 784 / 2000, step 32/63, loss = 0.6782\n",
            "epoch 784 / 2000, step 48/63, loss = 0.9293\n",
            "epoch 785 / 2000, step 16/63, loss = 0.6721\n",
            "epoch 785 / 2000, step 32/63, loss = 0.7862\n",
            "epoch 785 / 2000, step 48/63, loss = 0.6353\n",
            "epoch 786 / 2000, step 16/63, loss = 0.8494\n",
            "epoch 786 / 2000, step 32/63, loss = 0.7109\n",
            "epoch 786 / 2000, step 48/63, loss = 0.7315\n",
            "epoch 787 / 2000, step 16/63, loss = 0.6216\n",
            "epoch 787 / 2000, step 32/63, loss = 0.6801\n",
            "epoch 787 / 2000, step 48/63, loss = 0.7218\n",
            "epoch 788 / 2000, step 16/63, loss = 0.8809\n",
            "epoch 788 / 2000, step 32/63, loss = 0.7046\n",
            "epoch 788 / 2000, step 48/63, loss = 0.7780\n",
            "epoch 789 / 2000, step 16/63, loss = 0.5032\n",
            "epoch 789 / 2000, step 32/63, loss = 0.6660\n",
            "epoch 789 / 2000, step 48/63, loss = 0.7999\n",
            "epoch 790 / 2000, step 16/63, loss = 0.6858\n",
            "epoch 790 / 2000, step 32/63, loss = 0.6645\n",
            "epoch 790 / 2000, step 48/63, loss = 0.7989\n",
            "epoch 791 / 2000, step 16/63, loss = 0.8097\n",
            "epoch 791 / 2000, step 32/63, loss = 0.7647\n",
            "epoch 791 / 2000, step 48/63, loss = 0.6318\n",
            "epoch 792 / 2000, step 16/63, loss = 0.8500\n",
            "epoch 792 / 2000, step 32/63, loss = 0.8760\n",
            "epoch 792 / 2000, step 48/63, loss = 0.7053\n",
            "epoch 793 / 2000, step 16/63, loss = 0.9709\n",
            "epoch 793 / 2000, step 32/63, loss = 0.8002\n",
            "epoch 793 / 2000, step 48/63, loss = 0.6014\n",
            "epoch 794 / 2000, step 16/63, loss = 0.4961\n",
            "epoch 794 / 2000, step 32/63, loss = 0.7888\n",
            "epoch 794 / 2000, step 48/63, loss = 0.5964\n",
            "epoch 795 / 2000, step 16/63, loss = 0.7017\n",
            "epoch 795 / 2000, step 32/63, loss = 0.6675\n",
            "epoch 795 / 2000, step 48/63, loss = 0.6863\n",
            "epoch 796 / 2000, step 16/63, loss = 0.6572\n",
            "epoch 796 / 2000, step 32/63, loss = 0.7081\n",
            "epoch 796 / 2000, step 48/63, loss = 0.6453\n",
            "epoch 797 / 2000, step 16/63, loss = 0.6633\n",
            "epoch 797 / 2000, step 32/63, loss = 0.6638\n",
            "epoch 797 / 2000, step 48/63, loss = 0.8075\n",
            "epoch 798 / 2000, step 16/63, loss = 0.8907\n",
            "epoch 798 / 2000, step 32/63, loss = 0.5066\n",
            "epoch 798 / 2000, step 48/63, loss = 0.7627\n",
            "epoch 799 / 2000, step 16/63, loss = 0.6830\n",
            "epoch 799 / 2000, step 32/63, loss = 0.8760\n",
            "epoch 799 / 2000, step 48/63, loss = 0.6411\n",
            "epoch 800 / 2000, step 16/63, loss = 0.9276\n",
            "epoch 800 / 2000, step 32/63, loss = 0.6788\n",
            "epoch 800 / 2000, step 48/63, loss = 0.4969\n",
            "epoch 801 / 2000, step 16/63, loss = 0.8416\n",
            "epoch 801 / 2000, step 32/63, loss = 0.7562\n",
            "epoch 801 / 2000, step 48/63, loss = 0.8321\n",
            "epoch 802 / 2000, step 16/63, loss = 0.8555\n",
            "epoch 802 / 2000, step 32/63, loss = 0.7363\n",
            "epoch 802 / 2000, step 48/63, loss = 0.7705\n",
            "epoch 803 / 2000, step 16/63, loss = 0.6464\n",
            "epoch 803 / 2000, step 32/63, loss = 0.8093\n",
            "epoch 803 / 2000, step 48/63, loss = 0.5804\n",
            "epoch 804 / 2000, step 16/63, loss = 0.6760\n",
            "epoch 804 / 2000, step 32/63, loss = 0.8315\n",
            "epoch 804 / 2000, step 48/63, loss = 0.7015\n",
            "epoch 805 / 2000, step 16/63, loss = 0.7806\n",
            "epoch 805 / 2000, step 32/63, loss = 0.8877\n",
            "epoch 805 / 2000, step 48/63, loss = 0.5900\n",
            "epoch 806 / 2000, step 16/63, loss = 0.6961\n",
            "epoch 806 / 2000, step 32/63, loss = 0.7621\n",
            "epoch 806 / 2000, step 48/63, loss = 0.7725\n",
            "epoch 807 / 2000, step 16/63, loss = 0.5846\n",
            "epoch 807 / 2000, step 32/63, loss = 0.7992\n",
            "epoch 807 / 2000, step 48/63, loss = 0.5863\n",
            "epoch 808 / 2000, step 16/63, loss = 0.8112\n",
            "epoch 808 / 2000, step 32/63, loss = 0.9859\n",
            "epoch 808 / 2000, step 48/63, loss = 0.6260\n",
            "epoch 809 / 2000, step 16/63, loss = 0.6025\n",
            "epoch 809 / 2000, step 32/63, loss = 0.8570\n",
            "epoch 809 / 2000, step 48/63, loss = 1.0255\n",
            "epoch 810 / 2000, step 16/63, loss = 0.6601\n",
            "epoch 810 / 2000, step 32/63, loss = 0.7415\n",
            "epoch 810 / 2000, step 48/63, loss = 0.6959\n",
            "epoch 811 / 2000, step 16/63, loss = 0.6601\n",
            "epoch 811 / 2000, step 32/63, loss = 0.8191\n",
            "epoch 811 / 2000, step 48/63, loss = 0.5708\n",
            "epoch 812 / 2000, step 16/63, loss = 0.6199\n",
            "epoch 812 / 2000, step 32/63, loss = 0.7019\n",
            "epoch 812 / 2000, step 48/63, loss = 0.6783\n",
            "epoch 813 / 2000, step 16/63, loss = 0.6230\n",
            "epoch 813 / 2000, step 32/63, loss = 0.4946\n",
            "epoch 813 / 2000, step 48/63, loss = 0.6552\n",
            "epoch 814 / 2000, step 16/63, loss = 0.6609\n",
            "epoch 814 / 2000, step 32/63, loss = 0.6091\n",
            "epoch 814 / 2000, step 48/63, loss = 0.8072\n",
            "epoch 815 / 2000, step 16/63, loss = 0.8057\n",
            "epoch 815 / 2000, step 32/63, loss = 0.7603\n",
            "epoch 815 / 2000, step 48/63, loss = 0.6854\n",
            "epoch 816 / 2000, step 16/63, loss = 0.7693\n",
            "epoch 816 / 2000, step 32/63, loss = 0.9167\n",
            "epoch 816 / 2000, step 48/63, loss = 0.7230\n",
            "epoch 817 / 2000, step 16/63, loss = 0.8116\n",
            "epoch 817 / 2000, step 32/63, loss = 0.7600\n",
            "epoch 817 / 2000, step 48/63, loss = 0.6338\n",
            "epoch 818 / 2000, step 16/63, loss = 0.6339\n",
            "epoch 818 / 2000, step 32/63, loss = 0.7734\n",
            "epoch 818 / 2000, step 48/63, loss = 0.7990\n",
            "epoch 819 / 2000, step 16/63, loss = 0.9871\n",
            "epoch 819 / 2000, step 32/63, loss = 0.8165\n",
            "epoch 819 / 2000, step 48/63, loss = 0.5338\n",
            "epoch 820 / 2000, step 16/63, loss = 0.7829\n",
            "epoch 820 / 2000, step 32/63, loss = 0.7419\n",
            "epoch 820 / 2000, step 48/63, loss = 0.6617\n",
            "epoch 821 / 2000, step 16/63, loss = 0.5062\n",
            "epoch 821 / 2000, step 32/63, loss = 0.6712\n",
            "epoch 821 / 2000, step 48/63, loss = 0.6219\n",
            "epoch 822 / 2000, step 16/63, loss = 0.5742\n",
            "epoch 822 / 2000, step 32/63, loss = 0.5797\n",
            "epoch 822 / 2000, step 48/63, loss = 0.5812\n",
            "epoch 823 / 2000, step 16/63, loss = 0.7457\n",
            "epoch 823 / 2000, step 32/63, loss = 0.6672\n",
            "epoch 823 / 2000, step 48/63, loss = 0.7677\n",
            "epoch 824 / 2000, step 16/63, loss = 0.8768\n",
            "epoch 824 / 2000, step 32/63, loss = 0.7761\n",
            "epoch 824 / 2000, step 48/63, loss = 0.8231\n",
            "epoch 825 / 2000, step 16/63, loss = 0.7217\n",
            "epoch 825 / 2000, step 32/63, loss = 0.8279\n",
            "epoch 825 / 2000, step 48/63, loss = 0.7204\n",
            "epoch 826 / 2000, step 16/63, loss = 0.6266\n",
            "epoch 826 / 2000, step 32/63, loss = 0.6207\n",
            "epoch 826 / 2000, step 48/63, loss = 0.6909\n",
            "epoch 827 / 2000, step 16/63, loss = 0.5989\n",
            "epoch 827 / 2000, step 32/63, loss = 0.7530\n",
            "epoch 827 / 2000, step 48/63, loss = 0.5719\n",
            "epoch 828 / 2000, step 16/63, loss = 0.8699\n",
            "epoch 828 / 2000, step 32/63, loss = 0.5731\n",
            "epoch 828 / 2000, step 48/63, loss = 0.6898\n",
            "epoch 829 / 2000, step 16/63, loss = 0.5860\n",
            "epoch 829 / 2000, step 32/63, loss = 0.7118\n",
            "epoch 829 / 2000, step 48/63, loss = 0.7489\n",
            "epoch 830 / 2000, step 16/63, loss = 0.5703\n",
            "epoch 830 / 2000, step 32/63, loss = 1.1010\n",
            "epoch 830 / 2000, step 48/63, loss = 0.5499\n",
            "epoch 831 / 2000, step 16/63, loss = 0.8164\n",
            "epoch 831 / 2000, step 32/63, loss = 0.6108\n",
            "epoch 831 / 2000, step 48/63, loss = 0.8420\n",
            "epoch 832 / 2000, step 16/63, loss = 0.5381\n",
            "epoch 832 / 2000, step 32/63, loss = 0.6002\n",
            "epoch 832 / 2000, step 48/63, loss = 0.9492\n",
            "epoch 833 / 2000, step 16/63, loss = 0.8007\n",
            "epoch 833 / 2000, step 32/63, loss = 0.5449\n",
            "epoch 833 / 2000, step 48/63, loss = 0.7176\n",
            "epoch 834 / 2000, step 16/63, loss = 0.6343\n",
            "epoch 834 / 2000, step 32/63, loss = 0.7417\n",
            "epoch 834 / 2000, step 48/63, loss = 0.9743\n",
            "epoch 835 / 2000, step 16/63, loss = 0.6669\n",
            "epoch 835 / 2000, step 32/63, loss = 0.6506\n",
            "epoch 835 / 2000, step 48/63, loss = 0.8300\n",
            "epoch 836 / 2000, step 16/63, loss = 0.6351\n",
            "epoch 836 / 2000, step 32/63, loss = 0.5225\n",
            "epoch 836 / 2000, step 48/63, loss = 0.9837\n",
            "epoch 837 / 2000, step 16/63, loss = 0.8189\n",
            "epoch 837 / 2000, step 32/63, loss = 0.6414\n",
            "epoch 837 / 2000, step 48/63, loss = 0.8605\n",
            "epoch 838 / 2000, step 16/63, loss = 0.7476\n",
            "epoch 838 / 2000, step 32/63, loss = 0.7213\n",
            "epoch 838 / 2000, step 48/63, loss = 0.7215\n",
            "epoch 839 / 2000, step 16/63, loss = 0.7588\n",
            "epoch 839 / 2000, step 32/63, loss = 0.5955\n",
            "epoch 839 / 2000, step 48/63, loss = 0.7355\n",
            "epoch 840 / 2000, step 16/63, loss = 0.7067\n",
            "epoch 840 / 2000, step 32/63, loss = 0.5688\n",
            "epoch 840 / 2000, step 48/63, loss = 0.6103\n",
            "epoch 841 / 2000, step 16/63, loss = 0.5633\n",
            "epoch 841 / 2000, step 32/63, loss = 0.5953\n",
            "epoch 841 / 2000, step 48/63, loss = 0.7385\n",
            "epoch 842 / 2000, step 16/63, loss = 0.6632\n",
            "epoch 842 / 2000, step 32/63, loss = 0.7957\n",
            "epoch 842 / 2000, step 48/63, loss = 0.9246\n",
            "epoch 843 / 2000, step 16/63, loss = 0.6922\n",
            "epoch 843 / 2000, step 32/63, loss = 0.8836\n",
            "epoch 843 / 2000, step 48/63, loss = 0.6672\n",
            "epoch 844 / 2000, step 16/63, loss = 0.6558\n",
            "epoch 844 / 2000, step 32/63, loss = 0.5993\n",
            "epoch 844 / 2000, step 48/63, loss = 0.6765\n",
            "epoch 845 / 2000, step 16/63, loss = 0.5884\n",
            "epoch 845 / 2000, step 32/63, loss = 0.6950\n",
            "epoch 845 / 2000, step 48/63, loss = 0.7690\n",
            "epoch 846 / 2000, step 16/63, loss = 0.6162\n",
            "epoch 846 / 2000, step 32/63, loss = 0.5360\n",
            "epoch 846 / 2000, step 48/63, loss = 0.7241\n",
            "epoch 847 / 2000, step 16/63, loss = 0.7146\n",
            "epoch 847 / 2000, step 32/63, loss = 0.7458\n",
            "epoch 847 / 2000, step 48/63, loss = 0.5983\n",
            "epoch 848 / 2000, step 16/63, loss = 0.7399\n",
            "epoch 848 / 2000, step 32/63, loss = 0.5419\n",
            "epoch 848 / 2000, step 48/63, loss = 0.8862\n",
            "epoch 849 / 2000, step 16/63, loss = 0.4935\n",
            "epoch 849 / 2000, step 32/63, loss = 0.7294\n",
            "epoch 849 / 2000, step 48/63, loss = 0.6307\n",
            "epoch 850 / 2000, step 16/63, loss = 0.5986\n",
            "epoch 850 / 2000, step 32/63, loss = 0.6477\n",
            "epoch 850 / 2000, step 48/63, loss = 0.7086\n",
            "epoch 851 / 2000, step 16/63, loss = 0.9578\n",
            "epoch 851 / 2000, step 32/63, loss = 0.5065\n",
            "epoch 851 / 2000, step 48/63, loss = 0.8009\n",
            "epoch 852 / 2000, step 16/63, loss = 0.8752\n",
            "epoch 852 / 2000, step 32/63, loss = 0.8761\n",
            "epoch 852 / 2000, step 48/63, loss = 0.8122\n",
            "epoch 853 / 2000, step 16/63, loss = 0.4739\n",
            "epoch 853 / 2000, step 32/63, loss = 0.9295\n",
            "epoch 853 / 2000, step 48/63, loss = 0.9073\n",
            "epoch 854 / 2000, step 16/63, loss = 0.9817\n",
            "epoch 854 / 2000, step 32/63, loss = 0.6084\n",
            "epoch 854 / 2000, step 48/63, loss = 0.5861\n",
            "epoch 855 / 2000, step 16/63, loss = 0.8068\n",
            "epoch 855 / 2000, step 32/63, loss = 0.7034\n",
            "epoch 855 / 2000, step 48/63, loss = 0.5121\n",
            "epoch 856 / 2000, step 16/63, loss = 0.7697\n",
            "epoch 856 / 2000, step 32/63, loss = 0.5749\n",
            "epoch 856 / 2000, step 48/63, loss = 0.5308\n",
            "epoch 857 / 2000, step 16/63, loss = 0.6704\n",
            "epoch 857 / 2000, step 32/63, loss = 0.7093\n",
            "epoch 857 / 2000, step 48/63, loss = 0.6266\n",
            "epoch 858 / 2000, step 16/63, loss = 0.5672\n",
            "epoch 858 / 2000, step 32/63, loss = 0.6242\n",
            "epoch 858 / 2000, step 48/63, loss = 0.6004\n",
            "epoch 859 / 2000, step 16/63, loss = 0.7111\n",
            "epoch 859 / 2000, step 32/63, loss = 0.6795\n",
            "epoch 859 / 2000, step 48/63, loss = 0.6004\n",
            "epoch 860 / 2000, step 16/63, loss = 0.6445\n",
            "epoch 860 / 2000, step 32/63, loss = 0.6426\n",
            "epoch 860 / 2000, step 48/63, loss = 0.7088\n",
            "epoch 861 / 2000, step 16/63, loss = 0.7781\n",
            "epoch 861 / 2000, step 32/63, loss = 0.6022\n",
            "epoch 861 / 2000, step 48/63, loss = 0.7122\n",
            "epoch 862 / 2000, step 16/63, loss = 0.7151\n",
            "epoch 862 / 2000, step 32/63, loss = 0.6413\n",
            "epoch 862 / 2000, step 48/63, loss = 0.7339\n",
            "epoch 863 / 2000, step 16/63, loss = 0.6701\n",
            "epoch 863 / 2000, step 32/63, loss = 0.6550\n",
            "epoch 863 / 2000, step 48/63, loss = 0.9917\n",
            "epoch 864 / 2000, step 16/63, loss = 0.6979\n",
            "epoch 864 / 2000, step 32/63, loss = 0.9144\n",
            "epoch 864 / 2000, step 48/63, loss = 0.5486\n",
            "epoch 865 / 2000, step 16/63, loss = 0.7224\n",
            "epoch 865 / 2000, step 32/63, loss = 0.6977\n",
            "epoch 865 / 2000, step 48/63, loss = 0.4908\n",
            "epoch 866 / 2000, step 16/63, loss = 0.6099\n",
            "epoch 866 / 2000, step 32/63, loss = 0.6576\n",
            "epoch 866 / 2000, step 48/63, loss = 0.5862\n",
            "epoch 867 / 2000, step 16/63, loss = 0.6425\n",
            "epoch 867 / 2000, step 32/63, loss = 0.9853\n",
            "epoch 867 / 2000, step 48/63, loss = 0.6253\n",
            "epoch 868 / 2000, step 16/63, loss = 0.6821\n",
            "epoch 868 / 2000, step 32/63, loss = 0.8014\n",
            "epoch 868 / 2000, step 48/63, loss = 0.9602\n",
            "epoch 869 / 2000, step 16/63, loss = 0.5624\n",
            "epoch 869 / 2000, step 32/63, loss = 0.7504\n",
            "epoch 869 / 2000, step 48/63, loss = 0.7262\n",
            "epoch 870 / 2000, step 16/63, loss = 0.5165\n",
            "epoch 870 / 2000, step 32/63, loss = 0.6637\n",
            "epoch 870 / 2000, step 48/63, loss = 0.7683\n",
            "epoch 871 / 2000, step 16/63, loss = 0.6847\n",
            "epoch 871 / 2000, step 32/63, loss = 0.5013\n",
            "epoch 871 / 2000, step 48/63, loss = 0.6001\n",
            "epoch 872 / 2000, step 16/63, loss = 0.5102\n",
            "epoch 872 / 2000, step 32/63, loss = 0.7069\n",
            "epoch 872 / 2000, step 48/63, loss = 0.5841\n",
            "epoch 873 / 2000, step 16/63, loss = 0.9008\n",
            "epoch 873 / 2000, step 32/63, loss = 0.7418\n",
            "epoch 873 / 2000, step 48/63, loss = 0.7777\n",
            "epoch 874 / 2000, step 16/63, loss = 0.6991\n",
            "epoch 874 / 2000, step 32/63, loss = 0.6838\n",
            "epoch 874 / 2000, step 48/63, loss = 0.6944\n",
            "epoch 875 / 2000, step 16/63, loss = 1.0459\n",
            "epoch 875 / 2000, step 32/63, loss = 0.8284\n",
            "epoch 875 / 2000, step 48/63, loss = 0.7847\n",
            "epoch 876 / 2000, step 16/63, loss = 0.7172\n",
            "epoch 876 / 2000, step 32/63, loss = 0.6638\n",
            "epoch 876 / 2000, step 48/63, loss = 0.7238\n",
            "epoch 877 / 2000, step 16/63, loss = 0.5873\n",
            "epoch 877 / 2000, step 32/63, loss = 0.9350\n",
            "epoch 877 / 2000, step 48/63, loss = 0.5087\n",
            "epoch 878 / 2000, step 16/63, loss = 0.7210\n",
            "epoch 878 / 2000, step 32/63, loss = 0.7024\n",
            "epoch 878 / 2000, step 48/63, loss = 0.7594\n",
            "epoch 879 / 2000, step 16/63, loss = 0.5105\n",
            "epoch 879 / 2000, step 32/63, loss = 0.5462\n",
            "epoch 879 / 2000, step 48/63, loss = 0.5578\n",
            "epoch 880 / 2000, step 16/63, loss = 0.5887\n",
            "epoch 880 / 2000, step 32/63, loss = 0.7727\n",
            "epoch 880 / 2000, step 48/63, loss = 0.5784\n",
            "epoch 881 / 2000, step 16/63, loss = 0.7887\n",
            "epoch 881 / 2000, step 32/63, loss = 0.9753\n",
            "epoch 881 / 2000, step 48/63, loss = 0.7197\n",
            "epoch 882 / 2000, step 16/63, loss = 0.5806\n",
            "epoch 882 / 2000, step 32/63, loss = 0.7400\n",
            "epoch 882 / 2000, step 48/63, loss = 0.5997\n",
            "epoch 883 / 2000, step 16/63, loss = 0.7574\n",
            "epoch 883 / 2000, step 32/63, loss = 0.6971\n",
            "epoch 883 / 2000, step 48/63, loss = 0.6430\n",
            "epoch 884 / 2000, step 16/63, loss = 0.9151\n",
            "epoch 884 / 2000, step 32/63, loss = 0.5830\n",
            "epoch 884 / 2000, step 48/63, loss = 0.8276\n",
            "epoch 885 / 2000, step 16/63, loss = 0.8517\n",
            "epoch 885 / 2000, step 32/63, loss = 0.5361\n",
            "epoch 885 / 2000, step 48/63, loss = 0.8657\n",
            "epoch 886 / 2000, step 16/63, loss = 0.5664\n",
            "epoch 886 / 2000, step 32/63, loss = 0.7326\n",
            "epoch 886 / 2000, step 48/63, loss = 1.0226\n",
            "epoch 887 / 2000, step 16/63, loss = 0.7444\n",
            "epoch 887 / 2000, step 32/63, loss = 0.7492\n",
            "epoch 887 / 2000, step 48/63, loss = 0.6477\n",
            "epoch 888 / 2000, step 16/63, loss = 0.6303\n",
            "epoch 888 / 2000, step 32/63, loss = 0.6962\n",
            "epoch 888 / 2000, step 48/63, loss = 0.6234\n",
            "epoch 889 / 2000, step 16/63, loss = 0.6664\n",
            "epoch 889 / 2000, step 32/63, loss = 0.5206\n",
            "epoch 889 / 2000, step 48/63, loss = 1.0027\n",
            "epoch 890 / 2000, step 16/63, loss = 0.5589\n",
            "epoch 890 / 2000, step 32/63, loss = 0.7503\n",
            "epoch 890 / 2000, step 48/63, loss = 0.5413\n",
            "epoch 891 / 2000, step 16/63, loss = 0.8275\n",
            "epoch 891 / 2000, step 32/63, loss = 0.5666\n",
            "epoch 891 / 2000, step 48/63, loss = 0.8065\n",
            "epoch 892 / 2000, step 16/63, loss = 0.5402\n",
            "epoch 892 / 2000, step 32/63, loss = 0.7510\n",
            "epoch 892 / 2000, step 48/63, loss = 0.6808\n",
            "epoch 893 / 2000, step 16/63, loss = 0.5870\n",
            "epoch 893 / 2000, step 32/63, loss = 0.7339\n",
            "epoch 893 / 2000, step 48/63, loss = 0.7632\n",
            "epoch 894 / 2000, step 16/63, loss = 0.7939\n",
            "epoch 894 / 2000, step 32/63, loss = 0.6834\n",
            "epoch 894 / 2000, step 48/63, loss = 0.5057\n",
            "epoch 895 / 2000, step 16/63, loss = 0.8354\n",
            "epoch 895 / 2000, step 32/63, loss = 0.5896\n",
            "epoch 895 / 2000, step 48/63, loss = 0.5807\n",
            "epoch 896 / 2000, step 16/63, loss = 0.7092\n",
            "epoch 896 / 2000, step 32/63, loss = 0.7548\n",
            "epoch 896 / 2000, step 48/63, loss = 0.6574\n",
            "epoch 897 / 2000, step 16/63, loss = 0.7997\n",
            "epoch 897 / 2000, step 32/63, loss = 0.6272\n",
            "epoch 897 / 2000, step 48/63, loss = 0.7826\n",
            "epoch 898 / 2000, step 16/63, loss = 0.8582\n",
            "epoch 898 / 2000, step 32/63, loss = 0.7842\n",
            "epoch 898 / 2000, step 48/63, loss = 0.7463\n",
            "epoch 899 / 2000, step 16/63, loss = 0.8398\n",
            "epoch 899 / 2000, step 32/63, loss = 0.7161\n",
            "epoch 899 / 2000, step 48/63, loss = 0.7691\n",
            "epoch 900 / 2000, step 16/63, loss = 0.7498\n",
            "epoch 900 / 2000, step 32/63, loss = 0.8302\n",
            "epoch 900 / 2000, step 48/63, loss = 0.8846\n",
            "epoch 901 / 2000, step 16/63, loss = 0.5067\n",
            "epoch 901 / 2000, step 32/63, loss = 0.7627\n",
            "epoch 901 / 2000, step 48/63, loss = 0.5378\n",
            "epoch 902 / 2000, step 16/63, loss = 0.6092\n",
            "epoch 902 / 2000, step 32/63, loss = 0.5975\n",
            "epoch 902 / 2000, step 48/63, loss = 0.7861\n",
            "epoch 903 / 2000, step 16/63, loss = 0.8584\n",
            "epoch 903 / 2000, step 32/63, loss = 0.8822\n",
            "epoch 903 / 2000, step 48/63, loss = 0.5882\n",
            "epoch 904 / 2000, step 16/63, loss = 0.3606\n",
            "epoch 904 / 2000, step 32/63, loss = 0.6511\n",
            "epoch 904 / 2000, step 48/63, loss = 0.5711\n",
            "epoch 905 / 2000, step 16/63, loss = 0.6639\n",
            "epoch 905 / 2000, step 32/63, loss = 0.6067\n",
            "epoch 905 / 2000, step 48/63, loss = 0.6690\n",
            "epoch 906 / 2000, step 16/63, loss = 0.6395\n",
            "epoch 906 / 2000, step 32/63, loss = 0.8882\n",
            "epoch 906 / 2000, step 48/63, loss = 0.6515\n",
            "epoch 907 / 2000, step 16/63, loss = 0.9061\n",
            "epoch 907 / 2000, step 32/63, loss = 0.5769\n",
            "epoch 907 / 2000, step 48/63, loss = 0.6804\n",
            "epoch 908 / 2000, step 16/63, loss = 0.6455\n",
            "epoch 908 / 2000, step 32/63, loss = 0.5950\n",
            "epoch 908 / 2000, step 48/63, loss = 0.6757\n",
            "epoch 909 / 2000, step 16/63, loss = 0.5458\n",
            "epoch 909 / 2000, step 32/63, loss = 0.8071\n",
            "epoch 909 / 2000, step 48/63, loss = 0.7191\n",
            "epoch 910 / 2000, step 16/63, loss = 0.9231\n",
            "epoch 910 / 2000, step 32/63, loss = 0.5861\n",
            "epoch 910 / 2000, step 48/63, loss = 0.5969\n",
            "epoch 911 / 2000, step 16/63, loss = 0.7092\n",
            "epoch 911 / 2000, step 32/63, loss = 0.9323\n",
            "epoch 911 / 2000, step 48/63, loss = 0.7545\n",
            "epoch 912 / 2000, step 16/63, loss = 0.7669\n",
            "epoch 912 / 2000, step 32/63, loss = 0.6514\n",
            "epoch 912 / 2000, step 48/63, loss = 0.7910\n",
            "epoch 913 / 2000, step 16/63, loss = 0.5254\n",
            "epoch 913 / 2000, step 32/63, loss = 0.6981\n",
            "epoch 913 / 2000, step 48/63, loss = 0.5818\n",
            "epoch 914 / 2000, step 16/63, loss = 0.5235\n",
            "epoch 914 / 2000, step 32/63, loss = 0.6647\n",
            "epoch 914 / 2000, step 48/63, loss = 0.6117\n",
            "epoch 915 / 2000, step 16/63, loss = 0.7426\n",
            "epoch 915 / 2000, step 32/63, loss = 0.7511\n",
            "epoch 915 / 2000, step 48/63, loss = 0.7104\n",
            "epoch 916 / 2000, step 16/63, loss = 0.5669\n",
            "epoch 916 / 2000, step 32/63, loss = 0.6506\n",
            "epoch 916 / 2000, step 48/63, loss = 0.7062\n",
            "epoch 917 / 2000, step 16/63, loss = 0.6074\n",
            "epoch 917 / 2000, step 32/63, loss = 0.8340\n",
            "epoch 917 / 2000, step 48/63, loss = 0.6641\n",
            "epoch 918 / 2000, step 16/63, loss = 0.5746\n",
            "epoch 918 / 2000, step 32/63, loss = 1.0226\n",
            "epoch 918 / 2000, step 48/63, loss = 0.6539\n",
            "epoch 919 / 2000, step 16/63, loss = 0.5432\n",
            "epoch 919 / 2000, step 32/63, loss = 0.5435\n",
            "epoch 919 / 2000, step 48/63, loss = 0.7896\n",
            "epoch 920 / 2000, step 16/63, loss = 0.6606\n",
            "epoch 920 / 2000, step 32/63, loss = 0.7760\n",
            "epoch 920 / 2000, step 48/63, loss = 0.4419\n",
            "epoch 921 / 2000, step 16/63, loss = 0.6394\n",
            "epoch 921 / 2000, step 32/63, loss = 0.6820\n",
            "epoch 921 / 2000, step 48/63, loss = 1.0184\n",
            "epoch 922 / 2000, step 16/63, loss = 0.8481\n",
            "epoch 922 / 2000, step 32/63, loss = 0.5215\n",
            "epoch 922 / 2000, step 48/63, loss = 0.6784\n",
            "epoch 923 / 2000, step 16/63, loss = 0.7375\n",
            "epoch 923 / 2000, step 32/63, loss = 0.7472\n",
            "epoch 923 / 2000, step 48/63, loss = 0.6810\n",
            "epoch 924 / 2000, step 16/63, loss = 0.5950\n",
            "epoch 924 / 2000, step 32/63, loss = 0.6714\n",
            "epoch 924 / 2000, step 48/63, loss = 0.7220\n",
            "epoch 925 / 2000, step 16/63, loss = 0.7699\n",
            "epoch 925 / 2000, step 32/63, loss = 0.7039\n",
            "epoch 925 / 2000, step 48/63, loss = 0.6838\n",
            "epoch 926 / 2000, step 16/63, loss = 0.7251\n",
            "epoch 926 / 2000, step 32/63, loss = 0.6581\n",
            "epoch 926 / 2000, step 48/63, loss = 0.7485\n",
            "epoch 927 / 2000, step 16/63, loss = 0.9454\n",
            "epoch 927 / 2000, step 32/63, loss = 0.8174\n",
            "epoch 927 / 2000, step 48/63, loss = 0.8706\n",
            "epoch 928 / 2000, step 16/63, loss = 0.4698\n",
            "epoch 928 / 2000, step 32/63, loss = 0.7204\n",
            "epoch 928 / 2000, step 48/63, loss = 0.4900\n",
            "epoch 929 / 2000, step 16/63, loss = 0.4324\n",
            "epoch 929 / 2000, step 32/63, loss = 0.5916\n",
            "epoch 929 / 2000, step 48/63, loss = 0.6555\n",
            "epoch 930 / 2000, step 16/63, loss = 0.5799\n",
            "epoch 930 / 2000, step 32/63, loss = 0.5482\n",
            "epoch 930 / 2000, step 48/63, loss = 0.7564\n",
            "epoch 931 / 2000, step 16/63, loss = 0.7331\n",
            "epoch 931 / 2000, step 32/63, loss = 0.5308\n",
            "epoch 931 / 2000, step 48/63, loss = 0.6266\n",
            "epoch 932 / 2000, step 16/63, loss = 0.8372\n",
            "epoch 932 / 2000, step 32/63, loss = 0.5783\n",
            "epoch 932 / 2000, step 48/63, loss = 0.6274\n",
            "epoch 933 / 2000, step 16/63, loss = 0.7318\n",
            "epoch 933 / 2000, step 32/63, loss = 0.7573\n",
            "epoch 933 / 2000, step 48/63, loss = 0.8672\n",
            "epoch 934 / 2000, step 16/63, loss = 0.6404\n",
            "epoch 934 / 2000, step 32/63, loss = 0.6236\n",
            "epoch 934 / 2000, step 48/63, loss = 0.5937\n",
            "epoch 935 / 2000, step 16/63, loss = 0.7014\n",
            "epoch 935 / 2000, step 32/63, loss = 0.9150\n",
            "epoch 935 / 2000, step 48/63, loss = 0.7628\n",
            "epoch 936 / 2000, step 16/63, loss = 0.4921\n",
            "epoch 936 / 2000, step 32/63, loss = 0.8158\n",
            "epoch 936 / 2000, step 48/63, loss = 0.5407\n",
            "epoch 937 / 2000, step 16/63, loss = 0.7777\n",
            "epoch 937 / 2000, step 32/63, loss = 0.7169\n",
            "epoch 937 / 2000, step 48/63, loss = 0.6857\n",
            "epoch 938 / 2000, step 16/63, loss = 0.7349\n",
            "epoch 938 / 2000, step 32/63, loss = 0.4468\n",
            "epoch 938 / 2000, step 48/63, loss = 0.5799\n",
            "epoch 939 / 2000, step 16/63, loss = 0.5392\n",
            "epoch 939 / 2000, step 32/63, loss = 0.5322\n",
            "epoch 939 / 2000, step 48/63, loss = 1.0571\n",
            "epoch 940 / 2000, step 16/63, loss = 0.5572\n",
            "epoch 940 / 2000, step 32/63, loss = 0.3982\n",
            "epoch 940 / 2000, step 48/63, loss = 0.7922\n",
            "epoch 941 / 2000, step 16/63, loss = 0.4240\n",
            "epoch 941 / 2000, step 32/63, loss = 0.6074\n",
            "epoch 941 / 2000, step 48/63, loss = 0.7443\n",
            "epoch 942 / 2000, step 16/63, loss = 0.7640\n",
            "epoch 942 / 2000, step 32/63, loss = 0.6206\n",
            "epoch 942 / 2000, step 48/63, loss = 0.5230\n",
            "epoch 943 / 2000, step 16/63, loss = 0.6052\n",
            "epoch 943 / 2000, step 32/63, loss = 0.5029\n",
            "epoch 943 / 2000, step 48/63, loss = 0.7525\n",
            "epoch 944 / 2000, step 16/63, loss = 0.5610\n",
            "epoch 944 / 2000, step 32/63, loss = 0.5771\n",
            "epoch 944 / 2000, step 48/63, loss = 0.5881\n",
            "epoch 945 / 2000, step 16/63, loss = 0.7276\n",
            "epoch 945 / 2000, step 32/63, loss = 0.8284\n",
            "epoch 945 / 2000, step 48/63, loss = 0.6375\n",
            "epoch 946 / 2000, step 16/63, loss = 0.7792\n",
            "epoch 946 / 2000, step 32/63, loss = 0.6145\n",
            "epoch 946 / 2000, step 48/63, loss = 0.6275\n",
            "epoch 947 / 2000, step 16/63, loss = 0.5781\n",
            "epoch 947 / 2000, step 32/63, loss = 0.8273\n",
            "epoch 947 / 2000, step 48/63, loss = 0.4655\n",
            "epoch 948 / 2000, step 16/63, loss = 0.9297\n",
            "epoch 948 / 2000, step 32/63, loss = 0.6952\n",
            "epoch 948 / 2000, step 48/63, loss = 0.5597\n",
            "epoch 949 / 2000, step 16/63, loss = 0.5984\n",
            "epoch 949 / 2000, step 32/63, loss = 0.5247\n",
            "epoch 949 / 2000, step 48/63, loss = 0.6084\n",
            "epoch 950 / 2000, step 16/63, loss = 0.6374\n",
            "epoch 950 / 2000, step 32/63, loss = 0.7989\n",
            "epoch 950 / 2000, step 48/63, loss = 0.8528\n",
            "epoch 951 / 2000, step 16/63, loss = 0.5781\n",
            "epoch 951 / 2000, step 32/63, loss = 0.5187\n",
            "epoch 951 / 2000, step 48/63, loss = 0.6742\n",
            "epoch 952 / 2000, step 16/63, loss = 0.6321\n",
            "epoch 952 / 2000, step 32/63, loss = 0.7437\n",
            "epoch 952 / 2000, step 48/63, loss = 0.7758\n",
            "epoch 953 / 2000, step 16/63, loss = 0.6386\n",
            "epoch 953 / 2000, step 32/63, loss = 0.6820\n",
            "epoch 953 / 2000, step 48/63, loss = 0.6806\n",
            "epoch 954 / 2000, step 16/63, loss = 0.5993\n",
            "epoch 954 / 2000, step 32/63, loss = 0.5562\n",
            "epoch 954 / 2000, step 48/63, loss = 0.5632\n",
            "epoch 955 / 2000, step 16/63, loss = 0.5586\n",
            "epoch 955 / 2000, step 32/63, loss = 0.8575\n",
            "epoch 955 / 2000, step 48/63, loss = 0.5298\n",
            "epoch 956 / 2000, step 16/63, loss = 0.6736\n",
            "epoch 956 / 2000, step 32/63, loss = 0.5640\n",
            "epoch 956 / 2000, step 48/63, loss = 0.6290\n",
            "epoch 957 / 2000, step 16/63, loss = 0.8204\n",
            "epoch 957 / 2000, step 32/63, loss = 0.5916\n",
            "epoch 957 / 2000, step 48/63, loss = 0.7985\n",
            "epoch 958 / 2000, step 16/63, loss = 0.6086\n",
            "epoch 958 / 2000, step 32/63, loss = 0.6579\n",
            "epoch 958 / 2000, step 48/63, loss = 0.6685\n",
            "epoch 959 / 2000, step 16/63, loss = 0.7291\n",
            "epoch 959 / 2000, step 32/63, loss = 0.7341\n",
            "epoch 959 / 2000, step 48/63, loss = 0.8555\n",
            "epoch 960 / 2000, step 16/63, loss = 0.6856\n",
            "epoch 960 / 2000, step 32/63, loss = 0.6597\n",
            "epoch 960 / 2000, step 48/63, loss = 0.7375\n",
            "epoch 961 / 2000, step 16/63, loss = 0.5947\n",
            "epoch 961 / 2000, step 32/63, loss = 0.7267\n",
            "epoch 961 / 2000, step 48/63, loss = 0.8394\n",
            "epoch 962 / 2000, step 16/63, loss = 0.8311\n",
            "epoch 962 / 2000, step 32/63, loss = 0.6619\n",
            "epoch 962 / 2000, step 48/63, loss = 0.7690\n",
            "epoch 963 / 2000, step 16/63, loss = 0.6183\n",
            "epoch 963 / 2000, step 32/63, loss = 0.6057\n",
            "epoch 963 / 2000, step 48/63, loss = 0.5342\n",
            "epoch 964 / 2000, step 16/63, loss = 0.6782\n",
            "epoch 964 / 2000, step 32/63, loss = 0.5358\n",
            "epoch 964 / 2000, step 48/63, loss = 0.6676\n",
            "epoch 965 / 2000, step 16/63, loss = 0.9062\n",
            "epoch 965 / 2000, step 32/63, loss = 0.7620\n",
            "epoch 965 / 2000, step 48/63, loss = 0.5180\n",
            "epoch 966 / 2000, step 16/63, loss = 0.5076\n",
            "epoch 966 / 2000, step 32/63, loss = 0.6840\n",
            "epoch 966 / 2000, step 48/63, loss = 0.9852\n",
            "epoch 967 / 2000, step 16/63, loss = 0.7491\n",
            "epoch 967 / 2000, step 32/63, loss = 0.6105\n",
            "epoch 967 / 2000, step 48/63, loss = 0.7792\n",
            "epoch 968 / 2000, step 16/63, loss = 0.4234\n",
            "epoch 968 / 2000, step 32/63, loss = 0.7661\n",
            "epoch 968 / 2000, step 48/63, loss = 0.9067\n",
            "epoch 969 / 2000, step 16/63, loss = 0.4731\n",
            "epoch 969 / 2000, step 32/63, loss = 0.6341\n",
            "epoch 969 / 2000, step 48/63, loss = 0.6615\n",
            "epoch 970 / 2000, step 16/63, loss = 0.8040\n",
            "epoch 970 / 2000, step 32/63, loss = 0.6544\n",
            "epoch 970 / 2000, step 48/63, loss = 0.3600\n",
            "epoch 971 / 2000, step 16/63, loss = 0.5382\n",
            "epoch 971 / 2000, step 32/63, loss = 0.5710\n",
            "epoch 971 / 2000, step 48/63, loss = 0.5198\n",
            "epoch 972 / 2000, step 16/63, loss = 0.4746\n",
            "epoch 972 / 2000, step 32/63, loss = 0.5550\n",
            "epoch 972 / 2000, step 48/63, loss = 0.8442\n",
            "epoch 973 / 2000, step 16/63, loss = 0.6545\n",
            "epoch 973 / 2000, step 32/63, loss = 0.8243\n",
            "epoch 973 / 2000, step 48/63, loss = 0.7417\n",
            "epoch 974 / 2000, step 16/63, loss = 0.7499\n",
            "epoch 974 / 2000, step 32/63, loss = 0.6693\n",
            "epoch 974 / 2000, step 48/63, loss = 0.6135\n",
            "epoch 975 / 2000, step 16/63, loss = 0.6363\n",
            "epoch 975 / 2000, step 32/63, loss = 0.4859\n",
            "epoch 975 / 2000, step 48/63, loss = 0.4882\n",
            "epoch 976 / 2000, step 16/63, loss = 0.5961\n",
            "epoch 976 / 2000, step 32/63, loss = 0.5714\n",
            "epoch 976 / 2000, step 48/63, loss = 0.7587\n",
            "epoch 977 / 2000, step 16/63, loss = 0.5488\n",
            "epoch 977 / 2000, step 32/63, loss = 0.6235\n",
            "epoch 977 / 2000, step 48/63, loss = 0.7220\n",
            "epoch 978 / 2000, step 16/63, loss = 1.0044\n",
            "epoch 978 / 2000, step 32/63, loss = 0.8367\n",
            "epoch 978 / 2000, step 48/63, loss = 0.7345\n",
            "epoch 979 / 2000, step 16/63, loss = 0.6744\n",
            "epoch 979 / 2000, step 32/63, loss = 0.5392\n",
            "epoch 979 / 2000, step 48/63, loss = 0.7221\n",
            "epoch 980 / 2000, step 16/63, loss = 0.7537\n",
            "epoch 980 / 2000, step 32/63, loss = 0.6097\n",
            "epoch 980 / 2000, step 48/63, loss = 0.6505\n",
            "epoch 981 / 2000, step 16/63, loss = 0.6750\n",
            "epoch 981 / 2000, step 32/63, loss = 0.6230\n",
            "epoch 981 / 2000, step 48/63, loss = 0.6983\n",
            "epoch 982 / 2000, step 16/63, loss = 0.6442\n",
            "epoch 982 / 2000, step 32/63, loss = 0.6725\n",
            "epoch 982 / 2000, step 48/63, loss = 0.8325\n",
            "epoch 983 / 2000, step 16/63, loss = 0.5340\n",
            "epoch 983 / 2000, step 32/63, loss = 0.6498\n",
            "epoch 983 / 2000, step 48/63, loss = 0.6461\n",
            "epoch 984 / 2000, step 16/63, loss = 0.7327\n",
            "epoch 984 / 2000, step 32/63, loss = 0.6337\n",
            "epoch 984 / 2000, step 48/63, loss = 0.6464\n",
            "epoch 985 / 2000, step 16/63, loss = 0.8468\n",
            "epoch 985 / 2000, step 32/63, loss = 0.6984\n",
            "epoch 985 / 2000, step 48/63, loss = 0.6797\n",
            "epoch 986 / 2000, step 16/63, loss = 0.9197\n",
            "epoch 986 / 2000, step 32/63, loss = 0.7298\n",
            "epoch 986 / 2000, step 48/63, loss = 0.6898\n",
            "epoch 987 / 2000, step 16/63, loss = 0.5004\n",
            "epoch 987 / 2000, step 32/63, loss = 0.7210\n",
            "epoch 987 / 2000, step 48/63, loss = 0.8599\n",
            "epoch 988 / 2000, step 16/63, loss = 0.7524\n",
            "epoch 988 / 2000, step 32/63, loss = 0.6080\n",
            "epoch 988 / 2000, step 48/63, loss = 0.6168\n",
            "epoch 989 / 2000, step 16/63, loss = 1.0059\n",
            "epoch 989 / 2000, step 32/63, loss = 0.6890\n",
            "epoch 989 / 2000, step 48/63, loss = 0.4883\n",
            "epoch 990 / 2000, step 16/63, loss = 0.6761\n",
            "epoch 990 / 2000, step 32/63, loss = 0.6162\n",
            "epoch 990 / 2000, step 48/63, loss = 0.5738\n",
            "epoch 991 / 2000, step 16/63, loss = 0.8943\n",
            "epoch 991 / 2000, step 32/63, loss = 0.4443\n",
            "epoch 991 / 2000, step 48/63, loss = 0.8037\n",
            "epoch 992 / 2000, step 16/63, loss = 0.3575\n",
            "epoch 992 / 2000, step 32/63, loss = 0.6705\n",
            "epoch 992 / 2000, step 48/63, loss = 0.7227\n",
            "epoch 993 / 2000, step 16/63, loss = 0.5295\n",
            "epoch 993 / 2000, step 32/63, loss = 0.5613\n",
            "epoch 993 / 2000, step 48/63, loss = 0.7686\n",
            "epoch 994 / 2000, step 16/63, loss = 0.6494\n",
            "epoch 994 / 2000, step 32/63, loss = 0.4900\n",
            "epoch 994 / 2000, step 48/63, loss = 0.8404\n",
            "epoch 995 / 2000, step 16/63, loss = 0.5670\n",
            "epoch 995 / 2000, step 32/63, loss = 0.7842\n",
            "epoch 995 / 2000, step 48/63, loss = 0.6965\n",
            "epoch 996 / 2000, step 16/63, loss = 0.5122\n",
            "epoch 996 / 2000, step 32/63, loss = 0.6323\n",
            "epoch 996 / 2000, step 48/63, loss = 0.6559\n",
            "epoch 997 / 2000, step 16/63, loss = 0.7400\n",
            "epoch 997 / 2000, step 32/63, loss = 0.5137\n",
            "epoch 997 / 2000, step 48/63, loss = 0.5076\n",
            "epoch 998 / 2000, step 16/63, loss = 0.6796\n",
            "epoch 998 / 2000, step 32/63, loss = 0.5489\n",
            "epoch 998 / 2000, step 48/63, loss = 0.5654\n",
            "epoch 999 / 2000, step 16/63, loss = 0.6435\n",
            "epoch 999 / 2000, step 32/63, loss = 0.9130\n",
            "epoch 999 / 2000, step 48/63, loss = 0.5506\n",
            "epoch 1000 / 2000, step 16/63, loss = 0.9020\n",
            "epoch 1000 / 2000, step 32/63, loss = 0.6100\n",
            "epoch 1000 / 2000, step 48/63, loss = 0.6490\n",
            "epoch 1001 / 2000, step 16/63, loss = 0.5468\n",
            "epoch 1001 / 2000, step 32/63, loss = 0.5727\n",
            "epoch 1001 / 2000, step 48/63, loss = 0.6114\n",
            "epoch 1002 / 2000, step 16/63, loss = 0.5007\n",
            "epoch 1002 / 2000, step 32/63, loss = 0.6429\n",
            "epoch 1002 / 2000, step 48/63, loss = 0.7034\n",
            "epoch 1003 / 2000, step 16/63, loss = 0.8882\n",
            "epoch 1003 / 2000, step 32/63, loss = 0.8890\n",
            "epoch 1003 / 2000, step 48/63, loss = 0.5888\n",
            "epoch 1004 / 2000, step 16/63, loss = 0.6469\n",
            "epoch 1004 / 2000, step 32/63, loss = 0.8723\n",
            "epoch 1004 / 2000, step 48/63, loss = 0.7068\n",
            "epoch 1005 / 2000, step 16/63, loss = 0.5386\n",
            "epoch 1005 / 2000, step 32/63, loss = 0.6655\n",
            "epoch 1005 / 2000, step 48/63, loss = 0.6411\n",
            "epoch 1006 / 2000, step 16/63, loss = 0.5067\n",
            "epoch 1006 / 2000, step 32/63, loss = 0.6238\n",
            "epoch 1006 / 2000, step 48/63, loss = 1.0129\n",
            "epoch 1007 / 2000, step 16/63, loss = 0.5529\n",
            "epoch 1007 / 2000, step 32/63, loss = 0.8536\n",
            "epoch 1007 / 2000, step 48/63, loss = 0.5644\n",
            "epoch 1008 / 2000, step 16/63, loss = 0.4139\n",
            "epoch 1008 / 2000, step 32/63, loss = 0.7310\n",
            "epoch 1008 / 2000, step 48/63, loss = 0.5980\n",
            "epoch 1009 / 2000, step 16/63, loss = 0.7604\n",
            "epoch 1009 / 2000, step 32/63, loss = 0.7157\n",
            "epoch 1009 / 2000, step 48/63, loss = 0.5833\n",
            "epoch 1010 / 2000, step 16/63, loss = 0.7811\n",
            "epoch 1010 / 2000, step 32/63, loss = 0.5673\n",
            "epoch 1010 / 2000, step 48/63, loss = 0.5607\n",
            "epoch 1011 / 2000, step 16/63, loss = 0.6711\n",
            "epoch 1011 / 2000, step 32/63, loss = 0.8392\n",
            "epoch 1011 / 2000, step 48/63, loss = 0.5391\n",
            "epoch 1012 / 2000, step 16/63, loss = 0.5627\n",
            "epoch 1012 / 2000, step 32/63, loss = 0.7882\n",
            "epoch 1012 / 2000, step 48/63, loss = 0.5985\n",
            "epoch 1013 / 2000, step 16/63, loss = 0.4469\n",
            "epoch 1013 / 2000, step 32/63, loss = 0.5331\n",
            "epoch 1013 / 2000, step 48/63, loss = 0.5143\n",
            "epoch 1014 / 2000, step 16/63, loss = 0.5448\n",
            "epoch 1014 / 2000, step 32/63, loss = 0.7425\n",
            "epoch 1014 / 2000, step 48/63, loss = 0.6982\n",
            "epoch 1015 / 2000, step 16/63, loss = 0.6995\n",
            "epoch 1015 / 2000, step 32/63, loss = 0.5371\n",
            "epoch 1015 / 2000, step 48/63, loss = 0.5295\n",
            "epoch 1016 / 2000, step 16/63, loss = 0.5010\n",
            "epoch 1016 / 2000, step 32/63, loss = 0.5740\n",
            "epoch 1016 / 2000, step 48/63, loss = 0.7398\n",
            "epoch 1017 / 2000, step 16/63, loss = 0.7047\n",
            "epoch 1017 / 2000, step 32/63, loss = 0.5787\n",
            "epoch 1017 / 2000, step 48/63, loss = 0.6236\n",
            "epoch 1018 / 2000, step 16/63, loss = 0.7583\n",
            "epoch 1018 / 2000, step 32/63, loss = 0.4614\n",
            "epoch 1018 / 2000, step 48/63, loss = 0.6316\n",
            "epoch 1019 / 2000, step 16/63, loss = 0.6209\n",
            "epoch 1019 / 2000, step 32/63, loss = 0.6800\n",
            "epoch 1019 / 2000, step 48/63, loss = 0.4999\n",
            "epoch 1020 / 2000, step 16/63, loss = 0.4651\n",
            "epoch 1020 / 2000, step 32/63, loss = 0.6596\n",
            "epoch 1020 / 2000, step 48/63, loss = 0.5511\n",
            "epoch 1021 / 2000, step 16/63, loss = 0.4159\n",
            "epoch 1021 / 2000, step 32/63, loss = 0.7192\n",
            "epoch 1021 / 2000, step 48/63, loss = 0.6864\n",
            "epoch 1022 / 2000, step 16/63, loss = 0.9175\n",
            "epoch 1022 / 2000, step 32/63, loss = 0.5205\n",
            "epoch 1022 / 2000, step 48/63, loss = 0.5956\n",
            "epoch 1023 / 2000, step 16/63, loss = 0.3785\n",
            "epoch 1023 / 2000, step 32/63, loss = 0.5331\n",
            "epoch 1023 / 2000, step 48/63, loss = 0.7764\n",
            "epoch 1024 / 2000, step 16/63, loss = 0.4127\n",
            "epoch 1024 / 2000, step 32/63, loss = 0.6777\n",
            "epoch 1024 / 2000, step 48/63, loss = 0.7979\n",
            "epoch 1025 / 2000, step 16/63, loss = 0.5034\n",
            "epoch 1025 / 2000, step 32/63, loss = 0.8719\n",
            "epoch 1025 / 2000, step 48/63, loss = 0.8644\n",
            "epoch 1026 / 2000, step 16/63, loss = 0.5308\n",
            "epoch 1026 / 2000, step 32/63, loss = 0.5160\n",
            "epoch 1026 / 2000, step 48/63, loss = 0.7058\n",
            "epoch 1027 / 2000, step 16/63, loss = 0.5411\n",
            "epoch 1027 / 2000, step 32/63, loss = 0.4153\n",
            "epoch 1027 / 2000, step 48/63, loss = 0.4404\n",
            "epoch 1028 / 2000, step 16/63, loss = 0.5984\n",
            "epoch 1028 / 2000, step 32/63, loss = 0.5219\n",
            "epoch 1028 / 2000, step 48/63, loss = 0.5607\n",
            "epoch 1029 / 2000, step 16/63, loss = 0.5320\n",
            "epoch 1029 / 2000, step 32/63, loss = 0.5965\n",
            "epoch 1029 / 2000, step 48/63, loss = 0.4529\n",
            "epoch 1030 / 2000, step 16/63, loss = 0.5809\n",
            "epoch 1030 / 2000, step 32/63, loss = 0.7638\n",
            "epoch 1030 / 2000, step 48/63, loss = 0.4209\n",
            "epoch 1031 / 2000, step 16/63, loss = 0.6128\n",
            "epoch 1031 / 2000, step 32/63, loss = 0.6365\n",
            "epoch 1031 / 2000, step 48/63, loss = 0.4696\n",
            "epoch 1032 / 2000, step 16/63, loss = 0.7791\n",
            "epoch 1032 / 2000, step 32/63, loss = 0.9258\n",
            "epoch 1032 / 2000, step 48/63, loss = 0.4917\n",
            "epoch 1033 / 2000, step 16/63, loss = 0.6134\n",
            "epoch 1033 / 2000, step 32/63, loss = 0.6236\n",
            "epoch 1033 / 2000, step 48/63, loss = 0.7988\n",
            "epoch 1034 / 2000, step 16/63, loss = 0.7249\n",
            "epoch 1034 / 2000, step 32/63, loss = 0.8432\n",
            "epoch 1034 / 2000, step 48/63, loss = 0.5734\n",
            "epoch 1035 / 2000, step 16/63, loss = 0.5551\n",
            "epoch 1035 / 2000, step 32/63, loss = 0.5270\n",
            "epoch 1035 / 2000, step 48/63, loss = 0.6598\n",
            "epoch 1036 / 2000, step 16/63, loss = 0.7782\n",
            "epoch 1036 / 2000, step 32/63, loss = 0.7254\n",
            "epoch 1036 / 2000, step 48/63, loss = 0.6100\n",
            "epoch 1037 / 2000, step 16/63, loss = 0.5244\n",
            "epoch 1037 / 2000, step 32/63, loss = 0.4337\n",
            "epoch 1037 / 2000, step 48/63, loss = 0.6852\n",
            "epoch 1038 / 2000, step 16/63, loss = 0.4767\n",
            "epoch 1038 / 2000, step 32/63, loss = 0.6126\n",
            "epoch 1038 / 2000, step 48/63, loss = 0.5797\n",
            "epoch 1039 / 2000, step 16/63, loss = 0.5923\n",
            "epoch 1039 / 2000, step 32/63, loss = 0.5853\n",
            "epoch 1039 / 2000, step 48/63, loss = 0.6696\n",
            "epoch 1040 / 2000, step 16/63, loss = 0.6035\n",
            "epoch 1040 / 2000, step 32/63, loss = 0.5209\n",
            "epoch 1040 / 2000, step 48/63, loss = 0.6582\n",
            "epoch 1041 / 2000, step 16/63, loss = 0.7656\n",
            "epoch 1041 / 2000, step 32/63, loss = 0.8080\n",
            "epoch 1041 / 2000, step 48/63, loss = 0.5058\n",
            "epoch 1042 / 2000, step 16/63, loss = 0.5235\n",
            "epoch 1042 / 2000, step 32/63, loss = 0.5657\n",
            "epoch 1042 / 2000, step 48/63, loss = 0.6306\n",
            "epoch 1043 / 2000, step 16/63, loss = 0.6256\n",
            "epoch 1043 / 2000, step 32/63, loss = 0.6242\n",
            "epoch 1043 / 2000, step 48/63, loss = 0.5831\n",
            "epoch 1044 / 2000, step 16/63, loss = 0.6815\n",
            "epoch 1044 / 2000, step 32/63, loss = 0.6271\n",
            "epoch 1044 / 2000, step 48/63, loss = 0.4876\n",
            "epoch 1045 / 2000, step 16/63, loss = 0.6695\n",
            "epoch 1045 / 2000, step 32/63, loss = 0.4943\n",
            "epoch 1045 / 2000, step 48/63, loss = 0.5421\n",
            "epoch 1046 / 2000, step 16/63, loss = 0.6316\n",
            "epoch 1046 / 2000, step 32/63, loss = 0.5768\n",
            "epoch 1046 / 2000, step 48/63, loss = 0.4773\n",
            "epoch 1047 / 2000, step 16/63, loss = 0.5124\n",
            "epoch 1047 / 2000, step 32/63, loss = 0.5750\n",
            "epoch 1047 / 2000, step 48/63, loss = 0.7048\n",
            "epoch 1048 / 2000, step 16/63, loss = 0.6607\n",
            "epoch 1048 / 2000, step 32/63, loss = 0.5119\n",
            "epoch 1048 / 2000, step 48/63, loss = 0.6132\n",
            "epoch 1049 / 2000, step 16/63, loss = 0.6210\n",
            "epoch 1049 / 2000, step 32/63, loss = 0.4711\n",
            "epoch 1049 / 2000, step 48/63, loss = 0.7315\n",
            "epoch 1050 / 2000, step 16/63, loss = 0.7204\n",
            "epoch 1050 / 2000, step 32/63, loss = 0.5271\n",
            "epoch 1050 / 2000, step 48/63, loss = 0.5705\n",
            "epoch 1051 / 2000, step 16/63, loss = 0.5259\n",
            "epoch 1051 / 2000, step 32/63, loss = 0.5412\n",
            "epoch 1051 / 2000, step 48/63, loss = 0.5919\n",
            "epoch 1052 / 2000, step 16/63, loss = 0.6726\n",
            "epoch 1052 / 2000, step 32/63, loss = 0.5623\n",
            "epoch 1052 / 2000, step 48/63, loss = 0.6371\n",
            "epoch 1053 / 2000, step 16/63, loss = 0.7638\n",
            "epoch 1053 / 2000, step 32/63, loss = 0.8448\n",
            "epoch 1053 / 2000, step 48/63, loss = 0.4655\n",
            "epoch 1054 / 2000, step 16/63, loss = 0.5390\n",
            "epoch 1054 / 2000, step 32/63, loss = 0.6380\n",
            "epoch 1054 / 2000, step 48/63, loss = 0.5884\n",
            "epoch 1055 / 2000, step 16/63, loss = 0.5697\n",
            "epoch 1055 / 2000, step 32/63, loss = 0.4649\n",
            "epoch 1055 / 2000, step 48/63, loss = 0.4853\n",
            "epoch 1056 / 2000, step 16/63, loss = 0.3812\n",
            "epoch 1056 / 2000, step 32/63, loss = 0.4217\n",
            "epoch 1056 / 2000, step 48/63, loss = 0.5678\n",
            "epoch 1057 / 2000, step 16/63, loss = 0.4957\n",
            "epoch 1057 / 2000, step 32/63, loss = 0.6666\n",
            "epoch 1057 / 2000, step 48/63, loss = 0.7467\n",
            "epoch 1058 / 2000, step 16/63, loss = 0.5166\n",
            "epoch 1058 / 2000, step 32/63, loss = 0.3679\n",
            "epoch 1058 / 2000, step 48/63, loss = 0.5613\n",
            "epoch 1059 / 2000, step 16/63, loss = 0.5120\n",
            "epoch 1059 / 2000, step 32/63, loss = 0.6060\n",
            "epoch 1059 / 2000, step 48/63, loss = 0.6668\n",
            "epoch 1060 / 2000, step 16/63, loss = 0.6309\n",
            "epoch 1060 / 2000, step 32/63, loss = 0.6356\n",
            "epoch 1060 / 2000, step 48/63, loss = 0.6492\n",
            "epoch 1061 / 2000, step 16/63, loss = 0.5796\n",
            "epoch 1061 / 2000, step 32/63, loss = 0.8135\n",
            "epoch 1061 / 2000, step 48/63, loss = 0.5861\n",
            "epoch 1062 / 2000, step 16/63, loss = 0.6798\n",
            "epoch 1062 / 2000, step 32/63, loss = 0.6294\n",
            "epoch 1062 / 2000, step 48/63, loss = 0.5838\n",
            "epoch 1063 / 2000, step 16/63, loss = 0.6454\n",
            "epoch 1063 / 2000, step 32/63, loss = 0.8010\n",
            "epoch 1063 / 2000, step 48/63, loss = 0.5236\n",
            "epoch 1064 / 2000, step 16/63, loss = 0.7513\n",
            "epoch 1064 / 2000, step 32/63, loss = 0.5099\n",
            "epoch 1064 / 2000, step 48/63, loss = 0.6230\n",
            "epoch 1065 / 2000, step 16/63, loss = 0.6018\n",
            "epoch 1065 / 2000, step 32/63, loss = 0.5304\n",
            "epoch 1065 / 2000, step 48/63, loss = 0.5449\n",
            "epoch 1066 / 2000, step 16/63, loss = 0.8744\n",
            "epoch 1066 / 2000, step 32/63, loss = 0.6203\n",
            "epoch 1066 / 2000, step 48/63, loss = 0.5314\n",
            "epoch 1067 / 2000, step 16/63, loss = 0.8406\n",
            "epoch 1067 / 2000, step 32/63, loss = 0.3928\n",
            "epoch 1067 / 2000, step 48/63, loss = 0.4853\n",
            "epoch 1068 / 2000, step 16/63, loss = 0.6410\n",
            "epoch 1068 / 2000, step 32/63, loss = 0.4534\n",
            "epoch 1068 / 2000, step 48/63, loss = 0.7561\n",
            "epoch 1069 / 2000, step 16/63, loss = 0.7840\n",
            "epoch 1069 / 2000, step 32/63, loss = 0.7383\n",
            "epoch 1069 / 2000, step 48/63, loss = 0.4743\n",
            "epoch 1070 / 2000, step 16/63, loss = 0.5340\n",
            "epoch 1070 / 2000, step 32/63, loss = 1.0188\n",
            "epoch 1070 / 2000, step 48/63, loss = 0.5877\n",
            "epoch 1071 / 2000, step 16/63, loss = 0.6564\n",
            "epoch 1071 / 2000, step 32/63, loss = 0.5219\n",
            "epoch 1071 / 2000, step 48/63, loss = 0.5597\n",
            "epoch 1072 / 2000, step 16/63, loss = 0.6267\n",
            "epoch 1072 / 2000, step 32/63, loss = 0.6012\n",
            "epoch 1072 / 2000, step 48/63, loss = 0.3773\n",
            "epoch 1073 / 2000, step 16/63, loss = 0.7540\n",
            "epoch 1073 / 2000, step 32/63, loss = 0.7268\n",
            "epoch 1073 / 2000, step 48/63, loss = 0.6229\n",
            "epoch 1074 / 2000, step 16/63, loss = 0.5842\n",
            "epoch 1074 / 2000, step 32/63, loss = 0.5680\n",
            "epoch 1074 / 2000, step 48/63, loss = 0.6454\n",
            "epoch 1075 / 2000, step 16/63, loss = 0.6564\n",
            "epoch 1075 / 2000, step 32/63, loss = 0.6695\n",
            "epoch 1075 / 2000, step 48/63, loss = 0.7331\n",
            "epoch 1076 / 2000, step 16/63, loss = 0.4961\n",
            "epoch 1076 / 2000, step 32/63, loss = 0.7614\n",
            "epoch 1076 / 2000, step 48/63, loss = 0.5543\n",
            "epoch 1077 / 2000, step 16/63, loss = 0.7797\n",
            "epoch 1077 / 2000, step 32/63, loss = 0.6746\n",
            "epoch 1077 / 2000, step 48/63, loss = 0.7821\n",
            "epoch 1078 / 2000, step 16/63, loss = 0.5473\n",
            "epoch 1078 / 2000, step 32/63, loss = 0.4726\n",
            "epoch 1078 / 2000, step 48/63, loss = 0.5214\n",
            "epoch 1079 / 2000, step 16/63, loss = 0.4818\n",
            "epoch 1079 / 2000, step 32/63, loss = 0.5120\n",
            "epoch 1079 / 2000, step 48/63, loss = 0.5149\n",
            "epoch 1080 / 2000, step 16/63, loss = 0.5033\n",
            "epoch 1080 / 2000, step 32/63, loss = 0.4923\n",
            "epoch 1080 / 2000, step 48/63, loss = 0.3782\n",
            "epoch 1081 / 2000, step 16/63, loss = 0.5044\n",
            "epoch 1081 / 2000, step 32/63, loss = 0.5675\n",
            "epoch 1081 / 2000, step 48/63, loss = 0.4807\n",
            "epoch 1082 / 2000, step 16/63, loss = 0.4315\n",
            "epoch 1082 / 2000, step 32/63, loss = 0.5832\n",
            "epoch 1082 / 2000, step 48/63, loss = 0.5909\n",
            "epoch 1083 / 2000, step 16/63, loss = 0.8483\n",
            "epoch 1083 / 2000, step 32/63, loss = 0.5707\n",
            "epoch 1083 / 2000, step 48/63, loss = 0.5560\n",
            "epoch 1084 / 2000, step 16/63, loss = 0.4963\n",
            "epoch 1084 / 2000, step 32/63, loss = 0.6888\n",
            "epoch 1084 / 2000, step 48/63, loss = 0.5571\n",
            "epoch 1085 / 2000, step 16/63, loss = 0.3833\n",
            "epoch 1085 / 2000, step 32/63, loss = 0.4741\n",
            "epoch 1085 / 2000, step 48/63, loss = 0.5361\n",
            "epoch 1086 / 2000, step 16/63, loss = 0.8134\n",
            "epoch 1086 / 2000, step 32/63, loss = 0.4928\n",
            "epoch 1086 / 2000, step 48/63, loss = 0.4943\n",
            "epoch 1087 / 2000, step 16/63, loss = 0.4910\n",
            "epoch 1087 / 2000, step 32/63, loss = 0.8509\n",
            "epoch 1087 / 2000, step 48/63, loss = 0.4918\n",
            "epoch 1088 / 2000, step 16/63, loss = 0.4711\n",
            "epoch 1088 / 2000, step 32/63, loss = 0.8129\n",
            "epoch 1088 / 2000, step 48/63, loss = 0.7555\n",
            "epoch 1089 / 2000, step 16/63, loss = 0.7875\n",
            "epoch 1089 / 2000, step 32/63, loss = 0.6324\n",
            "epoch 1089 / 2000, step 48/63, loss = 0.4714\n",
            "epoch 1090 / 2000, step 16/63, loss = 0.6362\n",
            "epoch 1090 / 2000, step 32/63, loss = 0.4798\n",
            "epoch 1090 / 2000, step 48/63, loss = 0.5743\n",
            "epoch 1091 / 2000, step 16/63, loss = 0.7058\n",
            "epoch 1091 / 2000, step 32/63, loss = 0.5562\n",
            "epoch 1091 / 2000, step 48/63, loss = 0.4365\n",
            "epoch 1092 / 2000, step 16/63, loss = 0.8668\n",
            "epoch 1092 / 2000, step 32/63, loss = 0.6666\n",
            "epoch 1092 / 2000, step 48/63, loss = 0.6348\n",
            "epoch 1093 / 2000, step 16/63, loss = 0.6290\n",
            "epoch 1093 / 2000, step 32/63, loss = 0.3527\n",
            "epoch 1093 / 2000, step 48/63, loss = 0.7340\n",
            "epoch 1094 / 2000, step 16/63, loss = 0.4257\n",
            "epoch 1094 / 2000, step 32/63, loss = 0.7499\n",
            "epoch 1094 / 2000, step 48/63, loss = 0.3088\n",
            "epoch 1095 / 2000, step 16/63, loss = 0.4727\n",
            "epoch 1095 / 2000, step 32/63, loss = 0.5400\n",
            "epoch 1095 / 2000, step 48/63, loss = 0.6492\n",
            "epoch 1096 / 2000, step 16/63, loss = 0.9876\n",
            "epoch 1096 / 2000, step 32/63, loss = 0.5881\n",
            "epoch 1096 / 2000, step 48/63, loss = 0.6939\n",
            "epoch 1097 / 2000, step 16/63, loss = 0.4964\n",
            "epoch 1097 / 2000, step 32/63, loss = 0.5452\n",
            "epoch 1097 / 2000, step 48/63, loss = 0.6568\n",
            "epoch 1098 / 2000, step 16/63, loss = 0.6045\n",
            "epoch 1098 / 2000, step 32/63, loss = 0.6940\n",
            "epoch 1098 / 2000, step 48/63, loss = 0.5166\n",
            "epoch 1099 / 2000, step 16/63, loss = 0.3990\n",
            "epoch 1099 / 2000, step 32/63, loss = 0.5077\n",
            "epoch 1099 / 2000, step 48/63, loss = 0.5740\n",
            "epoch 1100 / 2000, step 16/63, loss = 0.5524\n",
            "epoch 1100 / 2000, step 32/63, loss = 0.7017\n",
            "epoch 1100 / 2000, step 48/63, loss = 0.5273\n",
            "epoch 1101 / 2000, step 16/63, loss = 0.5802\n",
            "epoch 1101 / 2000, step 32/63, loss = 0.6713\n",
            "epoch 1101 / 2000, step 48/63, loss = 0.5696\n",
            "epoch 1102 / 2000, step 16/63, loss = 0.5874\n",
            "epoch 1102 / 2000, step 32/63, loss = 0.4101\n",
            "epoch 1102 / 2000, step 48/63, loss = 0.5187\n",
            "epoch 1103 / 2000, step 16/63, loss = 0.5031\n",
            "epoch 1103 / 2000, step 32/63, loss = 0.6857\n",
            "epoch 1103 / 2000, step 48/63, loss = 0.8197\n",
            "epoch 1104 / 2000, step 16/63, loss = 0.7838\n",
            "epoch 1104 / 2000, step 32/63, loss = 0.6979\n",
            "epoch 1104 / 2000, step 48/63, loss = 0.5015\n",
            "epoch 1105 / 2000, step 16/63, loss = 0.5273\n",
            "epoch 1105 / 2000, step 32/63, loss = 0.4497\n",
            "epoch 1105 / 2000, step 48/63, loss = 0.8004\n",
            "epoch 1106 / 2000, step 16/63, loss = 0.4982\n",
            "epoch 1106 / 2000, step 32/63, loss = 0.8143\n",
            "epoch 1106 / 2000, step 48/63, loss = 0.6236\n",
            "epoch 1107 / 2000, step 16/63, loss = 0.6946\n",
            "epoch 1107 / 2000, step 32/63, loss = 0.5964\n",
            "epoch 1107 / 2000, step 48/63, loss = 0.7098\n",
            "epoch 1108 / 2000, step 16/63, loss = 0.5254\n",
            "epoch 1108 / 2000, step 32/63, loss = 0.6384\n",
            "epoch 1108 / 2000, step 48/63, loss = 0.4177\n",
            "epoch 1109 / 2000, step 16/63, loss = 0.5212\n",
            "epoch 1109 / 2000, step 32/63, loss = 0.5322\n",
            "epoch 1109 / 2000, step 48/63, loss = 0.3945\n",
            "epoch 1110 / 2000, step 16/63, loss = 0.5179\n",
            "epoch 1110 / 2000, step 32/63, loss = 0.8845\n",
            "epoch 1110 / 2000, step 48/63, loss = 0.4730\n",
            "epoch 1111 / 2000, step 16/63, loss = 0.7805\n",
            "epoch 1111 / 2000, step 32/63, loss = 0.5418\n",
            "epoch 1111 / 2000, step 48/63, loss = 0.4985\n",
            "epoch 1112 / 2000, step 16/63, loss = 0.6131\n",
            "epoch 1112 / 2000, step 32/63, loss = 0.5387\n",
            "epoch 1112 / 2000, step 48/63, loss = 0.7635\n",
            "epoch 1113 / 2000, step 16/63, loss = 0.5562\n",
            "epoch 1113 / 2000, step 32/63, loss = 0.7974\n",
            "epoch 1113 / 2000, step 48/63, loss = 0.5130\n",
            "epoch 1114 / 2000, step 16/63, loss = 0.4372\n",
            "epoch 1114 / 2000, step 32/63, loss = 0.5381\n",
            "epoch 1114 / 2000, step 48/63, loss = 0.5846\n",
            "epoch 1115 / 2000, step 16/63, loss = 0.6666\n",
            "epoch 1115 / 2000, step 32/63, loss = 0.6680\n",
            "epoch 1115 / 2000, step 48/63, loss = 0.5459\n",
            "epoch 1116 / 2000, step 16/63, loss = 0.6191\n",
            "epoch 1116 / 2000, step 32/63, loss = 0.7263\n",
            "epoch 1116 / 2000, step 48/63, loss = 0.5186\n",
            "epoch 1117 / 2000, step 16/63, loss = 0.5394\n",
            "epoch 1117 / 2000, step 32/63, loss = 0.5225\n",
            "epoch 1117 / 2000, step 48/63, loss = 0.5584\n",
            "epoch 1118 / 2000, step 16/63, loss = 0.9281\n",
            "epoch 1118 / 2000, step 32/63, loss = 0.3624\n",
            "epoch 1118 / 2000, step 48/63, loss = 0.6673\n",
            "epoch 1119 / 2000, step 16/63, loss = 0.5228\n",
            "epoch 1119 / 2000, step 32/63, loss = 0.4080\n",
            "epoch 1119 / 2000, step 48/63, loss = 0.7290\n",
            "epoch 1120 / 2000, step 16/63, loss = 0.5392\n",
            "epoch 1120 / 2000, step 32/63, loss = 0.3412\n",
            "epoch 1120 / 2000, step 48/63, loss = 0.4622\n",
            "epoch 1121 / 2000, step 16/63, loss = 0.6821\n",
            "epoch 1121 / 2000, step 32/63, loss = 0.5604\n",
            "epoch 1121 / 2000, step 48/63, loss = 0.5062\n",
            "epoch 1122 / 2000, step 16/63, loss = 0.4775\n",
            "epoch 1122 / 2000, step 32/63, loss = 0.5719\n",
            "epoch 1122 / 2000, step 48/63, loss = 0.5290\n",
            "epoch 1123 / 2000, step 16/63, loss = 0.5412\n",
            "epoch 1123 / 2000, step 32/63, loss = 0.6315\n",
            "epoch 1123 / 2000, step 48/63, loss = 0.5390\n",
            "epoch 1124 / 2000, step 16/63, loss = 0.6406\n",
            "epoch 1124 / 2000, step 32/63, loss = 0.6181\n",
            "epoch 1124 / 2000, step 48/63, loss = 0.9119\n",
            "epoch 1125 / 2000, step 16/63, loss = 0.4783\n",
            "epoch 1125 / 2000, step 32/63, loss = 0.5668\n",
            "epoch 1125 / 2000, step 48/63, loss = 0.4983\n",
            "epoch 1126 / 2000, step 16/63, loss = 0.7127\n",
            "epoch 1126 / 2000, step 32/63, loss = 0.5206\n",
            "epoch 1126 / 2000, step 48/63, loss = 0.5474\n",
            "epoch 1127 / 2000, step 16/63, loss = 0.4139\n",
            "epoch 1127 / 2000, step 32/63, loss = 0.5913\n",
            "epoch 1127 / 2000, step 48/63, loss = 0.4688\n",
            "epoch 1128 / 2000, step 16/63, loss = 0.5895\n",
            "epoch 1128 / 2000, step 32/63, loss = 0.4288\n",
            "epoch 1128 / 2000, step 48/63, loss = 0.5226\n",
            "epoch 1129 / 2000, step 16/63, loss = 0.6126\n",
            "epoch 1129 / 2000, step 32/63, loss = 0.5220\n",
            "epoch 1129 / 2000, step 48/63, loss = 0.4816\n",
            "epoch 1130 / 2000, step 16/63, loss = 0.6524\n",
            "epoch 1130 / 2000, step 32/63, loss = 0.7512\n",
            "epoch 1130 / 2000, step 48/63, loss = 0.6338\n",
            "epoch 1131 / 2000, step 16/63, loss = 0.6521\n",
            "epoch 1131 / 2000, step 32/63, loss = 0.5963\n",
            "epoch 1131 / 2000, step 48/63, loss = 0.6110\n",
            "epoch 1132 / 2000, step 16/63, loss = 0.4580\n",
            "epoch 1132 / 2000, step 32/63, loss = 0.4780\n",
            "epoch 1132 / 2000, step 48/63, loss = 0.5421\n",
            "epoch 1133 / 2000, step 16/63, loss = 0.6894\n",
            "epoch 1133 / 2000, step 32/63, loss = 0.5167\n",
            "epoch 1133 / 2000, step 48/63, loss = 0.5247\n",
            "epoch 1134 / 2000, step 16/63, loss = 0.7146\n",
            "epoch 1134 / 2000, step 32/63, loss = 0.4990\n",
            "epoch 1134 / 2000, step 48/63, loss = 0.6517\n",
            "epoch 1135 / 2000, step 16/63, loss = 0.6427\n",
            "epoch 1135 / 2000, step 32/63, loss = 0.5080\n",
            "epoch 1135 / 2000, step 48/63, loss = 0.5342\n",
            "epoch 1136 / 2000, step 16/63, loss = 0.5037\n",
            "epoch 1136 / 2000, step 32/63, loss = 0.4095\n",
            "epoch 1136 / 2000, step 48/63, loss = 0.4677\n",
            "epoch 1137 / 2000, step 16/63, loss = 0.6148\n",
            "epoch 1137 / 2000, step 32/63, loss = 0.7304\n",
            "epoch 1137 / 2000, step 48/63, loss = 0.5868\n",
            "epoch 1138 / 2000, step 16/63, loss = 0.6605\n",
            "epoch 1138 / 2000, step 32/63, loss = 0.6457\n",
            "epoch 1138 / 2000, step 48/63, loss = 0.5348\n",
            "epoch 1139 / 2000, step 16/63, loss = 0.4975\n",
            "epoch 1139 / 2000, step 32/63, loss = 0.5628\n",
            "epoch 1139 / 2000, step 48/63, loss = 0.4118\n",
            "epoch 1140 / 2000, step 16/63, loss = 0.6207\n",
            "epoch 1140 / 2000, step 32/63, loss = 0.5574\n",
            "epoch 1140 / 2000, step 48/63, loss = 0.5535\n",
            "epoch 1141 / 2000, step 16/63, loss = 0.3949\n",
            "epoch 1141 / 2000, step 32/63, loss = 0.4224\n",
            "epoch 1141 / 2000, step 48/63, loss = 0.5663\n",
            "epoch 1142 / 2000, step 16/63, loss = 0.6005\n",
            "epoch 1142 / 2000, step 32/63, loss = 0.4743\n",
            "epoch 1142 / 2000, step 48/63, loss = 0.5203\n",
            "epoch 1143 / 2000, step 16/63, loss = 0.4666\n",
            "epoch 1143 / 2000, step 32/63, loss = 0.6357\n",
            "epoch 1143 / 2000, step 48/63, loss = 0.6486\n",
            "epoch 1144 / 2000, step 16/63, loss = 0.6458\n",
            "epoch 1144 / 2000, step 32/63, loss = 0.6214\n",
            "epoch 1144 / 2000, step 48/63, loss = 0.3916\n",
            "epoch 1145 / 2000, step 16/63, loss = 0.6357\n",
            "epoch 1145 / 2000, step 32/63, loss = 0.2733\n",
            "epoch 1145 / 2000, step 48/63, loss = 0.5024\n",
            "epoch 1146 / 2000, step 16/63, loss = 0.4389\n",
            "epoch 1146 / 2000, step 32/63, loss = 0.5522\n",
            "epoch 1146 / 2000, step 48/63, loss = 0.6350\n",
            "epoch 1147 / 2000, step 16/63, loss = 0.3696\n",
            "epoch 1147 / 2000, step 32/63, loss = 0.6153\n",
            "epoch 1147 / 2000, step 48/63, loss = 0.4725\n",
            "epoch 1148 / 2000, step 16/63, loss = 0.7176\n",
            "epoch 1148 / 2000, step 32/63, loss = 0.4745\n",
            "epoch 1148 / 2000, step 48/63, loss = 0.5308\n",
            "epoch 1149 / 2000, step 16/63, loss = 0.4891\n",
            "epoch 1149 / 2000, step 32/63, loss = 0.5473\n",
            "epoch 1149 / 2000, step 48/63, loss = 0.4047\n",
            "epoch 1150 / 2000, step 16/63, loss = 0.5292\n",
            "epoch 1150 / 2000, step 32/63, loss = 0.6363\n",
            "epoch 1150 / 2000, step 48/63, loss = 0.7368\n",
            "epoch 1151 / 2000, step 16/63, loss = 0.6921\n",
            "epoch 1151 / 2000, step 32/63, loss = 0.5214\n",
            "epoch 1151 / 2000, step 48/63, loss = 0.5404\n",
            "epoch 1152 / 2000, step 16/63, loss = 0.4804\n",
            "epoch 1152 / 2000, step 32/63, loss = 0.7428\n",
            "epoch 1152 / 2000, step 48/63, loss = 0.9171\n",
            "epoch 1153 / 2000, step 16/63, loss = 0.6960\n",
            "epoch 1153 / 2000, step 32/63, loss = 0.6852\n",
            "epoch 1153 / 2000, step 48/63, loss = 0.5333\n",
            "epoch 1154 / 2000, step 16/63, loss = 0.4751\n",
            "epoch 1154 / 2000, step 32/63, loss = 0.6939\n",
            "epoch 1154 / 2000, step 48/63, loss = 0.6088\n",
            "epoch 1155 / 2000, step 16/63, loss = 0.5114\n",
            "epoch 1155 / 2000, step 32/63, loss = 0.7708\n",
            "epoch 1155 / 2000, step 48/63, loss = 0.5248\n",
            "epoch 1156 / 2000, step 16/63, loss = 0.4538\n",
            "epoch 1156 / 2000, step 32/63, loss = 0.5644\n",
            "epoch 1156 / 2000, step 48/63, loss = 0.3481\n",
            "epoch 1157 / 2000, step 16/63, loss = 0.7722\n",
            "epoch 1157 / 2000, step 32/63, loss = 0.6006\n",
            "epoch 1157 / 2000, step 48/63, loss = 0.6069\n",
            "epoch 1158 / 2000, step 16/63, loss = 0.4837\n",
            "epoch 1158 / 2000, step 32/63, loss = 0.4914\n",
            "epoch 1158 / 2000, step 48/63, loss = 0.4776\n",
            "epoch 1159 / 2000, step 16/63, loss = 0.4442\n",
            "epoch 1159 / 2000, step 32/63, loss = 0.6865\n",
            "epoch 1159 / 2000, step 48/63, loss = 0.4896\n",
            "epoch 1160 / 2000, step 16/63, loss = 0.4671\n",
            "epoch 1160 / 2000, step 32/63, loss = 0.4481\n",
            "epoch 1160 / 2000, step 48/63, loss = 0.5712\n",
            "epoch 1161 / 2000, step 16/63, loss = 0.5592\n",
            "epoch 1161 / 2000, step 32/63, loss = 0.4920\n",
            "epoch 1161 / 2000, step 48/63, loss = 0.4690\n",
            "epoch 1162 / 2000, step 16/63, loss = 0.7354\n",
            "epoch 1162 / 2000, step 32/63, loss = 0.5192\n",
            "epoch 1162 / 2000, step 48/63, loss = 0.5887\n",
            "epoch 1163 / 2000, step 16/63, loss = 0.4372\n",
            "epoch 1163 / 2000, step 32/63, loss = 0.5228\n",
            "epoch 1163 / 2000, step 48/63, loss = 0.6774\n",
            "epoch 1164 / 2000, step 16/63, loss = 0.5271\n",
            "epoch 1164 / 2000, step 32/63, loss = 0.5126\n",
            "epoch 1164 / 2000, step 48/63, loss = 0.8844\n",
            "epoch 1165 / 2000, step 16/63, loss = 0.3822\n",
            "epoch 1165 / 2000, step 32/63, loss = 0.5799\n",
            "epoch 1165 / 2000, step 48/63, loss = 0.6562\n",
            "epoch 1166 / 2000, step 16/63, loss = 0.5433\n",
            "epoch 1166 / 2000, step 32/63, loss = 0.5612\n",
            "epoch 1166 / 2000, step 48/63, loss = 0.6059\n",
            "epoch 1167 / 2000, step 16/63, loss = 0.6244\n",
            "epoch 1167 / 2000, step 32/63, loss = 0.5681\n",
            "epoch 1167 / 2000, step 48/63, loss = 0.6084\n",
            "epoch 1168 / 2000, step 16/63, loss = 0.6945\n",
            "epoch 1168 / 2000, step 32/63, loss = 0.4824\n",
            "epoch 1168 / 2000, step 48/63, loss = 0.5291\n",
            "epoch 1169 / 2000, step 16/63, loss = 0.5910\n",
            "epoch 1169 / 2000, step 32/63, loss = 0.4074\n",
            "epoch 1169 / 2000, step 48/63, loss = 0.4198\n",
            "epoch 1170 / 2000, step 16/63, loss = 1.1955\n",
            "epoch 1170 / 2000, step 32/63, loss = 0.7154\n",
            "epoch 1170 / 2000, step 48/63, loss = 0.5298\n",
            "epoch 1171 / 2000, step 16/63, loss = 0.5438\n",
            "epoch 1171 / 2000, step 32/63, loss = 0.4704\n",
            "epoch 1171 / 2000, step 48/63, loss = 0.6098\n",
            "epoch 1172 / 2000, step 16/63, loss = 0.4040\n",
            "epoch 1172 / 2000, step 32/63, loss = 0.5447\n",
            "epoch 1172 / 2000, step 48/63, loss = 0.4929\n",
            "epoch 1173 / 2000, step 16/63, loss = 0.5702\n",
            "epoch 1173 / 2000, step 32/63, loss = 0.7819\n",
            "epoch 1173 / 2000, step 48/63, loss = 0.7771\n",
            "epoch 1174 / 2000, step 16/63, loss = 0.4462\n",
            "epoch 1174 / 2000, step 32/63, loss = 0.8026\n",
            "epoch 1174 / 2000, step 48/63, loss = 0.5761\n",
            "epoch 1175 / 2000, step 16/63, loss = 0.5494\n",
            "epoch 1175 / 2000, step 32/63, loss = 0.7049\n",
            "epoch 1175 / 2000, step 48/63, loss = 0.5061\n",
            "epoch 1176 / 2000, step 16/63, loss = 0.6627\n",
            "epoch 1176 / 2000, step 32/63, loss = 0.3575\n",
            "epoch 1176 / 2000, step 48/63, loss = 0.4400\n",
            "epoch 1177 / 2000, step 16/63, loss = 0.4525\n",
            "epoch 1177 / 2000, step 32/63, loss = 0.6699\n",
            "epoch 1177 / 2000, step 48/63, loss = 0.6445\n",
            "epoch 1178 / 2000, step 16/63, loss = 0.6172\n",
            "epoch 1178 / 2000, step 32/63, loss = 0.5779\n",
            "epoch 1178 / 2000, step 48/63, loss = 0.8976\n",
            "epoch 1179 / 2000, step 16/63, loss = 0.5110\n",
            "epoch 1179 / 2000, step 32/63, loss = 0.5066\n",
            "epoch 1179 / 2000, step 48/63, loss = 0.5687\n",
            "epoch 1180 / 2000, step 16/63, loss = 0.4190\n",
            "epoch 1180 / 2000, step 32/63, loss = 0.6480\n",
            "epoch 1180 / 2000, step 48/63, loss = 0.4065\n",
            "epoch 1181 / 2000, step 16/63, loss = 0.5514\n",
            "epoch 1181 / 2000, step 32/63, loss = 0.4696\n",
            "epoch 1181 / 2000, step 48/63, loss = 0.6310\n",
            "epoch 1182 / 2000, step 16/63, loss = 0.6903\n",
            "epoch 1182 / 2000, step 32/63, loss = 0.4667\n",
            "epoch 1182 / 2000, step 48/63, loss = 0.6109\n",
            "epoch 1183 / 2000, step 16/63, loss = 0.4112\n",
            "epoch 1183 / 2000, step 32/63, loss = 0.7613\n",
            "epoch 1183 / 2000, step 48/63, loss = 0.5198\n",
            "epoch 1184 / 2000, step 16/63, loss = 0.6154\n",
            "epoch 1184 / 2000, step 32/63, loss = 0.4553\n",
            "epoch 1184 / 2000, step 48/63, loss = 0.6482\n",
            "epoch 1185 / 2000, step 16/63, loss = 0.5703\n",
            "epoch 1185 / 2000, step 32/63, loss = 0.7336\n",
            "epoch 1185 / 2000, step 48/63, loss = 0.3457\n",
            "epoch 1186 / 2000, step 16/63, loss = 0.5716\n",
            "epoch 1186 / 2000, step 32/63, loss = 0.8103\n",
            "epoch 1186 / 2000, step 48/63, loss = 0.6093\n",
            "epoch 1187 / 2000, step 16/63, loss = 0.6758\n",
            "epoch 1187 / 2000, step 32/63, loss = 0.6004\n",
            "epoch 1187 / 2000, step 48/63, loss = 0.7206\n",
            "epoch 1188 / 2000, step 16/63, loss = 0.7352\n",
            "epoch 1188 / 2000, step 32/63, loss = 0.4339\n",
            "epoch 1188 / 2000, step 48/63, loss = 0.5058\n",
            "epoch 1189 / 2000, step 16/63, loss = 0.7027\n",
            "epoch 1189 / 2000, step 32/63, loss = 0.7435\n",
            "epoch 1189 / 2000, step 48/63, loss = 0.7467\n",
            "epoch 1190 / 2000, step 16/63, loss = 0.4078\n",
            "epoch 1190 / 2000, step 32/63, loss = 0.6543\n",
            "epoch 1190 / 2000, step 48/63, loss = 0.4974\n",
            "epoch 1191 / 2000, step 16/63, loss = 0.5021\n",
            "epoch 1191 / 2000, step 32/63, loss = 0.5421\n",
            "epoch 1191 / 2000, step 48/63, loss = 0.6215\n",
            "epoch 1192 / 2000, step 16/63, loss = 0.5853\n",
            "epoch 1192 / 2000, step 32/63, loss = 0.4659\n",
            "epoch 1192 / 2000, step 48/63, loss = 0.4984\n",
            "epoch 1193 / 2000, step 16/63, loss = 0.7156\n",
            "epoch 1193 / 2000, step 32/63, loss = 0.7788\n",
            "epoch 1193 / 2000, step 48/63, loss = 0.5342\n",
            "epoch 1194 / 2000, step 16/63, loss = 0.5623\n",
            "epoch 1194 / 2000, step 32/63, loss = 0.6738\n",
            "epoch 1194 / 2000, step 48/63, loss = 0.3956\n",
            "epoch 1195 / 2000, step 16/63, loss = 0.4956\n",
            "epoch 1195 / 2000, step 32/63, loss = 0.6150\n",
            "epoch 1195 / 2000, step 48/63, loss = 0.5591\n",
            "epoch 1196 / 2000, step 16/63, loss = 0.4037\n",
            "epoch 1196 / 2000, step 32/63, loss = 0.9196\n",
            "epoch 1196 / 2000, step 48/63, loss = 0.5822\n",
            "epoch 1197 / 2000, step 16/63, loss = 0.5085\n",
            "epoch 1197 / 2000, step 32/63, loss = 0.5330\n",
            "epoch 1197 / 2000, step 48/63, loss = 0.7529\n",
            "epoch 1198 / 2000, step 16/63, loss = 0.6370\n",
            "epoch 1198 / 2000, step 32/63, loss = 0.8332\n",
            "epoch 1198 / 2000, step 48/63, loss = 0.5032\n",
            "epoch 1199 / 2000, step 16/63, loss = 0.5133\n",
            "epoch 1199 / 2000, step 32/63, loss = 0.7100\n",
            "epoch 1199 / 2000, step 48/63, loss = 0.4066\n",
            "epoch 1200 / 2000, step 16/63, loss = 0.7857\n",
            "epoch 1200 / 2000, step 32/63, loss = 0.5421\n",
            "epoch 1200 / 2000, step 48/63, loss = 0.4841\n",
            "epoch 1201 / 2000, step 16/63, loss = 0.4185\n",
            "epoch 1201 / 2000, step 32/63, loss = 0.6574\n",
            "epoch 1201 / 2000, step 48/63, loss = 0.3785\n",
            "epoch 1202 / 2000, step 16/63, loss = 0.5039\n",
            "epoch 1202 / 2000, step 32/63, loss = 0.5428\n",
            "epoch 1202 / 2000, step 48/63, loss = 0.6438\n",
            "epoch 1203 / 2000, step 16/63, loss = 0.4757\n",
            "epoch 1203 / 2000, step 32/63, loss = 0.5389\n",
            "epoch 1203 / 2000, step 48/63, loss = 0.6785\n",
            "epoch 1204 / 2000, step 16/63, loss = 0.4457\n",
            "epoch 1204 / 2000, step 32/63, loss = 0.6944\n",
            "epoch 1204 / 2000, step 48/63, loss = 0.4817\n",
            "epoch 1205 / 2000, step 16/63, loss = 0.4344\n",
            "epoch 1205 / 2000, step 32/63, loss = 0.3908\n",
            "epoch 1205 / 2000, step 48/63, loss = 0.8506\n",
            "epoch 1206 / 2000, step 16/63, loss = 0.6078\n",
            "epoch 1206 / 2000, step 32/63, loss = 0.5793\n",
            "epoch 1206 / 2000, step 48/63, loss = 0.3637\n",
            "epoch 1207 / 2000, step 16/63, loss = 0.4915\n",
            "epoch 1207 / 2000, step 32/63, loss = 0.4965\n",
            "epoch 1207 / 2000, step 48/63, loss = 0.6074\n",
            "epoch 1208 / 2000, step 16/63, loss = 0.3836\n",
            "epoch 1208 / 2000, step 32/63, loss = 0.5794\n",
            "epoch 1208 / 2000, step 48/63, loss = 0.3827\n",
            "epoch 1209 / 2000, step 16/63, loss = 0.4691\n",
            "epoch 1209 / 2000, step 32/63, loss = 0.6557\n",
            "epoch 1209 / 2000, step 48/63, loss = 0.6537\n",
            "epoch 1210 / 2000, step 16/63, loss = 0.5816\n",
            "epoch 1210 / 2000, step 32/63, loss = 0.4417\n",
            "epoch 1210 / 2000, step 48/63, loss = 0.4921\n",
            "epoch 1211 / 2000, step 16/63, loss = 0.4391\n",
            "epoch 1211 / 2000, step 32/63, loss = 0.4162\n",
            "epoch 1211 / 2000, step 48/63, loss = 0.6191\n",
            "epoch 1212 / 2000, step 16/63, loss = 0.6382\n",
            "epoch 1212 / 2000, step 32/63, loss = 0.4985\n",
            "epoch 1212 / 2000, step 48/63, loss = 0.8148\n",
            "epoch 1213 / 2000, step 16/63, loss = 0.6788\n",
            "epoch 1213 / 2000, step 32/63, loss = 0.3193\n",
            "epoch 1213 / 2000, step 48/63, loss = 0.4906\n",
            "epoch 1214 / 2000, step 16/63, loss = 0.5502\n",
            "epoch 1214 / 2000, step 32/63, loss = 0.7239\n",
            "epoch 1214 / 2000, step 48/63, loss = 0.5891\n",
            "epoch 1215 / 2000, step 16/63, loss = 0.3783\n",
            "epoch 1215 / 2000, step 32/63, loss = 0.5262\n",
            "epoch 1215 / 2000, step 48/63, loss = 0.5213\n",
            "epoch 1216 / 2000, step 16/63, loss = 0.4553\n",
            "epoch 1216 / 2000, step 32/63, loss = 0.5011\n",
            "epoch 1216 / 2000, step 48/63, loss = 0.5498\n",
            "epoch 1217 / 2000, step 16/63, loss = 0.3812\n",
            "epoch 1217 / 2000, step 32/63, loss = 0.5880\n",
            "epoch 1217 / 2000, step 48/63, loss = 0.5313\n",
            "epoch 1218 / 2000, step 16/63, loss = 0.7071\n",
            "epoch 1218 / 2000, step 32/63, loss = 0.4498\n",
            "epoch 1218 / 2000, step 48/63, loss = 0.5682\n",
            "epoch 1219 / 2000, step 16/63, loss = 0.3765\n",
            "epoch 1219 / 2000, step 32/63, loss = 0.5834\n",
            "epoch 1219 / 2000, step 48/63, loss = 0.8041\n",
            "epoch 1220 / 2000, step 16/63, loss = 0.4458\n",
            "epoch 1220 / 2000, step 32/63, loss = 0.7282\n",
            "epoch 1220 / 2000, step 48/63, loss = 0.5187\n",
            "epoch 1221 / 2000, step 16/63, loss = 0.5626\n",
            "epoch 1221 / 2000, step 32/63, loss = 0.5404\n",
            "epoch 1221 / 2000, step 48/63, loss = 0.3653\n",
            "epoch 1222 / 2000, step 16/63, loss = 0.5438\n",
            "epoch 1222 / 2000, step 32/63, loss = 0.4582\n",
            "epoch 1222 / 2000, step 48/63, loss = 0.5927\n",
            "epoch 1223 / 2000, step 16/63, loss = 0.4896\n",
            "epoch 1223 / 2000, step 32/63, loss = 0.6079\n",
            "epoch 1223 / 2000, step 48/63, loss = 0.6243\n",
            "epoch 1224 / 2000, step 16/63, loss = 0.3221\n",
            "epoch 1224 / 2000, step 32/63, loss = 0.8158\n",
            "epoch 1224 / 2000, step 48/63, loss = 0.6604\n",
            "epoch 1225 / 2000, step 16/63, loss = 0.4436\n",
            "epoch 1225 / 2000, step 32/63, loss = 0.4091\n",
            "epoch 1225 / 2000, step 48/63, loss = 0.5842\n",
            "epoch 1226 / 2000, step 16/63, loss = 0.6375\n",
            "epoch 1226 / 2000, step 32/63, loss = 0.8130\n",
            "epoch 1226 / 2000, step 48/63, loss = 0.6005\n",
            "epoch 1227 / 2000, step 16/63, loss = 0.5972\n",
            "epoch 1227 / 2000, step 32/63, loss = 0.4936\n",
            "epoch 1227 / 2000, step 48/63, loss = 0.5334\n",
            "epoch 1228 / 2000, step 16/63, loss = 0.5613\n",
            "epoch 1228 / 2000, step 32/63, loss = 0.4793\n",
            "epoch 1228 / 2000, step 48/63, loss = 0.7221\n",
            "epoch 1229 / 2000, step 16/63, loss = 0.6354\n",
            "epoch 1229 / 2000, step 32/63, loss = 0.7255\n",
            "epoch 1229 / 2000, step 48/63, loss = 0.4051\n",
            "epoch 1230 / 2000, step 16/63, loss = 0.4639\n",
            "epoch 1230 / 2000, step 32/63, loss = 0.5704\n",
            "epoch 1230 / 2000, step 48/63, loss = 0.6601\n",
            "epoch 1231 / 2000, step 16/63, loss = 0.5265\n",
            "epoch 1231 / 2000, step 32/63, loss = 0.4896\n",
            "epoch 1231 / 2000, step 48/63, loss = 0.5066\n",
            "epoch 1232 / 2000, step 16/63, loss = 0.6170\n",
            "epoch 1232 / 2000, step 32/63, loss = 0.7011\n",
            "epoch 1232 / 2000, step 48/63, loss = 0.3766\n",
            "epoch 1233 / 2000, step 16/63, loss = 0.4613\n",
            "epoch 1233 / 2000, step 32/63, loss = 0.5332\n",
            "epoch 1233 / 2000, step 48/63, loss = 0.5397\n",
            "epoch 1234 / 2000, step 16/63, loss = 0.6159\n",
            "epoch 1234 / 2000, step 32/63, loss = 0.8203\n",
            "epoch 1234 / 2000, step 48/63, loss = 0.6720\n",
            "epoch 1235 / 2000, step 16/63, loss = 0.7457\n",
            "epoch 1235 / 2000, step 32/63, loss = 0.6801\n",
            "epoch 1235 / 2000, step 48/63, loss = 0.6214\n",
            "epoch 1236 / 2000, step 16/63, loss = 0.8318\n",
            "epoch 1236 / 2000, step 32/63, loss = 0.5872\n",
            "epoch 1236 / 2000, step 48/63, loss = 0.6180\n",
            "epoch 1237 / 2000, step 16/63, loss = 0.5820\n",
            "epoch 1237 / 2000, step 32/63, loss = 0.7206\n",
            "epoch 1237 / 2000, step 48/63, loss = 0.4596\n",
            "epoch 1238 / 2000, step 16/63, loss = 0.6567\n",
            "epoch 1238 / 2000, step 32/63, loss = 0.5680\n",
            "epoch 1238 / 2000, step 48/63, loss = 0.5046\n",
            "epoch 1239 / 2000, step 16/63, loss = 0.6823\n",
            "epoch 1239 / 2000, step 32/63, loss = 0.7331\n",
            "epoch 1239 / 2000, step 48/63, loss = 0.5315\n",
            "epoch 1240 / 2000, step 16/63, loss = 0.8044\n",
            "epoch 1240 / 2000, step 32/63, loss = 0.5137\n",
            "epoch 1240 / 2000, step 48/63, loss = 0.6650\n",
            "epoch 1241 / 2000, step 16/63, loss = 0.4811\n",
            "epoch 1241 / 2000, step 32/63, loss = 0.6392\n",
            "epoch 1241 / 2000, step 48/63, loss = 0.4912\n",
            "epoch 1242 / 2000, step 16/63, loss = 0.4363\n",
            "epoch 1242 / 2000, step 32/63, loss = 0.5153\n",
            "epoch 1242 / 2000, step 48/63, loss = 0.3920\n",
            "epoch 1243 / 2000, step 16/63, loss = 0.4079\n",
            "epoch 1243 / 2000, step 32/63, loss = 0.3613\n",
            "epoch 1243 / 2000, step 48/63, loss = 0.4166\n",
            "epoch 1244 / 2000, step 16/63, loss = 0.5543\n",
            "epoch 1244 / 2000, step 32/63, loss = 0.5891\n",
            "epoch 1244 / 2000, step 48/63, loss = 0.6840\n",
            "epoch 1245 / 2000, step 16/63, loss = 0.5593\n",
            "epoch 1245 / 2000, step 32/63, loss = 0.5934\n",
            "epoch 1245 / 2000, step 48/63, loss = 0.6552\n",
            "epoch 1246 / 2000, step 16/63, loss = 0.4120\n",
            "epoch 1246 / 2000, step 32/63, loss = 0.4009\n",
            "epoch 1246 / 2000, step 48/63, loss = 0.4967\n",
            "epoch 1247 / 2000, step 16/63, loss = 0.5848\n",
            "epoch 1247 / 2000, step 32/63, loss = 0.4996\n",
            "epoch 1247 / 2000, step 48/63, loss = 0.5769\n",
            "epoch 1248 / 2000, step 16/63, loss = 0.4513\n",
            "epoch 1248 / 2000, step 32/63, loss = 0.5794\n",
            "epoch 1248 / 2000, step 48/63, loss = 0.4847\n",
            "epoch 1249 / 2000, step 16/63, loss = 0.6153\n",
            "epoch 1249 / 2000, step 32/63, loss = 0.3593\n",
            "epoch 1249 / 2000, step 48/63, loss = 0.4488\n",
            "epoch 1250 / 2000, step 16/63, loss = 0.5394\n",
            "epoch 1250 / 2000, step 32/63, loss = 0.4143\n",
            "epoch 1250 / 2000, step 48/63, loss = 0.5220\n",
            "epoch 1251 / 2000, step 16/63, loss = 0.4808\n",
            "epoch 1251 / 2000, step 32/63, loss = 0.5661\n",
            "epoch 1251 / 2000, step 48/63, loss = 0.6931\n",
            "epoch 1252 / 2000, step 16/63, loss = 0.6511\n",
            "epoch 1252 / 2000, step 32/63, loss = 0.3777\n",
            "epoch 1252 / 2000, step 48/63, loss = 0.5792\n",
            "epoch 1253 / 2000, step 16/63, loss = 0.6200\n",
            "epoch 1253 / 2000, step 32/63, loss = 0.4913\n",
            "epoch 1253 / 2000, step 48/63, loss = 0.4817\n",
            "epoch 1254 / 2000, step 16/63, loss = 0.6552\n",
            "epoch 1254 / 2000, step 32/63, loss = 0.5812\n",
            "epoch 1254 / 2000, step 48/63, loss = 0.3196\n",
            "epoch 1255 / 2000, step 16/63, loss = 0.4995\n",
            "epoch 1255 / 2000, step 32/63, loss = 0.6080\n",
            "epoch 1255 / 2000, step 48/63, loss = 0.3951\n",
            "epoch 1256 / 2000, step 16/63, loss = 0.4750\n",
            "epoch 1256 / 2000, step 32/63, loss = 0.7623\n",
            "epoch 1256 / 2000, step 48/63, loss = 0.6496\n",
            "epoch 1257 / 2000, step 16/63, loss = 0.5062\n",
            "epoch 1257 / 2000, step 32/63, loss = 0.4215\n",
            "epoch 1257 / 2000, step 48/63, loss = 0.6204\n",
            "epoch 1258 / 2000, step 16/63, loss = 0.5309\n",
            "epoch 1258 / 2000, step 32/63, loss = 0.3615\n",
            "epoch 1258 / 2000, step 48/63, loss = 0.6714\n",
            "epoch 1259 / 2000, step 16/63, loss = 0.5443\n",
            "epoch 1259 / 2000, step 32/63, loss = 0.7736\n",
            "epoch 1259 / 2000, step 48/63, loss = 0.5512\n",
            "epoch 1260 / 2000, step 16/63, loss = 0.4993\n",
            "epoch 1260 / 2000, step 32/63, loss = 0.3471\n",
            "epoch 1260 / 2000, step 48/63, loss = 0.6860\n",
            "epoch 1261 / 2000, step 16/63, loss = 0.6923\n",
            "epoch 1261 / 2000, step 32/63, loss = 0.3581\n",
            "epoch 1261 / 2000, step 48/63, loss = 0.7924\n",
            "epoch 1262 / 2000, step 16/63, loss = 0.6308\n",
            "epoch 1262 / 2000, step 32/63, loss = 0.4931\n",
            "epoch 1262 / 2000, step 48/63, loss = 0.6898\n",
            "epoch 1263 / 2000, step 16/63, loss = 0.5585\n",
            "epoch 1263 / 2000, step 32/63, loss = 0.4253\n",
            "epoch 1263 / 2000, step 48/63, loss = 0.6112\n",
            "epoch 1264 / 2000, step 16/63, loss = 0.5405\n",
            "epoch 1264 / 2000, step 32/63, loss = 0.4528\n",
            "epoch 1264 / 2000, step 48/63, loss = 0.4080\n",
            "epoch 1265 / 2000, step 16/63, loss = 0.5041\n",
            "epoch 1265 / 2000, step 32/63, loss = 0.5571\n",
            "epoch 1265 / 2000, step 48/63, loss = 0.5412\n",
            "epoch 1266 / 2000, step 16/63, loss = 0.3954\n",
            "epoch 1266 / 2000, step 32/63, loss = 0.5257\n",
            "epoch 1266 / 2000, step 48/63, loss = 0.4794\n",
            "epoch 1267 / 2000, step 16/63, loss = 0.3698\n",
            "epoch 1267 / 2000, step 32/63, loss = 0.3490\n",
            "epoch 1267 / 2000, step 48/63, loss = 0.5655\n",
            "epoch 1268 / 2000, step 16/63, loss = 0.5576\n",
            "epoch 1268 / 2000, step 32/63, loss = 0.3414\n",
            "epoch 1268 / 2000, step 48/63, loss = 0.5294\n",
            "epoch 1269 / 2000, step 16/63, loss = 0.5672\n",
            "epoch 1269 / 2000, step 32/63, loss = 0.5268\n",
            "epoch 1269 / 2000, step 48/63, loss = 0.5767\n",
            "epoch 1270 / 2000, step 16/63, loss = 0.6323\n",
            "epoch 1270 / 2000, step 32/63, loss = 0.7212\n",
            "epoch 1270 / 2000, step 48/63, loss = 0.5972\n",
            "epoch 1271 / 2000, step 16/63, loss = 0.5909\n",
            "epoch 1271 / 2000, step 32/63, loss = 0.6399\n",
            "epoch 1271 / 2000, step 48/63, loss = 0.3184\n",
            "epoch 1272 / 2000, step 16/63, loss = 0.8274\n",
            "epoch 1272 / 2000, step 32/63, loss = 0.3075\n",
            "epoch 1272 / 2000, step 48/63, loss = 0.4649\n",
            "epoch 1273 / 2000, step 16/63, loss = 0.5580\n",
            "epoch 1273 / 2000, step 32/63, loss = 0.6288\n",
            "epoch 1273 / 2000, step 48/63, loss = 0.6482\n",
            "epoch 1274 / 2000, step 16/63, loss = 0.5272\n",
            "epoch 1274 / 2000, step 32/63, loss = 0.3972\n",
            "epoch 1274 / 2000, step 48/63, loss = 0.6269\n",
            "epoch 1275 / 2000, step 16/63, loss = 0.5777\n",
            "epoch 1275 / 2000, step 32/63, loss = 0.3485\n",
            "epoch 1275 / 2000, step 48/63, loss = 0.7179\n",
            "epoch 1276 / 2000, step 16/63, loss = 0.6842\n",
            "epoch 1276 / 2000, step 32/63, loss = 0.5138\n",
            "epoch 1276 / 2000, step 48/63, loss = 0.5927\n",
            "epoch 1277 / 2000, step 16/63, loss = 0.5963\n",
            "epoch 1277 / 2000, step 32/63, loss = 0.5006\n",
            "epoch 1277 / 2000, step 48/63, loss = 0.6154\n",
            "epoch 1278 / 2000, step 16/63, loss = 0.6058\n",
            "epoch 1278 / 2000, step 32/63, loss = 0.3217\n",
            "epoch 1278 / 2000, step 48/63, loss = 0.4554\n",
            "epoch 1279 / 2000, step 16/63, loss = 0.4689\n",
            "epoch 1279 / 2000, step 32/63, loss = 0.4376\n",
            "epoch 1279 / 2000, step 48/63, loss = 0.4873\n",
            "epoch 1280 / 2000, step 16/63, loss = 0.6078\n",
            "epoch 1280 / 2000, step 32/63, loss = 0.4619\n",
            "epoch 1280 / 2000, step 48/63, loss = 0.6064\n",
            "epoch 1281 / 2000, step 16/63, loss = 0.5001\n",
            "epoch 1281 / 2000, step 32/63, loss = 0.6303\n",
            "epoch 1281 / 2000, step 48/63, loss = 0.5554\n",
            "epoch 1282 / 2000, step 16/63, loss = 0.5088\n",
            "epoch 1282 / 2000, step 32/63, loss = 0.7049\n",
            "epoch 1282 / 2000, step 48/63, loss = 0.6065\n",
            "epoch 1283 / 2000, step 16/63, loss = 0.8003\n",
            "epoch 1283 / 2000, step 32/63, loss = 0.8228\n",
            "epoch 1283 / 2000, step 48/63, loss = 0.3722\n",
            "epoch 1284 / 2000, step 16/63, loss = 0.6149\n",
            "epoch 1284 / 2000, step 32/63, loss = 0.6507\n",
            "epoch 1284 / 2000, step 48/63, loss = 0.8413\n",
            "epoch 1285 / 2000, step 16/63, loss = 0.3443\n",
            "epoch 1285 / 2000, step 32/63, loss = 0.6353\n",
            "epoch 1285 / 2000, step 48/63, loss = 0.4547\n",
            "epoch 1286 / 2000, step 16/63, loss = 0.4951\n",
            "epoch 1286 / 2000, step 32/63, loss = 0.4888\n",
            "epoch 1286 / 2000, step 48/63, loss = 0.3841\n",
            "epoch 1287 / 2000, step 16/63, loss = 0.5737\n",
            "epoch 1287 / 2000, step 32/63, loss = 0.3504\n",
            "epoch 1287 / 2000, step 48/63, loss = 0.5270\n",
            "epoch 1288 / 2000, step 16/63, loss = 0.3869\n",
            "epoch 1288 / 2000, step 32/63, loss = 0.3398\n",
            "epoch 1288 / 2000, step 48/63, loss = 0.6687\n",
            "epoch 1289 / 2000, step 16/63, loss = 0.2503\n",
            "epoch 1289 / 2000, step 32/63, loss = 0.4398\n",
            "epoch 1289 / 2000, step 48/63, loss = 0.4397\n",
            "epoch 1290 / 2000, step 16/63, loss = 0.5546\n",
            "epoch 1290 / 2000, step 32/63, loss = 0.5341\n",
            "epoch 1290 / 2000, step 48/63, loss = 0.5630\n",
            "epoch 1291 / 2000, step 16/63, loss = 0.6505\n",
            "epoch 1291 / 2000, step 32/63, loss = 0.9843\n",
            "epoch 1291 / 2000, step 48/63, loss = 0.5162\n",
            "epoch 1292 / 2000, step 16/63, loss = 0.6570\n",
            "epoch 1292 / 2000, step 32/63, loss = 0.5443\n",
            "epoch 1292 / 2000, step 48/63, loss = 0.6443\n",
            "epoch 1293 / 2000, step 16/63, loss = 0.5346\n",
            "epoch 1293 / 2000, step 32/63, loss = 0.5445\n",
            "epoch 1293 / 2000, step 48/63, loss = 0.5970\n",
            "epoch 1294 / 2000, step 16/63, loss = 0.5686\n",
            "epoch 1294 / 2000, step 32/63, loss = 0.9445\n",
            "epoch 1294 / 2000, step 48/63, loss = 0.5535\n",
            "epoch 1295 / 2000, step 16/63, loss = 0.4555\n",
            "epoch 1295 / 2000, step 32/63, loss = 0.3408\n",
            "epoch 1295 / 2000, step 48/63, loss = 0.4990\n",
            "epoch 1296 / 2000, step 16/63, loss = 0.3603\n",
            "epoch 1296 / 2000, step 32/63, loss = 0.4979\n",
            "epoch 1296 / 2000, step 48/63, loss = 0.4710\n",
            "epoch 1297 / 2000, step 16/63, loss = 0.5745\n",
            "epoch 1297 / 2000, step 32/63, loss = 0.4650\n",
            "epoch 1297 / 2000, step 48/63, loss = 0.4564\n",
            "epoch 1298 / 2000, step 16/63, loss = 0.2882\n",
            "epoch 1298 / 2000, step 32/63, loss = 0.3192\n",
            "epoch 1298 / 2000, step 48/63, loss = 0.3335\n",
            "epoch 1299 / 2000, step 16/63, loss = 0.4134\n",
            "epoch 1299 / 2000, step 32/63, loss = 0.5209\n",
            "epoch 1299 / 2000, step 48/63, loss = 0.3777\n",
            "epoch 1300 / 2000, step 16/63, loss = 0.5789\n",
            "epoch 1300 / 2000, step 32/63, loss = 0.5765\n",
            "epoch 1300 / 2000, step 48/63, loss = 0.4599\n",
            "epoch 1301 / 2000, step 16/63, loss = 0.5386\n",
            "epoch 1301 / 2000, step 32/63, loss = 0.3264\n",
            "epoch 1301 / 2000, step 48/63, loss = 0.5341\n",
            "epoch 1302 / 2000, step 16/63, loss = 0.4516\n",
            "epoch 1302 / 2000, step 32/63, loss = 0.3330\n",
            "epoch 1302 / 2000, step 48/63, loss = 0.6189\n",
            "epoch 1303 / 2000, step 16/63, loss = 0.5289\n",
            "epoch 1303 / 2000, step 32/63, loss = 0.6304\n",
            "epoch 1303 / 2000, step 48/63, loss = 0.4985\n",
            "epoch 1304 / 2000, step 16/63, loss = 0.5551\n",
            "epoch 1304 / 2000, step 32/63, loss = 0.6478\n",
            "epoch 1304 / 2000, step 48/63, loss = 0.6966\n",
            "epoch 1305 / 2000, step 16/63, loss = 0.5403\n",
            "epoch 1305 / 2000, step 32/63, loss = 0.4109\n",
            "epoch 1305 / 2000, step 48/63, loss = 0.2771\n",
            "epoch 1306 / 2000, step 16/63, loss = 0.3370\n",
            "epoch 1306 / 2000, step 32/63, loss = 0.5591\n",
            "epoch 1306 / 2000, step 48/63, loss = 0.5760\n",
            "epoch 1307 / 2000, step 16/63, loss = 0.3776\n",
            "epoch 1307 / 2000, step 32/63, loss = 0.5538\n",
            "epoch 1307 / 2000, step 48/63, loss = 0.6614\n",
            "epoch 1308 / 2000, step 16/63, loss = 0.5277\n",
            "epoch 1308 / 2000, step 32/63, loss = 0.3802\n",
            "epoch 1308 / 2000, step 48/63, loss = 0.5462\n",
            "epoch 1309 / 2000, step 16/63, loss = 0.3893\n",
            "epoch 1309 / 2000, step 32/63, loss = 0.7396\n",
            "epoch 1309 / 2000, step 48/63, loss = 0.3782\n",
            "epoch 1310 / 2000, step 16/63, loss = 0.7743\n",
            "epoch 1310 / 2000, step 32/63, loss = 0.4835\n",
            "epoch 1310 / 2000, step 48/63, loss = 0.6933\n",
            "epoch 1311 / 2000, step 16/63, loss = 0.6204\n",
            "epoch 1311 / 2000, step 32/63, loss = 0.5679\n",
            "epoch 1311 / 2000, step 48/63, loss = 0.3832\n",
            "epoch 1312 / 2000, step 16/63, loss = 0.5354\n",
            "epoch 1312 / 2000, step 32/63, loss = 0.6844\n",
            "epoch 1312 / 2000, step 48/63, loss = 0.7377\n",
            "epoch 1313 / 2000, step 16/63, loss = 0.5034\n",
            "epoch 1313 / 2000, step 32/63, loss = 0.4556\n",
            "epoch 1313 / 2000, step 48/63, loss = 0.4855\n",
            "epoch 1314 / 2000, step 16/63, loss = 0.4092\n",
            "epoch 1314 / 2000, step 32/63, loss = 0.4265\n",
            "epoch 1314 / 2000, step 48/63, loss = 0.5976\n",
            "epoch 1315 / 2000, step 16/63, loss = 0.5684\n",
            "epoch 1315 / 2000, step 32/63, loss = 0.5497\n",
            "epoch 1315 / 2000, step 48/63, loss = 0.2942\n",
            "epoch 1316 / 2000, step 16/63, loss = 0.7185\n",
            "epoch 1316 / 2000, step 32/63, loss = 0.5708\n",
            "epoch 1316 / 2000, step 48/63, loss = 0.5501\n",
            "epoch 1317 / 2000, step 16/63, loss = 0.6786\n",
            "epoch 1317 / 2000, step 32/63, loss = 0.4005\n",
            "epoch 1317 / 2000, step 48/63, loss = 0.5073\n",
            "epoch 1318 / 2000, step 16/63, loss = 0.3593\n",
            "epoch 1318 / 2000, step 32/63, loss = 0.4405\n",
            "epoch 1318 / 2000, step 48/63, loss = 0.4553\n",
            "epoch 1319 / 2000, step 16/63, loss = 0.4283\n",
            "epoch 1319 / 2000, step 32/63, loss = 0.8272\n",
            "epoch 1319 / 2000, step 48/63, loss = 0.5567\n",
            "epoch 1320 / 2000, step 16/63, loss = 0.3956\n",
            "epoch 1320 / 2000, step 32/63, loss = 0.6031\n",
            "epoch 1320 / 2000, step 48/63, loss = 0.4611\n",
            "epoch 1321 / 2000, step 16/63, loss = 0.7014\n",
            "epoch 1321 / 2000, step 32/63, loss = 0.6036\n",
            "epoch 1321 / 2000, step 48/63, loss = 0.6131\n",
            "epoch 1322 / 2000, step 16/63, loss = 0.4877\n",
            "epoch 1322 / 2000, step 32/63, loss = 0.6081\n",
            "epoch 1322 / 2000, step 48/63, loss = 0.5936\n",
            "epoch 1323 / 2000, step 16/63, loss = 0.4707\n",
            "epoch 1323 / 2000, step 32/63, loss = 0.6069\n",
            "epoch 1323 / 2000, step 48/63, loss = 0.8836\n",
            "epoch 1324 / 2000, step 16/63, loss = 0.5116\n",
            "epoch 1324 / 2000, step 32/63, loss = 0.4052\n",
            "epoch 1324 / 2000, step 48/63, loss = 0.3947\n",
            "epoch 1325 / 2000, step 16/63, loss = 0.5266\n",
            "epoch 1325 / 2000, step 32/63, loss = 0.4418\n",
            "epoch 1325 / 2000, step 48/63, loss = 0.4780\n",
            "epoch 1326 / 2000, step 16/63, loss = 0.5068\n",
            "epoch 1326 / 2000, step 32/63, loss = 0.5020\n",
            "epoch 1326 / 2000, step 48/63, loss = 0.5190\n",
            "epoch 1327 / 2000, step 16/63, loss = 0.7847\n",
            "epoch 1327 / 2000, step 32/63, loss = 0.2959\n",
            "epoch 1327 / 2000, step 48/63, loss = 0.3730\n",
            "epoch 1328 / 2000, step 16/63, loss = 0.5798\n",
            "epoch 1328 / 2000, step 32/63, loss = 0.5465\n",
            "epoch 1328 / 2000, step 48/63, loss = 0.4391\n",
            "epoch 1329 / 2000, step 16/63, loss = 0.5681\n",
            "epoch 1329 / 2000, step 32/63, loss = 0.7476\n",
            "epoch 1329 / 2000, step 48/63, loss = 0.4229\n",
            "epoch 1330 / 2000, step 16/63, loss = 0.5531\n",
            "epoch 1330 / 2000, step 32/63, loss = 0.4971\n",
            "epoch 1330 / 2000, step 48/63, loss = 0.5470\n",
            "epoch 1331 / 2000, step 16/63, loss = 0.3785\n",
            "epoch 1331 / 2000, step 32/63, loss = 0.5678\n",
            "epoch 1331 / 2000, step 48/63, loss = 0.6867\n",
            "epoch 1332 / 2000, step 16/63, loss = 0.3723\n",
            "epoch 1332 / 2000, step 32/63, loss = 0.5062\n",
            "epoch 1332 / 2000, step 48/63, loss = 0.3991\n",
            "epoch 1333 / 2000, step 16/63, loss = 0.4111\n",
            "epoch 1333 / 2000, step 32/63, loss = 0.3794\n",
            "epoch 1333 / 2000, step 48/63, loss = 0.4699\n",
            "epoch 1334 / 2000, step 16/63, loss = 0.4243\n",
            "epoch 1334 / 2000, step 32/63, loss = 0.3527\n",
            "epoch 1334 / 2000, step 48/63, loss = 0.4632\n",
            "epoch 1335 / 2000, step 16/63, loss = 0.3911\n",
            "epoch 1335 / 2000, step 32/63, loss = 0.4093\n",
            "epoch 1335 / 2000, step 48/63, loss = 0.4327\n",
            "epoch 1336 / 2000, step 16/63, loss = 0.5290\n",
            "epoch 1336 / 2000, step 32/63, loss = 0.5876\n",
            "epoch 1336 / 2000, step 48/63, loss = 0.4657\n",
            "epoch 1337 / 2000, step 16/63, loss = 0.3825\n",
            "epoch 1337 / 2000, step 32/63, loss = 0.4179\n",
            "epoch 1337 / 2000, step 48/63, loss = 0.3438\n",
            "epoch 1338 / 2000, step 16/63, loss = 0.4775\n",
            "epoch 1338 / 2000, step 32/63, loss = 0.5427\n",
            "epoch 1338 / 2000, step 48/63, loss = 0.7709\n",
            "epoch 1339 / 2000, step 16/63, loss = 0.4393\n",
            "epoch 1339 / 2000, step 32/63, loss = 0.4685\n",
            "epoch 1339 / 2000, step 48/63, loss = 0.5655\n",
            "epoch 1340 / 2000, step 16/63, loss = 0.6454\n",
            "epoch 1340 / 2000, step 32/63, loss = 0.3548\n",
            "epoch 1340 / 2000, step 48/63, loss = 0.6165\n",
            "epoch 1341 / 2000, step 16/63, loss = 0.7287\n",
            "epoch 1341 / 2000, step 32/63, loss = 0.6076\n",
            "epoch 1341 / 2000, step 48/63, loss = 0.4385\n",
            "epoch 1342 / 2000, step 16/63, loss = 0.5386\n",
            "epoch 1342 / 2000, step 32/63, loss = 0.5268\n",
            "epoch 1342 / 2000, step 48/63, loss = 0.5722\n",
            "epoch 1343 / 2000, step 16/63, loss = 0.3622\n",
            "epoch 1343 / 2000, step 32/63, loss = 0.3822\n",
            "epoch 1343 / 2000, step 48/63, loss = 0.3987\n",
            "epoch 1344 / 2000, step 16/63, loss = 0.7874\n",
            "epoch 1344 / 2000, step 32/63, loss = 0.5146\n",
            "epoch 1344 / 2000, step 48/63, loss = 0.4806\n",
            "epoch 1345 / 2000, step 16/63, loss = 0.4597\n",
            "epoch 1345 / 2000, step 32/63, loss = 0.6246\n",
            "epoch 1345 / 2000, step 48/63, loss = 0.7448\n",
            "epoch 1346 / 2000, step 16/63, loss = 0.2910\n",
            "epoch 1346 / 2000, step 32/63, loss = 0.5840\n",
            "epoch 1346 / 2000, step 48/63, loss = 0.6352\n",
            "epoch 1347 / 2000, step 16/63, loss = 0.4243\n",
            "epoch 1347 / 2000, step 32/63, loss = 0.4345\n",
            "epoch 1347 / 2000, step 48/63, loss = 0.5571\n",
            "epoch 1348 / 2000, step 16/63, loss = 0.4104\n",
            "epoch 1348 / 2000, step 32/63, loss = 0.5225\n",
            "epoch 1348 / 2000, step 48/63, loss = 0.4974\n",
            "epoch 1349 / 2000, step 16/63, loss = 0.4158\n",
            "epoch 1349 / 2000, step 32/63, loss = 0.3913\n",
            "epoch 1349 / 2000, step 48/63, loss = 0.5378\n",
            "epoch 1350 / 2000, step 16/63, loss = 0.7113\n",
            "epoch 1350 / 2000, step 32/63, loss = 0.6790\n",
            "epoch 1350 / 2000, step 48/63, loss = 0.4316\n",
            "epoch 1351 / 2000, step 16/63, loss = 0.3771\n",
            "epoch 1351 / 2000, step 32/63, loss = 0.4618\n",
            "epoch 1351 / 2000, step 48/63, loss = 0.6364\n",
            "epoch 1352 / 2000, step 16/63, loss = 0.3514\n",
            "epoch 1352 / 2000, step 32/63, loss = 0.6791\n",
            "epoch 1352 / 2000, step 48/63, loss = 0.5440\n",
            "epoch 1353 / 2000, step 16/63, loss = 0.4595\n",
            "epoch 1353 / 2000, step 32/63, loss = 0.6377\n",
            "epoch 1353 / 2000, step 48/63, loss = 0.4455\n",
            "epoch 1354 / 2000, step 16/63, loss = 0.3663\n",
            "epoch 1354 / 2000, step 32/63, loss = 0.5544\n",
            "epoch 1354 / 2000, step 48/63, loss = 0.5303\n",
            "epoch 1355 / 2000, step 16/63, loss = 0.6456\n",
            "epoch 1355 / 2000, step 32/63, loss = 0.7096\n",
            "epoch 1355 / 2000, step 48/63, loss = 0.6571\n",
            "epoch 1356 / 2000, step 16/63, loss = 0.5231\n",
            "epoch 1356 / 2000, step 32/63, loss = 0.4083\n",
            "epoch 1356 / 2000, step 48/63, loss = 0.6648\n",
            "epoch 1357 / 2000, step 16/63, loss = 0.6753\n",
            "epoch 1357 / 2000, step 32/63, loss = 0.4221\n",
            "epoch 1357 / 2000, step 48/63, loss = 0.3900\n",
            "epoch 1358 / 2000, step 16/63, loss = 0.5710\n",
            "epoch 1358 / 2000, step 32/63, loss = 0.6143\n",
            "epoch 1358 / 2000, step 48/63, loss = 0.4945\n",
            "epoch 1359 / 2000, step 16/63, loss = 0.7817\n",
            "epoch 1359 / 2000, step 32/63, loss = 0.5286\n",
            "epoch 1359 / 2000, step 48/63, loss = 0.3430\n",
            "epoch 1360 / 2000, step 16/63, loss = 0.4682\n",
            "epoch 1360 / 2000, step 32/63, loss = 0.4100\n",
            "epoch 1360 / 2000, step 48/63, loss = 0.6198\n",
            "epoch 1361 / 2000, step 16/63, loss = 0.4113\n",
            "epoch 1361 / 2000, step 32/63, loss = 0.3988\n",
            "epoch 1361 / 2000, step 48/63, loss = 0.4843\n",
            "epoch 1362 / 2000, step 16/63, loss = 0.6259\n",
            "epoch 1362 / 2000, step 32/63, loss = 0.3964\n",
            "epoch 1362 / 2000, step 48/63, loss = 0.6433\n",
            "epoch 1363 / 2000, step 16/63, loss = 0.6383\n",
            "epoch 1363 / 2000, step 32/63, loss = 0.5149\n",
            "epoch 1363 / 2000, step 48/63, loss = 0.4755\n",
            "epoch 1364 / 2000, step 16/63, loss = 0.4467\n",
            "epoch 1364 / 2000, step 32/63, loss = 0.3610\n",
            "epoch 1364 / 2000, step 48/63, loss = 0.7991\n",
            "epoch 1365 / 2000, step 16/63, loss = 0.3179\n",
            "epoch 1365 / 2000, step 32/63, loss = 0.7027\n",
            "epoch 1365 / 2000, step 48/63, loss = 0.5660\n",
            "epoch 1366 / 2000, step 16/63, loss = 0.4397\n",
            "epoch 1366 / 2000, step 32/63, loss = 0.6713\n",
            "epoch 1366 / 2000, step 48/63, loss = 0.4624\n",
            "epoch 1367 / 2000, step 16/63, loss = 0.3566\n",
            "epoch 1367 / 2000, step 32/63, loss = 0.5053\n",
            "epoch 1367 / 2000, step 48/63, loss = 0.4176\n",
            "epoch 1368 / 2000, step 16/63, loss = 0.4375\n",
            "epoch 1368 / 2000, step 32/63, loss = 0.3596\n",
            "epoch 1368 / 2000, step 48/63, loss = 0.5781\n",
            "epoch 1369 / 2000, step 16/63, loss = 0.3771\n",
            "epoch 1369 / 2000, step 32/63, loss = 0.4017\n",
            "epoch 1369 / 2000, step 48/63, loss = 0.3052\n",
            "epoch 1370 / 2000, step 16/63, loss = 0.3902\n",
            "epoch 1370 / 2000, step 32/63, loss = 0.5583\n",
            "epoch 1370 / 2000, step 48/63, loss = 0.4501\n",
            "epoch 1371 / 2000, step 16/63, loss = 0.6052\n",
            "epoch 1371 / 2000, step 32/63, loss = 0.3977\n",
            "epoch 1371 / 2000, step 48/63, loss = 0.4356\n",
            "epoch 1372 / 2000, step 16/63, loss = 0.5308\n",
            "epoch 1372 / 2000, step 32/63, loss = 0.3490\n",
            "epoch 1372 / 2000, step 48/63, loss = 0.6815\n",
            "epoch 1373 / 2000, step 16/63, loss = 0.4428\n",
            "epoch 1373 / 2000, step 32/63, loss = 0.4883\n",
            "epoch 1373 / 2000, step 48/63, loss = 0.5042\n",
            "epoch 1374 / 2000, step 16/63, loss = 0.3545\n",
            "epoch 1374 / 2000, step 32/63, loss = 0.5883\n",
            "epoch 1374 / 2000, step 48/63, loss = 0.5335\n",
            "epoch 1375 / 2000, step 16/63, loss = 0.6912\n",
            "epoch 1375 / 2000, step 32/63, loss = 0.3707\n",
            "epoch 1375 / 2000, step 48/63, loss = 0.4058\n",
            "epoch 1376 / 2000, step 16/63, loss = 0.4181\n",
            "epoch 1376 / 2000, step 32/63, loss = 0.4148\n",
            "epoch 1376 / 2000, step 48/63, loss = 0.3984\n",
            "epoch 1377 / 2000, step 16/63, loss = 0.3419\n",
            "epoch 1377 / 2000, step 32/63, loss = 0.4942\n",
            "epoch 1377 / 2000, step 48/63, loss = 0.4181\n",
            "epoch 1378 / 2000, step 16/63, loss = 0.5701\n",
            "epoch 1378 / 2000, step 32/63, loss = 0.3980\n",
            "epoch 1378 / 2000, step 48/63, loss = 0.6248\n",
            "epoch 1379 / 2000, step 16/63, loss = 0.3670\n",
            "epoch 1379 / 2000, step 32/63, loss = 0.3364\n",
            "epoch 1379 / 2000, step 48/63, loss = 0.3065\n",
            "epoch 1380 / 2000, step 16/63, loss = 0.4492\n",
            "epoch 1380 / 2000, step 32/63, loss = 0.5794\n",
            "epoch 1380 / 2000, step 48/63, loss = 0.4631\n",
            "epoch 1381 / 2000, step 16/63, loss = 0.4788\n",
            "epoch 1381 / 2000, step 32/63, loss = 0.5748\n",
            "epoch 1381 / 2000, step 48/63, loss = 0.3516\n",
            "epoch 1382 / 2000, step 16/63, loss = 0.4618\n",
            "epoch 1382 / 2000, step 32/63, loss = 0.4033\n",
            "epoch 1382 / 2000, step 48/63, loss = 0.5394\n",
            "epoch 1383 / 2000, step 16/63, loss = 0.3570\n",
            "epoch 1383 / 2000, step 32/63, loss = 0.4707\n",
            "epoch 1383 / 2000, step 48/63, loss = 0.4661\n",
            "epoch 1384 / 2000, step 16/63, loss = 0.4012\n",
            "epoch 1384 / 2000, step 32/63, loss = 0.4855\n",
            "epoch 1384 / 2000, step 48/63, loss = 0.7037\n",
            "epoch 1385 / 2000, step 16/63, loss = 0.5342\n",
            "epoch 1385 / 2000, step 32/63, loss = 0.4596\n",
            "epoch 1385 / 2000, step 48/63, loss = 0.3589\n",
            "epoch 1386 / 2000, step 16/63, loss = 0.5791\n",
            "epoch 1386 / 2000, step 32/63, loss = 0.4628\n",
            "epoch 1386 / 2000, step 48/63, loss = 0.4266\n",
            "epoch 1387 / 2000, step 16/63, loss = 0.5153\n",
            "epoch 1387 / 2000, step 32/63, loss = 0.4502\n",
            "epoch 1387 / 2000, step 48/63, loss = 0.3783\n",
            "epoch 1388 / 2000, step 16/63, loss = 0.6309\n",
            "epoch 1388 / 2000, step 32/63, loss = 0.4806\n",
            "epoch 1388 / 2000, step 48/63, loss = 0.3607\n",
            "epoch 1389 / 2000, step 16/63, loss = 0.5840\n",
            "epoch 1389 / 2000, step 32/63, loss = 0.2666\n",
            "epoch 1389 / 2000, step 48/63, loss = 0.5342\n",
            "epoch 1390 / 2000, step 16/63, loss = 0.7810\n",
            "epoch 1390 / 2000, step 32/63, loss = 0.4630\n",
            "epoch 1390 / 2000, step 48/63, loss = 0.6272\n",
            "epoch 1391 / 2000, step 16/63, loss = 0.4144\n",
            "epoch 1391 / 2000, step 32/63, loss = 0.5245\n",
            "epoch 1391 / 2000, step 48/63, loss = 0.7212\n",
            "epoch 1392 / 2000, step 16/63, loss = 0.6618\n",
            "epoch 1392 / 2000, step 32/63, loss = 0.4970\n",
            "epoch 1392 / 2000, step 48/63, loss = 0.7090\n",
            "epoch 1393 / 2000, step 16/63, loss = 0.3565\n",
            "epoch 1393 / 2000, step 32/63, loss = 0.3642\n",
            "epoch 1393 / 2000, step 48/63, loss = 0.3612\n",
            "epoch 1394 / 2000, step 16/63, loss = 0.5779\n",
            "epoch 1394 / 2000, step 32/63, loss = 0.4770\n",
            "epoch 1394 / 2000, step 48/63, loss = 0.5582\n",
            "epoch 1395 / 2000, step 16/63, loss = 0.3323\n",
            "epoch 1395 / 2000, step 32/63, loss = 0.3054\n",
            "epoch 1395 / 2000, step 48/63, loss = 0.5907\n",
            "epoch 1396 / 2000, step 16/63, loss = 0.5417\n",
            "epoch 1396 / 2000, step 32/63, loss = 0.4704\n",
            "epoch 1396 / 2000, step 48/63, loss = 0.4404\n",
            "epoch 1397 / 2000, step 16/63, loss = 0.4071\n",
            "epoch 1397 / 2000, step 32/63, loss = 0.5127\n",
            "epoch 1397 / 2000, step 48/63, loss = 0.3493\n",
            "epoch 1398 / 2000, step 16/63, loss = 0.4538\n",
            "epoch 1398 / 2000, step 32/63, loss = 0.4421\n",
            "epoch 1398 / 2000, step 48/63, loss = 0.3441\n",
            "epoch 1399 / 2000, step 16/63, loss = 0.3458\n",
            "epoch 1399 / 2000, step 32/63, loss = 0.3028\n",
            "epoch 1399 / 2000, step 48/63, loss = 0.6487\n",
            "epoch 1400 / 2000, step 16/63, loss = 0.6838\n",
            "epoch 1400 / 2000, step 32/63, loss = 0.5132\n",
            "epoch 1400 / 2000, step 48/63, loss = 0.5258\n",
            "epoch 1401 / 2000, step 16/63, loss = 0.5155\n",
            "epoch 1401 / 2000, step 32/63, loss = 0.4761\n",
            "epoch 1401 / 2000, step 48/63, loss = 0.6036\n",
            "epoch 1402 / 2000, step 16/63, loss = 0.6246\n",
            "epoch 1402 / 2000, step 32/63, loss = 0.4272\n",
            "epoch 1402 / 2000, step 48/63, loss = 0.3582\n",
            "epoch 1403 / 2000, step 16/63, loss = 0.4326\n",
            "epoch 1403 / 2000, step 32/63, loss = 0.5959\n",
            "epoch 1403 / 2000, step 48/63, loss = 0.3505\n",
            "epoch 1404 / 2000, step 16/63, loss = 0.6082\n",
            "epoch 1404 / 2000, step 32/63, loss = 0.4843\n",
            "epoch 1404 / 2000, step 48/63, loss = 0.5619\n",
            "epoch 1405 / 2000, step 16/63, loss = 0.4051\n",
            "epoch 1405 / 2000, step 32/63, loss = 0.4956\n",
            "epoch 1405 / 2000, step 48/63, loss = 0.3019\n",
            "epoch 1406 / 2000, step 16/63, loss = 0.4677\n",
            "epoch 1406 / 2000, step 32/63, loss = 0.4148\n",
            "epoch 1406 / 2000, step 48/63, loss = 0.3438\n",
            "epoch 1407 / 2000, step 16/63, loss = 0.4079\n",
            "epoch 1407 / 2000, step 32/63, loss = 0.3262\n",
            "epoch 1407 / 2000, step 48/63, loss = 0.3069\n",
            "epoch 1408 / 2000, step 16/63, loss = 0.4144\n",
            "epoch 1408 / 2000, step 32/63, loss = 0.4363\n",
            "epoch 1408 / 2000, step 48/63, loss = 0.3706\n",
            "epoch 1409 / 2000, step 16/63, loss = 0.5034\n",
            "epoch 1409 / 2000, step 32/63, loss = 0.4450\n",
            "epoch 1409 / 2000, step 48/63, loss = 0.3383\n",
            "epoch 1410 / 2000, step 16/63, loss = 0.7350\n",
            "epoch 1410 / 2000, step 32/63, loss = 0.5971\n",
            "epoch 1410 / 2000, step 48/63, loss = 0.4905\n",
            "epoch 1411 / 2000, step 16/63, loss = 0.5220\n",
            "epoch 1411 / 2000, step 32/63, loss = 0.7935\n",
            "epoch 1411 / 2000, step 48/63, loss = 0.5694\n",
            "epoch 1412 / 2000, step 16/63, loss = 0.5882\n",
            "epoch 1412 / 2000, step 32/63, loss = 0.4057\n",
            "epoch 1412 / 2000, step 48/63, loss = 0.4317\n",
            "epoch 1413 / 2000, step 16/63, loss = 0.4252\n",
            "epoch 1413 / 2000, step 32/63, loss = 0.4997\n",
            "epoch 1413 / 2000, step 48/63, loss = 0.3514\n",
            "epoch 1414 / 2000, step 16/63, loss = 0.3739\n",
            "epoch 1414 / 2000, step 32/63, loss = 0.4525\n",
            "epoch 1414 / 2000, step 48/63, loss = 0.4773\n",
            "epoch 1415 / 2000, step 16/63, loss = 0.4526\n",
            "epoch 1415 / 2000, step 32/63, loss = 0.4667\n",
            "epoch 1415 / 2000, step 48/63, loss = 0.3288\n",
            "epoch 1416 / 2000, step 16/63, loss = 0.3755\n",
            "epoch 1416 / 2000, step 32/63, loss = 0.5084\n",
            "epoch 1416 / 2000, step 48/63, loss = 0.5865\n",
            "epoch 1417 / 2000, step 16/63, loss = 0.5207\n",
            "epoch 1417 / 2000, step 32/63, loss = 0.5361\n",
            "epoch 1417 / 2000, step 48/63, loss = 0.4547\n",
            "epoch 1418 / 2000, step 16/63, loss = 0.4850\n",
            "epoch 1418 / 2000, step 32/63, loss = 0.5756\n",
            "epoch 1418 / 2000, step 48/63, loss = 0.8867\n",
            "epoch 1419 / 2000, step 16/63, loss = 0.6132\n",
            "epoch 1419 / 2000, step 32/63, loss = 0.4668\n",
            "epoch 1419 / 2000, step 48/63, loss = 0.4384\n",
            "epoch 1420 / 2000, step 16/63, loss = 0.8091\n",
            "epoch 1420 / 2000, step 32/63, loss = 0.4483\n",
            "epoch 1420 / 2000, step 48/63, loss = 0.3602\n",
            "epoch 1421 / 2000, step 16/63, loss = 0.5566\n",
            "epoch 1421 / 2000, step 32/63, loss = 0.6027\n",
            "epoch 1421 / 2000, step 48/63, loss = 0.4270\n",
            "epoch 1422 / 2000, step 16/63, loss = 0.4895\n",
            "epoch 1422 / 2000, step 32/63, loss = 0.3942\n",
            "epoch 1422 / 2000, step 48/63, loss = 0.5871\n",
            "epoch 1423 / 2000, step 16/63, loss = 0.5683\n",
            "epoch 1423 / 2000, step 32/63, loss = 0.3429\n",
            "epoch 1423 / 2000, step 48/63, loss = 0.4419\n",
            "epoch 1424 / 2000, step 16/63, loss = 0.4463\n",
            "epoch 1424 / 2000, step 32/63, loss = 0.4042\n",
            "epoch 1424 / 2000, step 48/63, loss = 0.2111\n",
            "epoch 1425 / 2000, step 16/63, loss = 0.4792\n",
            "epoch 1425 / 2000, step 32/63, loss = 0.5227\n",
            "epoch 1425 / 2000, step 48/63, loss = 0.4315\n",
            "epoch 1426 / 2000, step 16/63, loss = 0.4404\n",
            "epoch 1426 / 2000, step 32/63, loss = 0.6042\n",
            "epoch 1426 / 2000, step 48/63, loss = 0.6109\n",
            "epoch 1427 / 2000, step 16/63, loss = 0.5829\n",
            "epoch 1427 / 2000, step 32/63, loss = 0.4979\n",
            "epoch 1427 / 2000, step 48/63, loss = 0.5481\n",
            "epoch 1428 / 2000, step 16/63, loss = 0.4574\n",
            "epoch 1428 / 2000, step 32/63, loss = 0.6555\n",
            "epoch 1428 / 2000, step 48/63, loss = 0.3687\n",
            "epoch 1429 / 2000, step 16/63, loss = 0.2506\n",
            "epoch 1429 / 2000, step 32/63, loss = 0.3085\n",
            "epoch 1429 / 2000, step 48/63, loss = 0.6159\n",
            "epoch 1430 / 2000, step 16/63, loss = 0.3602\n",
            "epoch 1430 / 2000, step 32/63, loss = 0.4916\n",
            "epoch 1430 / 2000, step 48/63, loss = 0.2703\n",
            "epoch 1431 / 2000, step 16/63, loss = 0.4666\n",
            "epoch 1431 / 2000, step 32/63, loss = 0.7136\n",
            "epoch 1431 / 2000, step 48/63, loss = 0.3562\n",
            "epoch 1432 / 2000, step 16/63, loss = 0.5909\n",
            "epoch 1432 / 2000, step 32/63, loss = 0.3941\n",
            "epoch 1432 / 2000, step 48/63, loss = 0.3524\n",
            "epoch 1433 / 2000, step 16/63, loss = 0.3085\n",
            "epoch 1433 / 2000, step 32/63, loss = 0.4143\n",
            "epoch 1433 / 2000, step 48/63, loss = 0.5928\n",
            "epoch 1434 / 2000, step 16/63, loss = 0.5065\n",
            "epoch 1434 / 2000, step 32/63, loss = 0.6675\n",
            "epoch 1434 / 2000, step 48/63, loss = 0.6253\n",
            "epoch 1435 / 2000, step 16/63, loss = 0.4834\n",
            "epoch 1435 / 2000, step 32/63, loss = 0.2988\n",
            "epoch 1435 / 2000, step 48/63, loss = 0.4926\n",
            "epoch 1436 / 2000, step 16/63, loss = 0.5040\n",
            "epoch 1436 / 2000, step 32/63, loss = 0.4595\n",
            "epoch 1436 / 2000, step 48/63, loss = 0.5889\n",
            "epoch 1437 / 2000, step 16/63, loss = 0.3982\n",
            "epoch 1437 / 2000, step 32/63, loss = 0.8410\n",
            "epoch 1437 / 2000, step 48/63, loss = 0.6066\n",
            "epoch 1438 / 2000, step 16/63, loss = 0.7032\n",
            "epoch 1438 / 2000, step 32/63, loss = 0.3885\n",
            "epoch 1438 / 2000, step 48/63, loss = 0.5712\n",
            "epoch 1439 / 2000, step 16/63, loss = 0.8331\n",
            "epoch 1439 / 2000, step 32/63, loss = 0.6789\n",
            "epoch 1439 / 2000, step 48/63, loss = 0.4807\n",
            "epoch 1440 / 2000, step 16/63, loss = 0.5829\n",
            "epoch 1440 / 2000, step 32/63, loss = 0.3986\n",
            "epoch 1440 / 2000, step 48/63, loss = 0.6872\n",
            "epoch 1441 / 2000, step 16/63, loss = 0.5208\n",
            "epoch 1441 / 2000, step 32/63, loss = 0.5285\n",
            "epoch 1441 / 2000, step 48/63, loss = 0.4778\n",
            "epoch 1442 / 2000, step 16/63, loss = 0.5460\n",
            "epoch 1442 / 2000, step 32/63, loss = 0.3659\n",
            "epoch 1442 / 2000, step 48/63, loss = 0.5996\n",
            "epoch 1443 / 2000, step 16/63, loss = 0.3830\n",
            "epoch 1443 / 2000, step 32/63, loss = 0.4585\n",
            "epoch 1443 / 2000, step 48/63, loss = 0.5333\n",
            "epoch 1444 / 2000, step 16/63, loss = 0.6530\n",
            "epoch 1444 / 2000, step 32/63, loss = 0.4577\n",
            "epoch 1444 / 2000, step 48/63, loss = 0.3519\n",
            "epoch 1445 / 2000, step 16/63, loss = 0.4034\n",
            "epoch 1445 / 2000, step 32/63, loss = 0.6742\n",
            "epoch 1445 / 2000, step 48/63, loss = 0.3362\n",
            "epoch 1446 / 2000, step 16/63, loss = 0.5963\n",
            "epoch 1446 / 2000, step 32/63, loss = 0.5797\n",
            "epoch 1446 / 2000, step 48/63, loss = 0.5736\n",
            "epoch 1447 / 2000, step 16/63, loss = 0.4217\n",
            "epoch 1447 / 2000, step 32/63, loss = 0.4994\n",
            "epoch 1447 / 2000, step 48/63, loss = 0.4701\n",
            "epoch 1448 / 2000, step 16/63, loss = 0.3989\n",
            "epoch 1448 / 2000, step 32/63, loss = 0.3363\n",
            "epoch 1448 / 2000, step 48/63, loss = 0.6215\n",
            "epoch 1449 / 2000, step 16/63, loss = 0.3887\n",
            "epoch 1449 / 2000, step 32/63, loss = 0.2799\n",
            "epoch 1449 / 2000, step 48/63, loss = 0.3425\n",
            "epoch 1450 / 2000, step 16/63, loss = 0.3463\n",
            "epoch 1450 / 2000, step 32/63, loss = 0.4313\n",
            "epoch 1450 / 2000, step 48/63, loss = 0.6074\n",
            "epoch 1451 / 2000, step 16/63, loss = 0.3865\n",
            "epoch 1451 / 2000, step 32/63, loss = 0.5060\n",
            "epoch 1451 / 2000, step 48/63, loss = 0.5478\n",
            "epoch 1452 / 2000, step 16/63, loss = 0.3430\n",
            "epoch 1452 / 2000, step 32/63, loss = 0.5980\n",
            "epoch 1452 / 2000, step 48/63, loss = 0.4215\n",
            "epoch 1453 / 2000, step 16/63, loss = 0.2580\n",
            "epoch 1453 / 2000, step 32/63, loss = 0.5126\n",
            "epoch 1453 / 2000, step 48/63, loss = 0.3383\n",
            "epoch 1454 / 2000, step 16/63, loss = 0.5773\n",
            "epoch 1454 / 2000, step 32/63, loss = 0.6204\n",
            "epoch 1454 / 2000, step 48/63, loss = 0.3761\n",
            "epoch 1455 / 2000, step 16/63, loss = 0.2868\n",
            "epoch 1455 / 2000, step 32/63, loss = 0.3420\n",
            "epoch 1455 / 2000, step 48/63, loss = 0.6053\n",
            "epoch 1456 / 2000, step 16/63, loss = 0.4482\n",
            "epoch 1456 / 2000, step 32/63, loss = 0.5763\n",
            "epoch 1456 / 2000, step 48/63, loss = 0.8587\n",
            "epoch 1457 / 2000, step 16/63, loss = 0.4473\n",
            "epoch 1457 / 2000, step 32/63, loss = 0.4009\n",
            "epoch 1457 / 2000, step 48/63, loss = 0.5504\n",
            "epoch 1458 / 2000, step 16/63, loss = 0.6874\n",
            "epoch 1458 / 2000, step 32/63, loss = 0.4682\n",
            "epoch 1458 / 2000, step 48/63, loss = 0.3811\n",
            "epoch 1459 / 2000, step 16/63, loss = 0.3851\n",
            "epoch 1459 / 2000, step 32/63, loss = 0.4357\n",
            "epoch 1459 / 2000, step 48/63, loss = 0.4836\n",
            "epoch 1460 / 2000, step 16/63, loss = 0.3158\n",
            "epoch 1460 / 2000, step 32/63, loss = 0.6440\n",
            "epoch 1460 / 2000, step 48/63, loss = 0.5356\n",
            "epoch 1461 / 2000, step 16/63, loss = 0.1974\n",
            "epoch 1461 / 2000, step 32/63, loss = 0.4312\n",
            "epoch 1461 / 2000, step 48/63, loss = 0.4073\n",
            "epoch 1462 / 2000, step 16/63, loss = 0.4388\n",
            "epoch 1462 / 2000, step 32/63, loss = 0.3867\n",
            "epoch 1462 / 2000, step 48/63, loss = 0.3363\n",
            "epoch 1463 / 2000, step 16/63, loss = 0.3424\n",
            "epoch 1463 / 2000, step 32/63, loss = 0.3277\n",
            "epoch 1463 / 2000, step 48/63, loss = 0.2963\n",
            "epoch 1464 / 2000, step 16/63, loss = 0.5052\n",
            "epoch 1464 / 2000, step 32/63, loss = 0.3226\n",
            "epoch 1464 / 2000, step 48/63, loss = 0.3951\n",
            "epoch 1465 / 2000, step 16/63, loss = 0.4547\n",
            "epoch 1465 / 2000, step 32/63, loss = 0.4217\n",
            "epoch 1465 / 2000, step 48/63, loss = 0.3458\n",
            "epoch 1466 / 2000, step 16/63, loss = 0.4262\n",
            "epoch 1466 / 2000, step 32/63, loss = 0.3461\n",
            "epoch 1466 / 2000, step 48/63, loss = 0.4366\n",
            "epoch 1467 / 2000, step 16/63, loss = 0.3897\n",
            "epoch 1467 / 2000, step 32/63, loss = 0.5981\n",
            "epoch 1467 / 2000, step 48/63, loss = 0.3888\n",
            "epoch 1468 / 2000, step 16/63, loss = 0.3730\n",
            "epoch 1468 / 2000, step 32/63, loss = 0.3829\n",
            "epoch 1468 / 2000, step 48/63, loss = 0.5468\n",
            "epoch 1469 / 2000, step 16/63, loss = 0.3806\n",
            "epoch 1469 / 2000, step 32/63, loss = 0.4359\n",
            "epoch 1469 / 2000, step 48/63, loss = 0.2375\n",
            "epoch 1470 / 2000, step 16/63, loss = 0.4674\n",
            "epoch 1470 / 2000, step 32/63, loss = 0.5500\n",
            "epoch 1470 / 2000, step 48/63, loss = 0.5587\n",
            "epoch 1471 / 2000, step 16/63, loss = 0.2918\n",
            "epoch 1471 / 2000, step 32/63, loss = 0.3888\n",
            "epoch 1471 / 2000, step 48/63, loss = 0.5736\n",
            "epoch 1472 / 2000, step 16/63, loss = 0.5809\n",
            "epoch 1472 / 2000, step 32/63, loss = 0.3674\n",
            "epoch 1472 / 2000, step 48/63, loss = 0.7123\n",
            "epoch 1473 / 2000, step 16/63, loss = 0.4575\n",
            "epoch 1473 / 2000, step 32/63, loss = 0.3860\n",
            "epoch 1473 / 2000, step 48/63, loss = 0.4549\n",
            "epoch 1474 / 2000, step 16/63, loss = 0.4379\n",
            "epoch 1474 / 2000, step 32/63, loss = 0.4285\n",
            "epoch 1474 / 2000, step 48/63, loss = 0.4010\n",
            "epoch 1475 / 2000, step 16/63, loss = 0.4629\n",
            "epoch 1475 / 2000, step 32/63, loss = 0.3657\n",
            "epoch 1475 / 2000, step 48/63, loss = 0.4159\n",
            "epoch 1476 / 2000, step 16/63, loss = 0.4351\n",
            "epoch 1476 / 2000, step 32/63, loss = 0.3162\n",
            "epoch 1476 / 2000, step 48/63, loss = 0.4948\n",
            "epoch 1477 / 2000, step 16/63, loss = 0.3203\n",
            "epoch 1477 / 2000, step 32/63, loss = 0.2995\n",
            "epoch 1477 / 2000, step 48/63, loss = 0.5229\n",
            "epoch 1478 / 2000, step 16/63, loss = 0.2822\n",
            "epoch 1478 / 2000, step 32/63, loss = 0.3777\n",
            "epoch 1478 / 2000, step 48/63, loss = 0.4025\n",
            "epoch 1479 / 2000, step 16/63, loss = 0.4248\n",
            "epoch 1479 / 2000, step 32/63, loss = 0.5109\n",
            "epoch 1479 / 2000, step 48/63, loss = 0.3497\n",
            "epoch 1480 / 2000, step 16/63, loss = 0.5095\n",
            "epoch 1480 / 2000, step 32/63, loss = 0.7024\n",
            "epoch 1480 / 2000, step 48/63, loss = 0.3620\n",
            "epoch 1481 / 2000, step 16/63, loss = 0.4554\n",
            "epoch 1481 / 2000, step 32/63, loss = 0.4358\n",
            "epoch 1481 / 2000, step 48/63, loss = 0.3549\n",
            "epoch 1482 / 2000, step 16/63, loss = 0.3920\n",
            "epoch 1482 / 2000, step 32/63, loss = 0.8575\n",
            "epoch 1482 / 2000, step 48/63, loss = 0.6165\n",
            "epoch 1483 / 2000, step 16/63, loss = 0.3578\n",
            "epoch 1483 / 2000, step 32/63, loss = 0.4570\n",
            "epoch 1483 / 2000, step 48/63, loss = 0.4304\n",
            "epoch 1484 / 2000, step 16/63, loss = 0.5358\n",
            "epoch 1484 / 2000, step 32/63, loss = 0.4686\n",
            "epoch 1484 / 2000, step 48/63, loss = 0.4372\n",
            "epoch 1485 / 2000, step 16/63, loss = 0.3939\n",
            "epoch 1485 / 2000, step 32/63, loss = 0.4423\n",
            "epoch 1485 / 2000, step 48/63, loss = 0.3144\n",
            "epoch 1486 / 2000, step 16/63, loss = 0.3626\n",
            "epoch 1486 / 2000, step 32/63, loss = 0.6002\n",
            "epoch 1486 / 2000, step 48/63, loss = 0.3931\n",
            "epoch 1487 / 2000, step 16/63, loss = 0.4641\n",
            "epoch 1487 / 2000, step 32/63, loss = 0.4110\n",
            "epoch 1487 / 2000, step 48/63, loss = 0.4256\n",
            "epoch 1488 / 2000, step 16/63, loss = 0.3893\n",
            "epoch 1488 / 2000, step 32/63, loss = 0.3885\n",
            "epoch 1488 / 2000, step 48/63, loss = 0.2939\n",
            "epoch 1489 / 2000, step 16/63, loss = 0.4137\n",
            "epoch 1489 / 2000, step 32/63, loss = 0.5165\n",
            "epoch 1489 / 2000, step 48/63, loss = 0.4629\n",
            "epoch 1490 / 2000, step 16/63, loss = 0.4568\n",
            "epoch 1490 / 2000, step 32/63, loss = 0.4104\n",
            "epoch 1490 / 2000, step 48/63, loss = 0.3228\n",
            "epoch 1491 / 2000, step 16/63, loss = 0.6003\n",
            "epoch 1491 / 2000, step 32/63, loss = 0.3962\n",
            "epoch 1491 / 2000, step 48/63, loss = 0.6063\n",
            "epoch 1492 / 2000, step 16/63, loss = 0.7831\n",
            "epoch 1492 / 2000, step 32/63, loss = 0.5269\n",
            "epoch 1492 / 2000, step 48/63, loss = 0.2677\n",
            "epoch 1493 / 2000, step 16/63, loss = 0.2974\n",
            "epoch 1493 / 2000, step 32/63, loss = 0.3281\n",
            "epoch 1493 / 2000, step 48/63, loss = 0.5527\n",
            "epoch 1494 / 2000, step 16/63, loss = 0.7893\n",
            "epoch 1494 / 2000, step 32/63, loss = 0.4294\n",
            "epoch 1494 / 2000, step 48/63, loss = 0.4201\n",
            "epoch 1495 / 2000, step 16/63, loss = 0.2616\n",
            "epoch 1495 / 2000, step 32/63, loss = 0.5726\n",
            "epoch 1495 / 2000, step 48/63, loss = 0.4540\n",
            "epoch 1496 / 2000, step 16/63, loss = 0.4365\n",
            "epoch 1496 / 2000, step 32/63, loss = 0.4221\n",
            "epoch 1496 / 2000, step 48/63, loss = 0.5560\n",
            "epoch 1497 / 2000, step 16/63, loss = 0.3725\n",
            "epoch 1497 / 2000, step 32/63, loss = 0.3718\n",
            "epoch 1497 / 2000, step 48/63, loss = 0.3293\n",
            "epoch 1498 / 2000, step 16/63, loss = 0.4684\n",
            "epoch 1498 / 2000, step 32/63, loss = 0.3225\n",
            "epoch 1498 / 2000, step 48/63, loss = 0.3566\n",
            "epoch 1499 / 2000, step 16/63, loss = 0.4755\n",
            "epoch 1499 / 2000, step 32/63, loss = 0.4841\n",
            "epoch 1499 / 2000, step 48/63, loss = 0.5034\n",
            "epoch 1500 / 2000, step 16/63, loss = 0.4919\n",
            "epoch 1500 / 2000, step 32/63, loss = 0.4367\n",
            "epoch 1500 / 2000, step 48/63, loss = 0.3338\n",
            "epoch 1501 / 2000, step 16/63, loss = 0.3875\n",
            "epoch 1501 / 2000, step 32/63, loss = 0.4701\n",
            "epoch 1501 / 2000, step 48/63, loss = 0.3998\n",
            "epoch 1502 / 2000, step 16/63, loss = 0.5126\n",
            "epoch 1502 / 2000, step 32/63, loss = 0.4769\n",
            "epoch 1502 / 2000, step 48/63, loss = 0.5033\n",
            "epoch 1503 / 2000, step 16/63, loss = 0.4672\n",
            "epoch 1503 / 2000, step 32/63, loss = 0.4129\n",
            "epoch 1503 / 2000, step 48/63, loss = 0.5192\n",
            "epoch 1504 / 2000, step 16/63, loss = 0.4275\n",
            "epoch 1504 / 2000, step 32/63, loss = 0.3929\n",
            "epoch 1504 / 2000, step 48/63, loss = 0.3725\n",
            "epoch 1505 / 2000, step 16/63, loss = 0.3452\n",
            "epoch 1505 / 2000, step 32/63, loss = 0.3827\n",
            "epoch 1505 / 2000, step 48/63, loss = 0.2607\n",
            "epoch 1506 / 2000, step 16/63, loss = 0.4936\n",
            "epoch 1506 / 2000, step 32/63, loss = 0.4452\n",
            "epoch 1506 / 2000, step 48/63, loss = 0.6373\n",
            "epoch 1507 / 2000, step 16/63, loss = 0.3307\n",
            "epoch 1507 / 2000, step 32/63, loss = 0.8218\n",
            "epoch 1507 / 2000, step 48/63, loss = 0.4986\n",
            "epoch 1508 / 2000, step 16/63, loss = 0.3347\n",
            "epoch 1508 / 2000, step 32/63, loss = 0.4029\n",
            "epoch 1508 / 2000, step 48/63, loss = 0.6421\n",
            "epoch 1509 / 2000, step 16/63, loss = 0.4833\n",
            "epoch 1509 / 2000, step 32/63, loss = 0.4504\n",
            "epoch 1509 / 2000, step 48/63, loss = 0.6765\n",
            "epoch 1510 / 2000, step 16/63, loss = 0.3747\n",
            "epoch 1510 / 2000, step 32/63, loss = 0.3071\n",
            "epoch 1510 / 2000, step 48/63, loss = 0.4418\n",
            "epoch 1511 / 2000, step 16/63, loss = 0.3443\n",
            "epoch 1511 / 2000, step 32/63, loss = 0.4033\n",
            "epoch 1511 / 2000, step 48/63, loss = 0.4373\n",
            "epoch 1512 / 2000, step 16/63, loss = 0.5789\n",
            "epoch 1512 / 2000, step 32/63, loss = 0.4190\n",
            "epoch 1512 / 2000, step 48/63, loss = 0.6281\n",
            "epoch 1513 / 2000, step 16/63, loss = 0.6492\n",
            "epoch 1513 / 2000, step 32/63, loss = 0.5360\n",
            "epoch 1513 / 2000, step 48/63, loss = 0.4740\n",
            "epoch 1514 / 2000, step 16/63, loss = 0.4162\n",
            "epoch 1514 / 2000, step 32/63, loss = 0.5990\n",
            "epoch 1514 / 2000, step 48/63, loss = 0.4829\n",
            "epoch 1515 / 2000, step 16/63, loss = 0.4488\n",
            "epoch 1515 / 2000, step 32/63, loss = 0.3364\n",
            "epoch 1515 / 2000, step 48/63, loss = 0.3250\n",
            "epoch 1516 / 2000, step 16/63, loss = 0.6247\n",
            "epoch 1516 / 2000, step 32/63, loss = 0.4971\n",
            "epoch 1516 / 2000, step 48/63, loss = 0.3764\n",
            "epoch 1517 / 2000, step 16/63, loss = 0.4937\n",
            "epoch 1517 / 2000, step 32/63, loss = 0.3778\n",
            "epoch 1517 / 2000, step 48/63, loss = 0.2574\n",
            "epoch 1518 / 2000, step 16/63, loss = 0.4087\n",
            "epoch 1518 / 2000, step 32/63, loss = 0.5038\n",
            "epoch 1518 / 2000, step 48/63, loss = 0.4276\n",
            "epoch 1519 / 2000, step 16/63, loss = 0.3278\n",
            "epoch 1519 / 2000, step 32/63, loss = 0.5523\n",
            "epoch 1519 / 2000, step 48/63, loss = 0.6371\n",
            "epoch 1520 / 2000, step 16/63, loss = 0.8836\n",
            "epoch 1520 / 2000, step 32/63, loss = 0.5674\n",
            "epoch 1520 / 2000, step 48/63, loss = 0.4066\n",
            "epoch 1521 / 2000, step 16/63, loss = 0.2843\n",
            "epoch 1521 / 2000, step 32/63, loss = 0.3869\n",
            "epoch 1521 / 2000, step 48/63, loss = 0.5935\n",
            "epoch 1522 / 2000, step 16/63, loss = 0.3197\n",
            "epoch 1522 / 2000, step 32/63, loss = 0.7283\n",
            "epoch 1522 / 2000, step 48/63, loss = 0.3134\n",
            "epoch 1523 / 2000, step 16/63, loss = 0.6724\n",
            "epoch 1523 / 2000, step 32/63, loss = 0.1812\n",
            "epoch 1523 / 2000, step 48/63, loss = 0.2871\n",
            "epoch 1524 / 2000, step 16/63, loss = 0.2892\n",
            "epoch 1524 / 2000, step 32/63, loss = 0.5723\n",
            "epoch 1524 / 2000, step 48/63, loss = 0.2252\n",
            "epoch 1525 / 2000, step 16/63, loss = 0.5676\n",
            "epoch 1525 / 2000, step 32/63, loss = 0.3668\n",
            "epoch 1525 / 2000, step 48/63, loss = 0.3729\n",
            "epoch 1526 / 2000, step 16/63, loss = 0.4156\n",
            "epoch 1526 / 2000, step 32/63, loss = 0.3750\n",
            "epoch 1526 / 2000, step 48/63, loss = 0.6604\n",
            "epoch 1527 / 2000, step 16/63, loss = 0.4997\n",
            "epoch 1527 / 2000, step 32/63, loss = 0.2809\n",
            "epoch 1527 / 2000, step 48/63, loss = 0.2604\n",
            "epoch 1528 / 2000, step 16/63, loss = 0.6687\n",
            "epoch 1528 / 2000, step 32/63, loss = 0.4331\n",
            "epoch 1528 / 2000, step 48/63, loss = 0.5862\n",
            "epoch 1529 / 2000, step 16/63, loss = 0.4146\n",
            "epoch 1529 / 2000, step 32/63, loss = 0.3003\n",
            "epoch 1529 / 2000, step 48/63, loss = 0.4203\n",
            "epoch 1530 / 2000, step 16/63, loss = 0.3700\n",
            "epoch 1530 / 2000, step 32/63, loss = 0.3624\n",
            "epoch 1530 / 2000, step 48/63, loss = 0.6039\n",
            "epoch 1531 / 2000, step 16/63, loss = 0.3042\n",
            "epoch 1531 / 2000, step 32/63, loss = 0.7965\n",
            "epoch 1531 / 2000, step 48/63, loss = 0.3198\n",
            "epoch 1532 / 2000, step 16/63, loss = 0.4185\n",
            "epoch 1532 / 2000, step 32/63, loss = 0.4852\n",
            "epoch 1532 / 2000, step 48/63, loss = 0.3933\n",
            "epoch 1533 / 2000, step 16/63, loss = 0.4484\n",
            "epoch 1533 / 2000, step 32/63, loss = 0.2794\n",
            "epoch 1533 / 2000, step 48/63, loss = 0.6909\n",
            "epoch 1534 / 2000, step 16/63, loss = 0.1495\n",
            "epoch 1534 / 2000, step 32/63, loss = 0.3349\n",
            "epoch 1534 / 2000, step 48/63, loss = 0.5700\n",
            "epoch 1535 / 2000, step 16/63, loss = 0.5607\n",
            "epoch 1535 / 2000, step 32/63, loss = 0.3972\n",
            "epoch 1535 / 2000, step 48/63, loss = 0.6826\n",
            "epoch 1536 / 2000, step 16/63, loss = 0.5051\n",
            "epoch 1536 / 2000, step 32/63, loss = 0.3864\n",
            "epoch 1536 / 2000, step 48/63, loss = 0.3597\n",
            "epoch 1537 / 2000, step 16/63, loss = 0.5002\n",
            "epoch 1537 / 2000, step 32/63, loss = 0.4158\n",
            "epoch 1537 / 2000, step 48/63, loss = 0.2711\n",
            "epoch 1538 / 2000, step 16/63, loss = 0.3754\n",
            "epoch 1538 / 2000, step 32/63, loss = 0.6017\n",
            "epoch 1538 / 2000, step 48/63, loss = 0.3173\n",
            "epoch 1539 / 2000, step 16/63, loss = 0.3200\n",
            "epoch 1539 / 2000, step 32/63, loss = 0.3853\n",
            "epoch 1539 / 2000, step 48/63, loss = 0.3245\n",
            "epoch 1540 / 2000, step 16/63, loss = 0.2378\n",
            "epoch 1540 / 2000, step 32/63, loss = 0.3059\n",
            "epoch 1540 / 2000, step 48/63, loss = 0.4798\n",
            "epoch 1541 / 2000, step 16/63, loss = 0.3852\n",
            "epoch 1541 / 2000, step 32/63, loss = 0.3279\n",
            "epoch 1541 / 2000, step 48/63, loss = 0.4564\n",
            "epoch 1542 / 2000, step 16/63, loss = 0.4782\n",
            "epoch 1542 / 2000, step 32/63, loss = 0.4645\n",
            "epoch 1542 / 2000, step 48/63, loss = 0.3742\n",
            "epoch 1543 / 2000, step 16/63, loss = 0.4154\n",
            "epoch 1543 / 2000, step 32/63, loss = 0.3120\n",
            "epoch 1543 / 2000, step 48/63, loss = 0.5671\n",
            "epoch 1544 / 2000, step 16/63, loss = 0.4511\n",
            "epoch 1544 / 2000, step 32/63, loss = 0.1972\n",
            "epoch 1544 / 2000, step 48/63, loss = 0.2678\n",
            "epoch 1545 / 2000, step 16/63, loss = 0.3015\n",
            "epoch 1545 / 2000, step 32/63, loss = 0.2694\n",
            "epoch 1545 / 2000, step 48/63, loss = 0.3584\n",
            "epoch 1546 / 2000, step 16/63, loss = 0.5189\n",
            "epoch 1546 / 2000, step 32/63, loss = 0.5535\n",
            "epoch 1546 / 2000, step 48/63, loss = 0.4849\n",
            "epoch 1547 / 2000, step 16/63, loss = 0.5374\n",
            "epoch 1547 / 2000, step 32/63, loss = 0.4852\n",
            "epoch 1547 / 2000, step 48/63, loss = 0.7412\n",
            "epoch 1548 / 2000, step 16/63, loss = 0.3430\n",
            "epoch 1548 / 2000, step 32/63, loss = 0.3133\n",
            "epoch 1548 / 2000, step 48/63, loss = 0.2374\n",
            "epoch 1549 / 2000, step 16/63, loss = 0.3822\n",
            "epoch 1549 / 2000, step 32/63, loss = 0.3321\n",
            "epoch 1549 / 2000, step 48/63, loss = 0.6299\n",
            "epoch 1550 / 2000, step 16/63, loss = 0.4036\n",
            "epoch 1550 / 2000, step 32/63, loss = 0.5122\n",
            "epoch 1550 / 2000, step 48/63, loss = 0.3723\n",
            "epoch 1551 / 2000, step 16/63, loss = 0.4099\n",
            "epoch 1551 / 2000, step 32/63, loss = 0.2631\n",
            "epoch 1551 / 2000, step 48/63, loss = 0.3111\n",
            "epoch 1552 / 2000, step 16/63, loss = 0.4912\n",
            "epoch 1552 / 2000, step 32/63, loss = 0.3634\n",
            "epoch 1552 / 2000, step 48/63, loss = 0.3875\n",
            "epoch 1553 / 2000, step 16/63, loss = 0.4810\n",
            "epoch 1553 / 2000, step 32/63, loss = 0.3122\n",
            "epoch 1553 / 2000, step 48/63, loss = 0.3344\n",
            "epoch 1554 / 2000, step 16/63, loss = 0.4065\n",
            "epoch 1554 / 2000, step 32/63, loss = 0.4156\n",
            "epoch 1554 / 2000, step 48/63, loss = 0.3719\n",
            "epoch 1555 / 2000, step 16/63, loss = 0.3080\n",
            "epoch 1555 / 2000, step 32/63, loss = 0.5463\n",
            "epoch 1555 / 2000, step 48/63, loss = 0.4915\n",
            "epoch 1556 / 2000, step 16/63, loss = 0.4148\n",
            "epoch 1556 / 2000, step 32/63, loss = 0.2458\n",
            "epoch 1556 / 2000, step 48/63, loss = 0.5538\n",
            "epoch 1557 / 2000, step 16/63, loss = 0.3462\n",
            "epoch 1557 / 2000, step 32/63, loss = 0.3170\n",
            "epoch 1557 / 2000, step 48/63, loss = 0.2464\n",
            "epoch 1558 / 2000, step 16/63, loss = 0.6095\n",
            "epoch 1558 / 2000, step 32/63, loss = 0.5345\n",
            "epoch 1558 / 2000, step 48/63, loss = 0.3493\n",
            "epoch 1559 / 2000, step 16/63, loss = 0.7821\n",
            "epoch 1559 / 2000, step 32/63, loss = 0.3601\n",
            "epoch 1559 / 2000, step 48/63, loss = 0.3903\n",
            "epoch 1560 / 2000, step 16/63, loss = 0.4348\n",
            "epoch 1560 / 2000, step 32/63, loss = 0.5084\n",
            "epoch 1560 / 2000, step 48/63, loss = 0.3641\n",
            "epoch 1561 / 2000, step 16/63, loss = 0.2859\n",
            "epoch 1561 / 2000, step 32/63, loss = 0.3598\n",
            "epoch 1561 / 2000, step 48/63, loss = 0.4082\n",
            "epoch 1562 / 2000, step 16/63, loss = 0.4675\n",
            "epoch 1562 / 2000, step 32/63, loss = 0.5897\n",
            "epoch 1562 / 2000, step 48/63, loss = 0.4368\n",
            "epoch 1563 / 2000, step 16/63, loss = 0.7051\n",
            "epoch 1563 / 2000, step 32/63, loss = 0.2810\n",
            "epoch 1563 / 2000, step 48/63, loss = 0.5689\n",
            "epoch 1564 / 2000, step 16/63, loss = 0.2839\n",
            "epoch 1564 / 2000, step 32/63, loss = 0.4132\n",
            "epoch 1564 / 2000, step 48/63, loss = 0.5167\n",
            "epoch 1565 / 2000, step 16/63, loss = 0.4615\n",
            "epoch 1565 / 2000, step 32/63, loss = 0.4202\n",
            "epoch 1565 / 2000, step 48/63, loss = 0.2964\n",
            "epoch 1566 / 2000, step 16/63, loss = 0.7348\n",
            "epoch 1566 / 2000, step 32/63, loss = 0.2894\n",
            "epoch 1566 / 2000, step 48/63, loss = 0.6990\n",
            "epoch 1567 / 2000, step 16/63, loss = 0.3421\n",
            "epoch 1567 / 2000, step 32/63, loss = 0.4069\n",
            "epoch 1567 / 2000, step 48/63, loss = 0.4840\n",
            "epoch 1568 / 2000, step 16/63, loss = 0.3864\n",
            "epoch 1568 / 2000, step 32/63, loss = 0.3758\n",
            "epoch 1568 / 2000, step 48/63, loss = 0.5192\n",
            "epoch 1569 / 2000, step 16/63, loss = 0.4175\n",
            "epoch 1569 / 2000, step 32/63, loss = 0.5118\n",
            "epoch 1569 / 2000, step 48/63, loss = 0.4316\n",
            "epoch 1570 / 2000, step 16/63, loss = 0.6514\n",
            "epoch 1570 / 2000, step 32/63, loss = 0.3520\n",
            "epoch 1570 / 2000, step 48/63, loss = 0.4697\n",
            "epoch 1571 / 2000, step 16/63, loss = 0.3666\n",
            "epoch 1571 / 2000, step 32/63, loss = 0.6914\n",
            "epoch 1571 / 2000, step 48/63, loss = 0.7893\n",
            "epoch 1572 / 2000, step 16/63, loss = 0.3469\n",
            "epoch 1572 / 2000, step 32/63, loss = 0.4155\n",
            "epoch 1572 / 2000, step 48/63, loss = 0.4275\n",
            "epoch 1573 / 2000, step 16/63, loss = 0.4855\n",
            "epoch 1573 / 2000, step 32/63, loss = 0.3241\n",
            "epoch 1573 / 2000, step 48/63, loss = 0.4535\n",
            "epoch 1574 / 2000, step 16/63, loss = 0.3884\n",
            "epoch 1574 / 2000, step 32/63, loss = 0.4162\n",
            "epoch 1574 / 2000, step 48/63, loss = 0.4081\n",
            "epoch 1575 / 2000, step 16/63, loss = 0.4677\n",
            "epoch 1575 / 2000, step 32/63, loss = 0.2625\n",
            "epoch 1575 / 2000, step 48/63, loss = 0.3429\n",
            "epoch 1576 / 2000, step 16/63, loss = 0.6815\n",
            "epoch 1576 / 2000, step 32/63, loss = 0.2886\n",
            "epoch 1576 / 2000, step 48/63, loss = 0.7479\n",
            "epoch 1577 / 2000, step 16/63, loss = 0.2055\n",
            "epoch 1577 / 2000, step 32/63, loss = 0.5926\n",
            "epoch 1577 / 2000, step 48/63, loss = 0.3969\n",
            "epoch 1578 / 2000, step 16/63, loss = 0.7195\n",
            "epoch 1578 / 2000, step 32/63, loss = 0.5253\n",
            "epoch 1578 / 2000, step 48/63, loss = 0.4704\n",
            "epoch 1579 / 2000, step 16/63, loss = 0.3329\n",
            "epoch 1579 / 2000, step 32/63, loss = 0.4074\n",
            "epoch 1579 / 2000, step 48/63, loss = 0.3339\n",
            "epoch 1580 / 2000, step 16/63, loss = 0.3680\n",
            "epoch 1580 / 2000, step 32/63, loss = 0.4703\n",
            "epoch 1580 / 2000, step 48/63, loss = 0.3892\n",
            "epoch 1581 / 2000, step 16/63, loss = 0.4578\n",
            "epoch 1581 / 2000, step 32/63, loss = 0.4403\n",
            "epoch 1581 / 2000, step 48/63, loss = 0.2467\n",
            "epoch 1582 / 2000, step 16/63, loss = 0.2999\n",
            "epoch 1582 / 2000, step 32/63, loss = 0.4355\n",
            "epoch 1582 / 2000, step 48/63, loss = 0.5429\n",
            "epoch 1583 / 2000, step 16/63, loss = 0.2352\n",
            "epoch 1583 / 2000, step 32/63, loss = 0.1440\n",
            "epoch 1583 / 2000, step 48/63, loss = 0.5894\n",
            "epoch 1584 / 2000, step 16/63, loss = 0.4328\n",
            "epoch 1584 / 2000, step 32/63, loss = 0.4352\n",
            "epoch 1584 / 2000, step 48/63, loss = 0.6320\n",
            "epoch 1585 / 2000, step 16/63, loss = 0.4671\n",
            "epoch 1585 / 2000, step 32/63, loss = 0.4030\n",
            "epoch 1585 / 2000, step 48/63, loss = 0.4362\n",
            "epoch 1586 / 2000, step 16/63, loss = 0.4209\n",
            "epoch 1586 / 2000, step 32/63, loss = 0.5864\n",
            "epoch 1586 / 2000, step 48/63, loss = 0.5331\n",
            "epoch 1587 / 2000, step 16/63, loss = 0.5201\n",
            "epoch 1587 / 2000, step 32/63, loss = 0.2970\n",
            "epoch 1587 / 2000, step 48/63, loss = 0.3462\n",
            "epoch 1588 / 2000, step 16/63, loss = 0.5534\n",
            "epoch 1588 / 2000, step 32/63, loss = 0.3674\n",
            "epoch 1588 / 2000, step 48/63, loss = 0.3829\n",
            "epoch 1589 / 2000, step 16/63, loss = 0.3458\n",
            "epoch 1589 / 2000, step 32/63, loss = 0.4076\n",
            "epoch 1589 / 2000, step 48/63, loss = 0.4035\n",
            "epoch 1590 / 2000, step 16/63, loss = 0.2216\n",
            "epoch 1590 / 2000, step 32/63, loss = 0.5903\n",
            "epoch 1590 / 2000, step 48/63, loss = 0.1841\n",
            "epoch 1591 / 2000, step 16/63, loss = 0.4241\n",
            "epoch 1591 / 2000, step 32/63, loss = 0.3196\n",
            "epoch 1591 / 2000, step 48/63, loss = 0.4064\n",
            "epoch 1592 / 2000, step 16/63, loss = 0.2863\n",
            "epoch 1592 / 2000, step 32/63, loss = 0.5263\n",
            "epoch 1592 / 2000, step 48/63, loss = 0.5248\n",
            "epoch 1593 / 2000, step 16/63, loss = 0.3358\n",
            "epoch 1593 / 2000, step 32/63, loss = 0.5731\n",
            "epoch 1593 / 2000, step 48/63, loss = 0.5835\n",
            "epoch 1594 / 2000, step 16/63, loss = 0.3411\n",
            "epoch 1594 / 2000, step 32/63, loss = 0.5410\n",
            "epoch 1594 / 2000, step 48/63, loss = 0.5051\n",
            "epoch 1595 / 2000, step 16/63, loss = 0.3707\n",
            "epoch 1595 / 2000, step 32/63, loss = 0.2187\n",
            "epoch 1595 / 2000, step 48/63, loss = 0.4373\n",
            "epoch 1596 / 2000, step 16/63, loss = 0.5515\n",
            "epoch 1596 / 2000, step 32/63, loss = 0.3509\n",
            "epoch 1596 / 2000, step 48/63, loss = 0.4143\n",
            "epoch 1597 / 2000, step 16/63, loss = 0.6020\n",
            "epoch 1597 / 2000, step 32/63, loss = 0.3523\n",
            "epoch 1597 / 2000, step 48/63, loss = 0.5721\n",
            "epoch 1598 / 2000, step 16/63, loss = 0.2897\n",
            "epoch 1598 / 2000, step 32/63, loss = 0.4429\n",
            "epoch 1598 / 2000, step 48/63, loss = 0.4906\n",
            "epoch 1599 / 2000, step 16/63, loss = 0.4820\n",
            "epoch 1599 / 2000, step 32/63, loss = 0.4329\n",
            "epoch 1599 / 2000, step 48/63, loss = 0.2630\n",
            "epoch 1600 / 2000, step 16/63, loss = 0.3039\n",
            "epoch 1600 / 2000, step 32/63, loss = 0.5455\n",
            "epoch 1600 / 2000, step 48/63, loss = 0.3724\n",
            "epoch 1601 / 2000, step 16/63, loss = 0.4422\n",
            "epoch 1601 / 2000, step 32/63, loss = 0.7257\n",
            "epoch 1601 / 2000, step 48/63, loss = 0.2848\n",
            "epoch 1602 / 2000, step 16/63, loss = 0.3133\n",
            "epoch 1602 / 2000, step 32/63, loss = 0.3988\n",
            "epoch 1602 / 2000, step 48/63, loss = 0.4493\n",
            "epoch 1603 / 2000, step 16/63, loss = 0.3434\n",
            "epoch 1603 / 2000, step 32/63, loss = 0.4836\n",
            "epoch 1603 / 2000, step 48/63, loss = 0.6080\n",
            "epoch 1604 / 2000, step 16/63, loss = 0.4876\n",
            "epoch 1604 / 2000, step 32/63, loss = 0.5368\n",
            "epoch 1604 / 2000, step 48/63, loss = 0.4216\n",
            "epoch 1605 / 2000, step 16/63, loss = 0.3950\n",
            "epoch 1605 / 2000, step 32/63, loss = 0.3455\n",
            "epoch 1605 / 2000, step 48/63, loss = 0.3778\n",
            "epoch 1606 / 2000, step 16/63, loss = 0.4937\n",
            "epoch 1606 / 2000, step 32/63, loss = 0.4453\n",
            "epoch 1606 / 2000, step 48/63, loss = 0.4509\n",
            "epoch 1607 / 2000, step 16/63, loss = 0.8684\n",
            "epoch 1607 / 2000, step 32/63, loss = 0.3483\n",
            "epoch 1607 / 2000, step 48/63, loss = 0.4103\n",
            "epoch 1608 / 2000, step 16/63, loss = 0.4869\n",
            "epoch 1608 / 2000, step 32/63, loss = 0.3177\n",
            "epoch 1608 / 2000, step 48/63, loss = 0.4778\n",
            "epoch 1609 / 2000, step 16/63, loss = 0.3999\n",
            "epoch 1609 / 2000, step 32/63, loss = 0.3756\n",
            "epoch 1609 / 2000, step 48/63, loss = 0.3498\n",
            "epoch 1610 / 2000, step 16/63, loss = 0.4164\n",
            "epoch 1610 / 2000, step 32/63, loss = 0.3375\n",
            "epoch 1610 / 2000, step 48/63, loss = 0.3803\n",
            "epoch 1611 / 2000, step 16/63, loss = 0.6429\n",
            "epoch 1611 / 2000, step 32/63, loss = 0.3800\n",
            "epoch 1611 / 2000, step 48/63, loss = 0.3639\n",
            "epoch 1612 / 2000, step 16/63, loss = 0.5096\n",
            "epoch 1612 / 2000, step 32/63, loss = 0.6288\n",
            "epoch 1612 / 2000, step 48/63, loss = 0.2697\n",
            "epoch 1613 / 2000, step 16/63, loss = 0.3688\n",
            "epoch 1613 / 2000, step 32/63, loss = 0.4087\n",
            "epoch 1613 / 2000, step 48/63, loss = 0.4313\n",
            "epoch 1614 / 2000, step 16/63, loss = 0.5198\n",
            "epoch 1614 / 2000, step 32/63, loss = 0.2008\n",
            "epoch 1614 / 2000, step 48/63, loss = 0.4673\n",
            "epoch 1615 / 2000, step 16/63, loss = 0.2526\n",
            "epoch 1615 / 2000, step 32/63, loss = 0.4814\n",
            "epoch 1615 / 2000, step 48/63, loss = 0.1926\n",
            "epoch 1616 / 2000, step 16/63, loss = 0.4870\n",
            "epoch 1616 / 2000, step 32/63, loss = 0.4434\n",
            "epoch 1616 / 2000, step 48/63, loss = 0.5355\n",
            "epoch 1617 / 2000, step 16/63, loss = 0.4802\n",
            "epoch 1617 / 2000, step 32/63, loss = 0.5394\n",
            "epoch 1617 / 2000, step 48/63, loss = 0.4157\n",
            "epoch 1618 / 2000, step 16/63, loss = 0.5700\n",
            "epoch 1618 / 2000, step 32/63, loss = 0.5096\n",
            "epoch 1618 / 2000, step 48/63, loss = 0.4996\n",
            "epoch 1619 / 2000, step 16/63, loss = 0.3505\n",
            "epoch 1619 / 2000, step 32/63, loss = 0.4610\n",
            "epoch 1619 / 2000, step 48/63, loss = 0.3644\n",
            "epoch 1620 / 2000, step 16/63, loss = 0.2461\n",
            "epoch 1620 / 2000, step 32/63, loss = 0.3041\n",
            "epoch 1620 / 2000, step 48/63, loss = 0.8048\n",
            "epoch 1621 / 2000, step 16/63, loss = 0.3769\n",
            "epoch 1621 / 2000, step 32/63, loss = 0.3618\n",
            "epoch 1621 / 2000, step 48/63, loss = 0.2279\n",
            "epoch 1622 / 2000, step 16/63, loss = 0.4536\n",
            "epoch 1622 / 2000, step 32/63, loss = 0.5611\n",
            "epoch 1622 / 2000, step 48/63, loss = 0.2972\n",
            "epoch 1623 / 2000, step 16/63, loss = 0.6086\n",
            "epoch 1623 / 2000, step 32/63, loss = 0.2217\n",
            "epoch 1623 / 2000, step 48/63, loss = 0.5593\n",
            "epoch 1624 / 2000, step 16/63, loss = 0.3956\n",
            "epoch 1624 / 2000, step 32/63, loss = 0.3233\n",
            "epoch 1624 / 2000, step 48/63, loss = 0.3680\n",
            "epoch 1625 / 2000, step 16/63, loss = 0.4961\n",
            "epoch 1625 / 2000, step 32/63, loss = 0.4098\n",
            "epoch 1625 / 2000, step 48/63, loss = 0.2628\n",
            "epoch 1626 / 2000, step 16/63, loss = 0.2713\n",
            "epoch 1626 / 2000, step 32/63, loss = 0.3387\n",
            "epoch 1626 / 2000, step 48/63, loss = 0.3554\n",
            "epoch 1627 / 2000, step 16/63, loss = 0.3083\n",
            "epoch 1627 / 2000, step 32/63, loss = 0.3489\n",
            "epoch 1627 / 2000, step 48/63, loss = 0.3927\n",
            "epoch 1628 / 2000, step 16/63, loss = 0.2691\n",
            "epoch 1628 / 2000, step 32/63, loss = 0.5404\n",
            "epoch 1628 / 2000, step 48/63, loss = 0.3000\n",
            "epoch 1629 / 2000, step 16/63, loss = 0.2437\n",
            "epoch 1629 / 2000, step 32/63, loss = 0.3230\n",
            "epoch 1629 / 2000, step 48/63, loss = 0.3332\n",
            "epoch 1630 / 2000, step 16/63, loss = 0.4546\n",
            "epoch 1630 / 2000, step 32/63, loss = 0.2074\n",
            "epoch 1630 / 2000, step 48/63, loss = 0.4887\n",
            "epoch 1631 / 2000, step 16/63, loss = 0.3658\n",
            "epoch 1631 / 2000, step 32/63, loss = 0.4642\n",
            "epoch 1631 / 2000, step 48/63, loss = 0.3702\n",
            "epoch 1632 / 2000, step 16/63, loss = 0.4794\n",
            "epoch 1632 / 2000, step 32/63, loss = 0.3791\n",
            "epoch 1632 / 2000, step 48/63, loss = 0.6443\n",
            "epoch 1633 / 2000, step 16/63, loss = 0.6234\n",
            "epoch 1633 / 2000, step 32/63, loss = 0.4949\n",
            "epoch 1633 / 2000, step 48/63, loss = 0.3804\n",
            "epoch 1634 / 2000, step 16/63, loss = 0.4885\n",
            "epoch 1634 / 2000, step 32/63, loss = 0.4260\n",
            "epoch 1634 / 2000, step 48/63, loss = 0.4133\n",
            "epoch 1635 / 2000, step 16/63, loss = 0.2086\n",
            "epoch 1635 / 2000, step 32/63, loss = 0.6783\n",
            "epoch 1635 / 2000, step 48/63, loss = 0.2759\n",
            "epoch 1636 / 2000, step 16/63, loss = 0.6115\n",
            "epoch 1636 / 2000, step 32/63, loss = 0.4339\n",
            "epoch 1636 / 2000, step 48/63, loss = 0.5433\n",
            "epoch 1637 / 2000, step 16/63, loss = 0.3664\n",
            "epoch 1637 / 2000, step 32/63, loss = 0.5275\n",
            "epoch 1637 / 2000, step 48/63, loss = 0.8997\n",
            "epoch 1638 / 2000, step 16/63, loss = 0.5304\n",
            "epoch 1638 / 2000, step 32/63, loss = 0.3591\n",
            "epoch 1638 / 2000, step 48/63, loss = 0.4419\n",
            "epoch 1639 / 2000, step 16/63, loss = 0.3048\n",
            "epoch 1639 / 2000, step 32/63, loss = 0.2940\n",
            "epoch 1639 / 2000, step 48/63, loss = 0.2516\n",
            "epoch 1640 / 2000, step 16/63, loss = 0.5437\n",
            "epoch 1640 / 2000, step 32/63, loss = 0.5790\n",
            "epoch 1640 / 2000, step 48/63, loss = 0.5380\n",
            "epoch 1641 / 2000, step 16/63, loss = 0.4862\n",
            "epoch 1641 / 2000, step 32/63, loss = 0.4180\n",
            "epoch 1641 / 2000, step 48/63, loss = 0.4997\n",
            "epoch 1642 / 2000, step 16/63, loss = 0.7272\n",
            "epoch 1642 / 2000, step 32/63, loss = 0.6030\n",
            "epoch 1642 / 2000, step 48/63, loss = 0.3537\n",
            "epoch 1643 / 2000, step 16/63, loss = 0.4259\n",
            "epoch 1643 / 2000, step 32/63, loss = 0.6990\n",
            "epoch 1643 / 2000, step 48/63, loss = 0.2310\n",
            "epoch 1644 / 2000, step 16/63, loss = 0.3810\n",
            "epoch 1644 / 2000, step 32/63, loss = 0.5184\n",
            "epoch 1644 / 2000, step 48/63, loss = 0.4208\n",
            "epoch 1645 / 2000, step 16/63, loss = 0.5820\n",
            "epoch 1645 / 2000, step 32/63, loss = 0.1677\n",
            "epoch 1645 / 2000, step 48/63, loss = 0.3911\n",
            "epoch 1646 / 2000, step 16/63, loss = 0.3579\n",
            "epoch 1646 / 2000, step 32/63, loss = 0.3407\n",
            "epoch 1646 / 2000, step 48/63, loss = 0.3309\n",
            "epoch 1647 / 2000, step 16/63, loss = 0.4543\n",
            "epoch 1647 / 2000, step 32/63, loss = 0.3154\n",
            "epoch 1647 / 2000, step 48/63, loss = 0.4504\n",
            "epoch 1648 / 2000, step 16/63, loss = 0.4844\n",
            "epoch 1648 / 2000, step 32/63, loss = 0.2809\n",
            "epoch 1648 / 2000, step 48/63, loss = 0.3915\n",
            "epoch 1649 / 2000, step 16/63, loss = 0.3342\n",
            "epoch 1649 / 2000, step 32/63, loss = 0.5102\n",
            "epoch 1649 / 2000, step 48/63, loss = 0.3488\n",
            "epoch 1650 / 2000, step 16/63, loss = 0.4210\n",
            "epoch 1650 / 2000, step 32/63, loss = 0.5220\n",
            "epoch 1650 / 2000, step 48/63, loss = 0.2522\n",
            "epoch 1651 / 2000, step 16/63, loss = 0.3969\n",
            "epoch 1651 / 2000, step 32/63, loss = 0.3123\n",
            "epoch 1651 / 2000, step 48/63, loss = 0.4648\n",
            "epoch 1652 / 2000, step 16/63, loss = 0.7021\n",
            "epoch 1652 / 2000, step 32/63, loss = 0.4914\n",
            "epoch 1652 / 2000, step 48/63, loss = 0.2670\n",
            "epoch 1653 / 2000, step 16/63, loss = 0.3125\n",
            "epoch 1653 / 2000, step 32/63, loss = 0.6097\n",
            "epoch 1653 / 2000, step 48/63, loss = 0.2435\n",
            "epoch 1654 / 2000, step 16/63, loss = 0.3941\n",
            "epoch 1654 / 2000, step 32/63, loss = 0.2371\n",
            "epoch 1654 / 2000, step 48/63, loss = 0.6159\n",
            "epoch 1655 / 2000, step 16/63, loss = 0.5544\n",
            "epoch 1655 / 2000, step 32/63, loss = 0.2347\n",
            "epoch 1655 / 2000, step 48/63, loss = 0.2806\n",
            "epoch 1656 / 2000, step 16/63, loss = 0.3008\n",
            "epoch 1656 / 2000, step 32/63, loss = 0.6199\n",
            "epoch 1656 / 2000, step 48/63, loss = 0.4115\n",
            "epoch 1657 / 2000, step 16/63, loss = 0.4713\n",
            "epoch 1657 / 2000, step 32/63, loss = 0.4389\n",
            "epoch 1657 / 2000, step 48/63, loss = 0.4773\n",
            "epoch 1658 / 2000, step 16/63, loss = 0.2763\n",
            "epoch 1658 / 2000, step 32/63, loss = 0.5002\n",
            "epoch 1658 / 2000, step 48/63, loss = 0.3011\n",
            "epoch 1659 / 2000, step 16/63, loss = 0.4289\n",
            "epoch 1659 / 2000, step 32/63, loss = 0.3212\n",
            "epoch 1659 / 2000, step 48/63, loss = 0.5518\n",
            "epoch 1660 / 2000, step 16/63, loss = 0.4843\n",
            "epoch 1660 / 2000, step 32/63, loss = 0.6041\n",
            "epoch 1660 / 2000, step 48/63, loss = 0.4176\n",
            "epoch 1661 / 2000, step 16/63, loss = 0.6722\n",
            "epoch 1661 / 2000, step 32/63, loss = 0.3203\n",
            "epoch 1661 / 2000, step 48/63, loss = 0.3084\n",
            "epoch 1662 / 2000, step 16/63, loss = 0.3802\n",
            "epoch 1662 / 2000, step 32/63, loss = 0.3290\n",
            "epoch 1662 / 2000, step 48/63, loss = 0.4698\n",
            "epoch 1663 / 2000, step 16/63, loss = 0.5543\n",
            "epoch 1663 / 2000, step 32/63, loss = 0.5688\n",
            "epoch 1663 / 2000, step 48/63, loss = 0.5473\n",
            "epoch 1664 / 2000, step 16/63, loss = 0.3018\n",
            "epoch 1664 / 2000, step 32/63, loss = 0.3677\n",
            "epoch 1664 / 2000, step 48/63, loss = 0.4982\n",
            "epoch 1665 / 2000, step 16/63, loss = 0.6383\n",
            "epoch 1665 / 2000, step 32/63, loss = 0.2728\n",
            "epoch 1665 / 2000, step 48/63, loss = 0.4416\n",
            "epoch 1666 / 2000, step 16/63, loss = 0.2501\n",
            "epoch 1666 / 2000, step 32/63, loss = 0.4976\n",
            "epoch 1666 / 2000, step 48/63, loss = 0.2951\n",
            "epoch 1667 / 2000, step 16/63, loss = 0.3398\n",
            "epoch 1667 / 2000, step 32/63, loss = 0.3198\n",
            "epoch 1667 / 2000, step 48/63, loss = 0.3573\n",
            "epoch 1668 / 2000, step 16/63, loss = 0.8568\n",
            "epoch 1668 / 2000, step 32/63, loss = 0.7242\n",
            "epoch 1668 / 2000, step 48/63, loss = 0.2073\n",
            "epoch 1669 / 2000, step 16/63, loss = 0.4436\n",
            "epoch 1669 / 2000, step 32/63, loss = 0.4326\n",
            "epoch 1669 / 2000, step 48/63, loss = 0.2708\n",
            "epoch 1670 / 2000, step 16/63, loss = 0.4527\n",
            "epoch 1670 / 2000, step 32/63, loss = 0.2520\n",
            "epoch 1670 / 2000, step 48/63, loss = 0.3076\n",
            "epoch 1671 / 2000, step 16/63, loss = 0.2755\n",
            "epoch 1671 / 2000, step 32/63, loss = 0.4164\n",
            "epoch 1671 / 2000, step 48/63, loss = 0.4085\n",
            "epoch 1672 / 2000, step 16/63, loss = 0.3594\n",
            "epoch 1672 / 2000, step 32/63, loss = 0.2904\n",
            "epoch 1672 / 2000, step 48/63, loss = 0.5021\n",
            "epoch 1673 / 2000, step 16/63, loss = 0.4442\n",
            "epoch 1673 / 2000, step 32/63, loss = 0.3834\n",
            "epoch 1673 / 2000, step 48/63, loss = 0.2632\n",
            "epoch 1674 / 2000, step 16/63, loss = 0.5874\n",
            "epoch 1674 / 2000, step 32/63, loss = 0.4793\n",
            "epoch 1674 / 2000, step 48/63, loss = 0.2796\n",
            "epoch 1675 / 2000, step 16/63, loss = 0.3555\n",
            "epoch 1675 / 2000, step 32/63, loss = 0.4439\n",
            "epoch 1675 / 2000, step 48/63, loss = 0.5304\n",
            "epoch 1676 / 2000, step 16/63, loss = 0.3097\n",
            "epoch 1676 / 2000, step 32/63, loss = 0.3426\n",
            "epoch 1676 / 2000, step 48/63, loss = 0.4435\n",
            "epoch 1677 / 2000, step 16/63, loss = 0.3841\n",
            "epoch 1677 / 2000, step 32/63, loss = 0.4037\n",
            "epoch 1677 / 2000, step 48/63, loss = 0.3794\n",
            "epoch 1678 / 2000, step 16/63, loss = 0.3716\n",
            "epoch 1678 / 2000, step 32/63, loss = 0.4448\n",
            "epoch 1678 / 2000, step 48/63, loss = 0.8154\n",
            "epoch 1679 / 2000, step 16/63, loss = 0.2805\n",
            "epoch 1679 / 2000, step 32/63, loss = 0.3985\n",
            "epoch 1679 / 2000, step 48/63, loss = 0.5123\n",
            "epoch 1680 / 2000, step 16/63, loss = 0.4309\n",
            "epoch 1680 / 2000, step 32/63, loss = 0.2589\n",
            "epoch 1680 / 2000, step 48/63, loss = 0.5213\n",
            "epoch 1681 / 2000, step 16/63, loss = 0.3348\n",
            "epoch 1681 / 2000, step 32/63, loss = 0.5446\n",
            "epoch 1681 / 2000, step 48/63, loss = 0.4234\n",
            "epoch 1682 / 2000, step 16/63, loss = 0.5379\n",
            "epoch 1682 / 2000, step 32/63, loss = 0.4004\n",
            "epoch 1682 / 2000, step 48/63, loss = 0.4696\n",
            "epoch 1683 / 2000, step 16/63, loss = 0.3875\n",
            "epoch 1683 / 2000, step 32/63, loss = 0.2484\n",
            "epoch 1683 / 2000, step 48/63, loss = 0.3123\n",
            "epoch 1684 / 2000, step 16/63, loss = 0.2314\n",
            "epoch 1684 / 2000, step 32/63, loss = 0.3841\n",
            "epoch 1684 / 2000, step 48/63, loss = 0.5004\n",
            "epoch 1685 / 2000, step 16/63, loss = 0.4453\n",
            "epoch 1685 / 2000, step 32/63, loss = 0.3626\n",
            "epoch 1685 / 2000, step 48/63, loss = 0.4932\n",
            "epoch 1686 / 2000, step 16/63, loss = 0.3117\n",
            "epoch 1686 / 2000, step 32/63, loss = 0.2036\n",
            "epoch 1686 / 2000, step 48/63, loss = 0.3449\n",
            "epoch 1687 / 2000, step 16/63, loss = 0.4601\n",
            "epoch 1687 / 2000, step 32/63, loss = 0.3721\n",
            "epoch 1687 / 2000, step 48/63, loss = 0.5067\n",
            "epoch 1688 / 2000, step 16/63, loss = 0.5428\n",
            "epoch 1688 / 2000, step 32/63, loss = 0.2013\n",
            "epoch 1688 / 2000, step 48/63, loss = 0.4041\n",
            "epoch 1689 / 2000, step 16/63, loss = 0.5060\n",
            "epoch 1689 / 2000, step 32/63, loss = 0.5811\n",
            "epoch 1689 / 2000, step 48/63, loss = 0.5403\n",
            "epoch 1690 / 2000, step 16/63, loss = 0.2228\n",
            "epoch 1690 / 2000, step 32/63, loss = 0.4636\n",
            "epoch 1690 / 2000, step 48/63, loss = 0.3710\n",
            "epoch 1691 / 2000, step 16/63, loss = 0.2626\n",
            "epoch 1691 / 2000, step 32/63, loss = 0.5269\n",
            "epoch 1691 / 2000, step 48/63, loss = 0.4827\n",
            "epoch 1692 / 2000, step 16/63, loss = 0.5782\n",
            "epoch 1692 / 2000, step 32/63, loss = 0.2690\n",
            "epoch 1692 / 2000, step 48/63, loss = 0.6110\n",
            "epoch 1693 / 2000, step 16/63, loss = 0.2549\n",
            "epoch 1693 / 2000, step 32/63, loss = 0.3560\n",
            "epoch 1693 / 2000, step 48/63, loss = 0.3073\n",
            "epoch 1694 / 2000, step 16/63, loss = 0.3984\n",
            "epoch 1694 / 2000, step 32/63, loss = 0.3135\n",
            "epoch 1694 / 2000, step 48/63, loss = 0.3345\n",
            "epoch 1695 / 2000, step 16/63, loss = 0.1727\n",
            "epoch 1695 / 2000, step 32/63, loss = 0.2984\n",
            "epoch 1695 / 2000, step 48/63, loss = 0.3701\n",
            "epoch 1696 / 2000, step 16/63, loss = 0.3783\n",
            "epoch 1696 / 2000, step 32/63, loss = 0.3399\n",
            "epoch 1696 / 2000, step 48/63, loss = 0.3991\n",
            "epoch 1697 / 2000, step 16/63, loss = 0.3266\n",
            "epoch 1697 / 2000, step 32/63, loss = 0.2942\n",
            "epoch 1697 / 2000, step 48/63, loss = 0.3879\n",
            "epoch 1698 / 2000, step 16/63, loss = 0.7119\n",
            "epoch 1698 / 2000, step 32/63, loss = 0.3644\n",
            "epoch 1698 / 2000, step 48/63, loss = 0.5130\n",
            "epoch 1699 / 2000, step 16/63, loss = 0.3284\n",
            "epoch 1699 / 2000, step 32/63, loss = 0.2468\n",
            "epoch 1699 / 2000, step 48/63, loss = 0.4425\n",
            "epoch 1700 / 2000, step 16/63, loss = 0.3577\n",
            "epoch 1700 / 2000, step 32/63, loss = 0.1672\n",
            "epoch 1700 / 2000, step 48/63, loss = 0.5188\n",
            "epoch 1701 / 2000, step 16/63, loss = 0.7155\n",
            "epoch 1701 / 2000, step 32/63, loss = 0.2124\n",
            "epoch 1701 / 2000, step 48/63, loss = 0.6753\n",
            "epoch 1702 / 2000, step 16/63, loss = 0.4064\n",
            "epoch 1702 / 2000, step 32/63, loss = 0.4951\n",
            "epoch 1702 / 2000, step 48/63, loss = 0.6847\n",
            "epoch 1703 / 2000, step 16/63, loss = 0.2699\n",
            "epoch 1703 / 2000, step 32/63, loss = 0.3463\n",
            "epoch 1703 / 2000, step 48/63, loss = 0.2728\n",
            "epoch 1704 / 2000, step 16/63, loss = 0.2972\n",
            "epoch 1704 / 2000, step 32/63, loss = 0.2905\n",
            "epoch 1704 / 2000, step 48/63, loss = 0.5420\n",
            "epoch 1705 / 2000, step 16/63, loss = 0.5290\n",
            "epoch 1705 / 2000, step 32/63, loss = 0.3087\n",
            "epoch 1705 / 2000, step 48/63, loss = 0.3682\n",
            "epoch 1706 / 2000, step 16/63, loss = 0.3244\n",
            "epoch 1706 / 2000, step 32/63, loss = 0.4480\n",
            "epoch 1706 / 2000, step 48/63, loss = 0.4883\n",
            "epoch 1707 / 2000, step 16/63, loss = 0.5141\n",
            "epoch 1707 / 2000, step 32/63, loss = 0.4432\n",
            "epoch 1707 / 2000, step 48/63, loss = 0.4687\n",
            "epoch 1708 / 2000, step 16/63, loss = 0.6586\n",
            "epoch 1708 / 2000, step 32/63, loss = 0.3618\n",
            "epoch 1708 / 2000, step 48/63, loss = 0.3812\n",
            "epoch 1709 / 2000, step 16/63, loss = 0.5029\n",
            "epoch 1709 / 2000, step 32/63, loss = 0.4153\n",
            "epoch 1709 / 2000, step 48/63, loss = 0.5525\n",
            "epoch 1710 / 2000, step 16/63, loss = 0.4895\n",
            "epoch 1710 / 2000, step 32/63, loss = 0.3525\n",
            "epoch 1710 / 2000, step 48/63, loss = 0.5458\n",
            "epoch 1711 / 2000, step 16/63, loss = 0.4414\n",
            "epoch 1711 / 2000, step 32/63, loss = 0.3997\n",
            "epoch 1711 / 2000, step 48/63, loss = 0.2628\n",
            "epoch 1712 / 2000, step 16/63, loss = 0.3806\n",
            "epoch 1712 / 2000, step 32/63, loss = 0.3835\n",
            "epoch 1712 / 2000, step 48/63, loss = 0.2916\n",
            "epoch 1713 / 2000, step 16/63, loss = 0.2209\n",
            "epoch 1713 / 2000, step 32/63, loss = 0.2542\n",
            "epoch 1713 / 2000, step 48/63, loss = 0.2443\n",
            "epoch 1714 / 2000, step 16/63, loss = 0.3378\n",
            "epoch 1714 / 2000, step 32/63, loss = 0.4055\n",
            "epoch 1714 / 2000, step 48/63, loss = 0.3055\n",
            "epoch 1715 / 2000, step 16/63, loss = 0.4014\n",
            "epoch 1715 / 2000, step 32/63, loss = 0.3683\n",
            "epoch 1715 / 2000, step 48/63, loss = 0.1756\n",
            "epoch 1716 / 2000, step 16/63, loss = 0.2557\n",
            "epoch 1716 / 2000, step 32/63, loss = 0.2481\n",
            "epoch 1716 / 2000, step 48/63, loss = 0.2889\n",
            "epoch 1717 / 2000, step 16/63, loss = 0.4866\n",
            "epoch 1717 / 2000, step 32/63, loss = 0.3545\n",
            "epoch 1717 / 2000, step 48/63, loss = 0.5607\n",
            "epoch 1718 / 2000, step 16/63, loss = 0.3993\n",
            "epoch 1718 / 2000, step 32/63, loss = 0.6152\n",
            "epoch 1718 / 2000, step 48/63, loss = 0.1287\n",
            "epoch 1719 / 2000, step 16/63, loss = 0.2111\n",
            "epoch 1719 / 2000, step 32/63, loss = 0.2759\n",
            "epoch 1719 / 2000, step 48/63, loss = 0.3466\n",
            "epoch 1720 / 2000, step 16/63, loss = 0.3715\n",
            "epoch 1720 / 2000, step 32/63, loss = 0.3689\n",
            "epoch 1720 / 2000, step 48/63, loss = 0.5688\n",
            "epoch 1721 / 2000, step 16/63, loss = 0.3269\n",
            "epoch 1721 / 2000, step 32/63, loss = 0.2753\n",
            "epoch 1721 / 2000, step 48/63, loss = 0.2949\n",
            "epoch 1722 / 2000, step 16/63, loss = 0.4305\n",
            "epoch 1722 / 2000, step 32/63, loss = 0.4666\n",
            "epoch 1722 / 2000, step 48/63, loss = 0.3216\n",
            "epoch 1723 / 2000, step 16/63, loss = 0.3994\n",
            "epoch 1723 / 2000, step 32/63, loss = 0.5127\n",
            "epoch 1723 / 2000, step 48/63, loss = 0.3476\n",
            "epoch 1724 / 2000, step 16/63, loss = 0.3071\n",
            "epoch 1724 / 2000, step 32/63, loss = 0.2830\n",
            "epoch 1724 / 2000, step 48/63, loss = 0.3368\n",
            "epoch 1725 / 2000, step 16/63, loss = 0.5001\n",
            "epoch 1725 / 2000, step 32/63, loss = 0.5373\n",
            "epoch 1725 / 2000, step 48/63, loss = 0.3689\n",
            "epoch 1726 / 2000, step 16/63, loss = 0.5077\n",
            "epoch 1726 / 2000, step 32/63, loss = 0.4917\n",
            "epoch 1726 / 2000, step 48/63, loss = 0.3795\n",
            "epoch 1727 / 2000, step 16/63, loss = 0.4208\n",
            "epoch 1727 / 2000, step 32/63, loss = 0.3981\n",
            "epoch 1727 / 2000, step 48/63, loss = 0.2810\n",
            "epoch 1728 / 2000, step 16/63, loss = 0.6717\n",
            "epoch 1728 / 2000, step 32/63, loss = 0.3413\n",
            "epoch 1728 / 2000, step 48/63, loss = 0.4310\n",
            "epoch 1729 / 2000, step 16/63, loss = 0.2564\n",
            "epoch 1729 / 2000, step 32/63, loss = 0.2362\n",
            "epoch 1729 / 2000, step 48/63, loss = 0.5825\n",
            "epoch 1730 / 2000, step 16/63, loss = 0.5851\n",
            "epoch 1730 / 2000, step 32/63, loss = 0.4361\n",
            "epoch 1730 / 2000, step 48/63, loss = 0.3138\n",
            "epoch 1731 / 2000, step 16/63, loss = 0.3315\n",
            "epoch 1731 / 2000, step 32/63, loss = 0.4043\n",
            "epoch 1731 / 2000, step 48/63, loss = 0.3414\n",
            "epoch 1732 / 2000, step 16/63, loss = 0.4060\n",
            "epoch 1732 / 2000, step 32/63, loss = 0.4526\n",
            "epoch 1732 / 2000, step 48/63, loss = 0.3710\n",
            "epoch 1733 / 2000, step 16/63, loss = 0.3801\n",
            "epoch 1733 / 2000, step 32/63, loss = 0.2956\n",
            "epoch 1733 / 2000, step 48/63, loss = 0.4144\n",
            "epoch 1734 / 2000, step 16/63, loss = 0.5381\n",
            "epoch 1734 / 2000, step 32/63, loss = 0.2105\n",
            "epoch 1734 / 2000, step 48/63, loss = 0.3656\n",
            "epoch 1735 / 2000, step 16/63, loss = 0.6669\n",
            "epoch 1735 / 2000, step 32/63, loss = 0.2845\n",
            "epoch 1735 / 2000, step 48/63, loss = 0.3144\n",
            "epoch 1736 / 2000, step 16/63, loss = 0.4763\n",
            "epoch 1736 / 2000, step 32/63, loss = 0.5463\n",
            "epoch 1736 / 2000, step 48/63, loss = 0.4919\n",
            "epoch 1737 / 2000, step 16/63, loss = 0.2603\n",
            "epoch 1737 / 2000, step 32/63, loss = 0.5117\n",
            "epoch 1737 / 2000, step 48/63, loss = 0.1694\n",
            "epoch 1738 / 2000, step 16/63, loss = 0.3584\n",
            "epoch 1738 / 2000, step 32/63, loss = 0.3161\n",
            "epoch 1738 / 2000, step 48/63, loss = 0.4115\n",
            "epoch 1739 / 2000, step 16/63, loss = 0.4684\n",
            "epoch 1739 / 2000, step 32/63, loss = 0.5326\n",
            "epoch 1739 / 2000, step 48/63, loss = 0.4570\n",
            "epoch 1740 / 2000, step 16/63, loss = 0.4460\n",
            "epoch 1740 / 2000, step 32/63, loss = 0.3687\n",
            "epoch 1740 / 2000, step 48/63, loss = 0.2964\n",
            "epoch 1741 / 2000, step 16/63, loss = 0.2775\n",
            "epoch 1741 / 2000, step 32/63, loss = 0.4848\n",
            "epoch 1741 / 2000, step 48/63, loss = 0.6564\n",
            "epoch 1742 / 2000, step 16/63, loss = 0.5922\n",
            "epoch 1742 / 2000, step 32/63, loss = 0.4418\n",
            "epoch 1742 / 2000, step 48/63, loss = 0.3325\n",
            "epoch 1743 / 2000, step 16/63, loss = 0.1722\n",
            "epoch 1743 / 2000, step 32/63, loss = 0.2593\n",
            "epoch 1743 / 2000, step 48/63, loss = 0.7690\n",
            "epoch 1744 / 2000, step 16/63, loss = 0.4917\n",
            "epoch 1744 / 2000, step 32/63, loss = 0.3171\n",
            "epoch 1744 / 2000, step 48/63, loss = 0.3824\n",
            "epoch 1745 / 2000, step 16/63, loss = 0.5526\n",
            "epoch 1745 / 2000, step 32/63, loss = 0.2991\n",
            "epoch 1745 / 2000, step 48/63, loss = 0.4419\n",
            "epoch 1746 / 2000, step 16/63, loss = 0.3440\n",
            "epoch 1746 / 2000, step 32/63, loss = 0.3001\n",
            "epoch 1746 / 2000, step 48/63, loss = 0.5331\n",
            "epoch 1747 / 2000, step 16/63, loss = 0.3611\n",
            "epoch 1747 / 2000, step 32/63, loss = 0.3791\n",
            "epoch 1747 / 2000, step 48/63, loss = 0.2916\n",
            "epoch 1748 / 2000, step 16/63, loss = 0.1956\n",
            "epoch 1748 / 2000, step 32/63, loss = 0.2523\n",
            "epoch 1748 / 2000, step 48/63, loss = 0.2819\n",
            "epoch 1749 / 2000, step 16/63, loss = 0.3432\n",
            "epoch 1749 / 2000, step 32/63, loss = 0.5196\n",
            "epoch 1749 / 2000, step 48/63, loss = 0.3932\n",
            "epoch 1750 / 2000, step 16/63, loss = 0.5434\n",
            "epoch 1750 / 2000, step 32/63, loss = 0.3871\n",
            "epoch 1750 / 2000, step 48/63, loss = 0.3689\n",
            "epoch 1751 / 2000, step 16/63, loss = 0.2365\n",
            "epoch 1751 / 2000, step 32/63, loss = 0.4343\n",
            "epoch 1751 / 2000, step 48/63, loss = 0.4134\n",
            "epoch 1752 / 2000, step 16/63, loss = 0.2903\n",
            "epoch 1752 / 2000, step 32/63, loss = 0.5491\n",
            "epoch 1752 / 2000, step 48/63, loss = 0.4074\n",
            "epoch 1753 / 2000, step 16/63, loss = 0.4138\n",
            "epoch 1753 / 2000, step 32/63, loss = 0.2408\n",
            "epoch 1753 / 2000, step 48/63, loss = 0.6311\n",
            "epoch 1754 / 2000, step 16/63, loss = 0.4367\n",
            "epoch 1754 / 2000, step 32/63, loss = 0.5705\n",
            "epoch 1754 / 2000, step 48/63, loss = 0.4950\n",
            "epoch 1755 / 2000, step 16/63, loss = 0.3344\n",
            "epoch 1755 / 2000, step 32/63, loss = 0.2385\n",
            "epoch 1755 / 2000, step 48/63, loss = 0.5549\n",
            "epoch 1756 / 2000, step 16/63, loss = 0.4273\n",
            "epoch 1756 / 2000, step 32/63, loss = 0.2297\n",
            "epoch 1756 / 2000, step 48/63, loss = 0.2065\n",
            "epoch 1757 / 2000, step 16/63, loss = 0.3351\n",
            "epoch 1757 / 2000, step 32/63, loss = 0.2693\n",
            "epoch 1757 / 2000, step 48/63, loss = 0.2458\n",
            "epoch 1758 / 2000, step 16/63, loss = 0.3725\n",
            "epoch 1758 / 2000, step 32/63, loss = 0.2561\n",
            "epoch 1758 / 2000, step 48/63, loss = 0.3702\n",
            "epoch 1759 / 2000, step 16/63, loss = 0.3307\n",
            "epoch 1759 / 2000, step 32/63, loss = 0.3775\n",
            "epoch 1759 / 2000, step 48/63, loss = 0.3363\n",
            "epoch 1760 / 2000, step 16/63, loss = 0.5170\n",
            "epoch 1760 / 2000, step 32/63, loss = 0.5027\n",
            "epoch 1760 / 2000, step 48/63, loss = 0.5016\n",
            "epoch 1761 / 2000, step 16/63, loss = 0.4223\n",
            "epoch 1761 / 2000, step 32/63, loss = 0.3688\n",
            "epoch 1761 / 2000, step 48/63, loss = 0.5864\n",
            "epoch 1762 / 2000, step 16/63, loss = 0.4614\n",
            "epoch 1762 / 2000, step 32/63, loss = 0.4975\n",
            "epoch 1762 / 2000, step 48/63, loss = 0.3262\n",
            "epoch 1763 / 2000, step 16/63, loss = 0.3152\n",
            "epoch 1763 / 2000, step 32/63, loss = 0.2803\n",
            "epoch 1763 / 2000, step 48/63, loss = 0.3219\n",
            "epoch 1764 / 2000, step 16/63, loss = 0.4289\n",
            "epoch 1764 / 2000, step 32/63, loss = 0.3682\n",
            "epoch 1764 / 2000, step 48/63, loss = 0.2950\n",
            "epoch 1765 / 2000, step 16/63, loss = 0.1815\n",
            "epoch 1765 / 2000, step 32/63, loss = 0.2741\n",
            "epoch 1765 / 2000, step 48/63, loss = 0.4548\n",
            "epoch 1766 / 2000, step 16/63, loss = 0.5576\n",
            "epoch 1766 / 2000, step 32/63, loss = 0.4461\n",
            "epoch 1766 / 2000, step 48/63, loss = 0.2147\n",
            "epoch 1767 / 2000, step 16/63, loss = 0.1729\n",
            "epoch 1767 / 2000, step 32/63, loss = 0.3151\n",
            "epoch 1767 / 2000, step 48/63, loss = 0.2847\n",
            "epoch 1768 / 2000, step 16/63, loss = 0.3478\n",
            "epoch 1768 / 2000, step 32/63, loss = 0.3301\n",
            "epoch 1768 / 2000, step 48/63, loss = 0.4092\n",
            "epoch 1769 / 2000, step 16/63, loss = 0.3813\n",
            "epoch 1769 / 2000, step 32/63, loss = 0.3264\n",
            "epoch 1769 / 2000, step 48/63, loss = 0.3104\n",
            "epoch 1770 / 2000, step 16/63, loss = 0.3669\n",
            "epoch 1770 / 2000, step 32/63, loss = 0.3587\n",
            "epoch 1770 / 2000, step 48/63, loss = 0.3904\n",
            "epoch 1771 / 2000, step 16/63, loss = 0.4119\n",
            "epoch 1771 / 2000, step 32/63, loss = 0.2532\n",
            "epoch 1771 / 2000, step 48/63, loss = 0.5274\n",
            "epoch 1772 / 2000, step 16/63, loss = 0.3208\n",
            "epoch 1772 / 2000, step 32/63, loss = 0.6897\n",
            "epoch 1772 / 2000, step 48/63, loss = 0.2649\n",
            "epoch 1773 / 2000, step 16/63, loss = 0.7307\n",
            "epoch 1773 / 2000, step 32/63, loss = 0.2340\n",
            "epoch 1773 / 2000, step 48/63, loss = 0.2515\n",
            "epoch 1774 / 2000, step 16/63, loss = 0.4335\n",
            "epoch 1774 / 2000, step 32/63, loss = 0.3228\n",
            "epoch 1774 / 2000, step 48/63, loss = 0.4361\n",
            "epoch 1775 / 2000, step 16/63, loss = 0.3698\n",
            "epoch 1775 / 2000, step 32/63, loss = 0.2830\n",
            "epoch 1775 / 2000, step 48/63, loss = 0.4160\n",
            "epoch 1776 / 2000, step 16/63, loss = 0.3441\n",
            "epoch 1776 / 2000, step 32/63, loss = 0.3930\n",
            "epoch 1776 / 2000, step 48/63, loss = 0.4352\n",
            "epoch 1777 / 2000, step 16/63, loss = 0.3309\n",
            "epoch 1777 / 2000, step 32/63, loss = 0.4128\n",
            "epoch 1777 / 2000, step 48/63, loss = 0.4939\n",
            "epoch 1778 / 2000, step 16/63, loss = 0.3144\n",
            "epoch 1778 / 2000, step 32/63, loss = 0.3579\n",
            "epoch 1778 / 2000, step 48/63, loss = 0.3268\n",
            "epoch 1779 / 2000, step 16/63, loss = 0.2316\n",
            "epoch 1779 / 2000, step 32/63, loss = 0.3431\n",
            "epoch 1779 / 2000, step 48/63, loss = 0.3671\n",
            "epoch 1780 / 2000, step 16/63, loss = 0.3444\n",
            "epoch 1780 / 2000, step 32/63, loss = 0.3574\n",
            "epoch 1780 / 2000, step 48/63, loss = 0.3225\n",
            "epoch 1781 / 2000, step 16/63, loss = 0.9332\n",
            "epoch 1781 / 2000, step 32/63, loss = 0.3210\n",
            "epoch 1781 / 2000, step 48/63, loss = 0.6341\n",
            "epoch 1782 / 2000, step 16/63, loss = 0.4940\n",
            "epoch 1782 / 2000, step 32/63, loss = 0.3151\n",
            "epoch 1782 / 2000, step 48/63, loss = 0.2478\n",
            "epoch 1783 / 2000, step 16/63, loss = 0.2925\n",
            "epoch 1783 / 2000, step 32/63, loss = 0.2500\n",
            "epoch 1783 / 2000, step 48/63, loss = 0.2636\n",
            "epoch 1784 / 2000, step 16/63, loss = 0.3923\n",
            "epoch 1784 / 2000, step 32/63, loss = 0.4504\n",
            "epoch 1784 / 2000, step 48/63, loss = 0.2988\n",
            "epoch 1785 / 2000, step 16/63, loss = 0.4303\n",
            "epoch 1785 / 2000, step 32/63, loss = 0.2948\n",
            "epoch 1785 / 2000, step 48/63, loss = 0.3372\n",
            "epoch 1786 / 2000, step 16/63, loss = 0.7165\n",
            "epoch 1786 / 2000, step 32/63, loss = 0.4494\n",
            "epoch 1786 / 2000, step 48/63, loss = 0.7071\n",
            "epoch 1787 / 2000, step 16/63, loss = 0.4213\n",
            "epoch 1787 / 2000, step 32/63, loss = 0.3946\n",
            "epoch 1787 / 2000, step 48/63, loss = 0.7933\n",
            "epoch 1788 / 2000, step 16/63, loss = 0.3089\n",
            "epoch 1788 / 2000, step 32/63, loss = 0.4361\n",
            "epoch 1788 / 2000, step 48/63, loss = 0.4646\n",
            "epoch 1789 / 2000, step 16/63, loss = 0.5424\n",
            "epoch 1789 / 2000, step 32/63, loss = 0.4693\n",
            "epoch 1789 / 2000, step 48/63, loss = 0.4076\n",
            "epoch 1790 / 2000, step 16/63, loss = 0.5268\n",
            "epoch 1790 / 2000, step 32/63, loss = 0.1983\n",
            "epoch 1790 / 2000, step 48/63, loss = 0.4404\n",
            "epoch 1791 / 2000, step 16/63, loss = 0.4482\n",
            "epoch 1791 / 2000, step 32/63, loss = 0.3411\n",
            "epoch 1791 / 2000, step 48/63, loss = 0.5320\n",
            "epoch 1792 / 2000, step 16/63, loss = 0.4390\n",
            "epoch 1792 / 2000, step 32/63, loss = 0.4356\n",
            "epoch 1792 / 2000, step 48/63, loss = 0.3610\n",
            "epoch 1793 / 2000, step 16/63, loss = 0.3203\n",
            "epoch 1793 / 2000, step 32/63, loss = 0.3854\n",
            "epoch 1793 / 2000, step 48/63, loss = 0.4475\n",
            "epoch 1794 / 2000, step 16/63, loss = 0.7157\n",
            "epoch 1794 / 2000, step 32/63, loss = 0.3131\n",
            "epoch 1794 / 2000, step 48/63, loss = 0.2669\n",
            "epoch 1795 / 2000, step 16/63, loss = 0.6020\n",
            "epoch 1795 / 2000, step 32/63, loss = 0.5732\n",
            "epoch 1795 / 2000, step 48/63, loss = 0.2262\n",
            "epoch 1796 / 2000, step 16/63, loss = 0.5132\n",
            "epoch 1796 / 2000, step 32/63, loss = 0.2904\n",
            "epoch 1796 / 2000, step 48/63, loss = 0.3065\n",
            "epoch 1797 / 2000, step 16/63, loss = 0.2516\n",
            "epoch 1797 / 2000, step 32/63, loss = 0.3372\n",
            "epoch 1797 / 2000, step 48/63, loss = 0.3880\n",
            "epoch 1798 / 2000, step 16/63, loss = 0.4024\n",
            "epoch 1798 / 2000, step 32/63, loss = 0.3773\n",
            "epoch 1798 / 2000, step 48/63, loss = 0.3463\n",
            "epoch 1799 / 2000, step 16/63, loss = 0.4144\n",
            "epoch 1799 / 2000, step 32/63, loss = 0.2565\n",
            "epoch 1799 / 2000, step 48/63, loss = 0.4186\n",
            "epoch 1800 / 2000, step 16/63, loss = 0.4983\n",
            "epoch 1800 / 2000, step 32/63, loss = 0.3607\n",
            "epoch 1800 / 2000, step 48/63, loss = 0.4943\n",
            "epoch 1801 / 2000, step 16/63, loss = 0.4120\n",
            "epoch 1801 / 2000, step 32/63, loss = 0.3745\n",
            "epoch 1801 / 2000, step 48/63, loss = 0.6923\n",
            "epoch 1802 / 2000, step 16/63, loss = 0.1677\n",
            "epoch 1802 / 2000, step 32/63, loss = 0.2376\n",
            "epoch 1802 / 2000, step 48/63, loss = 0.4792\n",
            "epoch 1803 / 2000, step 16/63, loss = 0.3229\n",
            "epoch 1803 / 2000, step 32/63, loss = 0.3007\n",
            "epoch 1803 / 2000, step 48/63, loss = 0.3091\n",
            "epoch 1804 / 2000, step 16/63, loss = 0.3680\n",
            "epoch 1804 / 2000, step 32/63, loss = 0.6660\n",
            "epoch 1804 / 2000, step 48/63, loss = 0.3288\n",
            "epoch 1805 / 2000, step 16/63, loss = 0.3067\n",
            "epoch 1805 / 2000, step 32/63, loss = 0.3020\n",
            "epoch 1805 / 2000, step 48/63, loss = 0.3225\n",
            "epoch 1806 / 2000, step 16/63, loss = 0.4590\n",
            "epoch 1806 / 2000, step 32/63, loss = 0.4737\n",
            "epoch 1806 / 2000, step 48/63, loss = 0.3144\n",
            "epoch 1807 / 2000, step 16/63, loss = 0.3168\n",
            "epoch 1807 / 2000, step 32/63, loss = 0.3451\n",
            "epoch 1807 / 2000, step 48/63, loss = 0.5738\n",
            "epoch 1808 / 2000, step 16/63, loss = 0.1738\n",
            "epoch 1808 / 2000, step 32/63, loss = 0.3855\n",
            "epoch 1808 / 2000, step 48/63, loss = 0.3269\n",
            "epoch 1809 / 2000, step 16/63, loss = 0.2315\n",
            "epoch 1809 / 2000, step 32/63, loss = 0.2337\n",
            "epoch 1809 / 2000, step 48/63, loss = 0.4989\n",
            "epoch 1810 / 2000, step 16/63, loss = 0.4065\n",
            "epoch 1810 / 2000, step 32/63, loss = 0.1854\n",
            "epoch 1810 / 2000, step 48/63, loss = 0.4864\n",
            "epoch 1811 / 2000, step 16/63, loss = 0.3638\n",
            "epoch 1811 / 2000, step 32/63, loss = 0.2137\n",
            "epoch 1811 / 2000, step 48/63, loss = 0.3111\n",
            "epoch 1812 / 2000, step 16/63, loss = 0.4409\n",
            "epoch 1812 / 2000, step 32/63, loss = 0.3752\n",
            "epoch 1812 / 2000, step 48/63, loss = 0.2229\n",
            "epoch 1813 / 2000, step 16/63, loss = 0.4645\n",
            "epoch 1813 / 2000, step 32/63, loss = 0.5016\n",
            "epoch 1813 / 2000, step 48/63, loss = 0.3611\n",
            "epoch 1814 / 2000, step 16/63, loss = 0.3132\n",
            "epoch 1814 / 2000, step 32/63, loss = 0.2842\n",
            "epoch 1814 / 2000, step 48/63, loss = 0.4258\n",
            "epoch 1815 / 2000, step 16/63, loss = 0.2317\n",
            "epoch 1815 / 2000, step 32/63, loss = 0.3468\n",
            "epoch 1815 / 2000, step 48/63, loss = 0.5087\n",
            "epoch 1816 / 2000, step 16/63, loss = 0.3113\n",
            "epoch 1816 / 2000, step 32/63, loss = 0.3299\n",
            "epoch 1816 / 2000, step 48/63, loss = 0.3610\n",
            "epoch 1817 / 2000, step 16/63, loss = 0.4549\n",
            "epoch 1817 / 2000, step 32/63, loss = 0.2297\n",
            "epoch 1817 / 2000, step 48/63, loss = 0.2670\n",
            "epoch 1818 / 2000, step 16/63, loss = 0.5859\n",
            "epoch 1818 / 2000, step 32/63, loss = 0.1780\n",
            "epoch 1818 / 2000, step 48/63, loss = 0.2820\n",
            "epoch 1819 / 2000, step 16/63, loss = 0.3831\n",
            "epoch 1819 / 2000, step 32/63, loss = 0.3211\n",
            "epoch 1819 / 2000, step 48/63, loss = 0.4600\n",
            "epoch 1820 / 2000, step 16/63, loss = 0.2462\n",
            "epoch 1820 / 2000, step 32/63, loss = 0.4603\n",
            "epoch 1820 / 2000, step 48/63, loss = 0.3247\n",
            "epoch 1821 / 2000, step 16/63, loss = 0.2573\n",
            "epoch 1821 / 2000, step 32/63, loss = 0.3837\n",
            "epoch 1821 / 2000, step 48/63, loss = 0.3886\n",
            "epoch 1822 / 2000, step 16/63, loss = 0.2360\n",
            "epoch 1822 / 2000, step 32/63, loss = 0.4042\n",
            "epoch 1822 / 2000, step 48/63, loss = 0.4198\n",
            "epoch 1823 / 2000, step 16/63, loss = 0.5648\n",
            "epoch 1823 / 2000, step 32/63, loss = 0.2968\n",
            "epoch 1823 / 2000, step 48/63, loss = 0.3156\n",
            "epoch 1824 / 2000, step 16/63, loss = 0.1938\n",
            "epoch 1824 / 2000, step 32/63, loss = 0.1801\n",
            "epoch 1824 / 2000, step 48/63, loss = 0.4115\n",
            "epoch 1825 / 2000, step 16/63, loss = 0.5787\n",
            "epoch 1825 / 2000, step 32/63, loss = 0.3980\n",
            "epoch 1825 / 2000, step 48/63, loss = 0.3594\n",
            "epoch 1826 / 2000, step 16/63, loss = 0.3834\n",
            "epoch 1826 / 2000, step 32/63, loss = 0.2891\n",
            "epoch 1826 / 2000, step 48/63, loss = 0.2600\n",
            "epoch 1827 / 2000, step 16/63, loss = 0.3202\n",
            "epoch 1827 / 2000, step 32/63, loss = 0.2619\n",
            "epoch 1827 / 2000, step 48/63, loss = 0.1881\n",
            "epoch 1828 / 2000, step 16/63, loss = 0.3984\n",
            "epoch 1828 / 2000, step 32/63, loss = 0.2689\n",
            "epoch 1828 / 2000, step 48/63, loss = 0.5497\n",
            "epoch 1829 / 2000, step 16/63, loss = 0.5253\n",
            "epoch 1829 / 2000, step 32/63, loss = 0.3308\n",
            "epoch 1829 / 2000, step 48/63, loss = 0.3370\n",
            "epoch 1830 / 2000, step 16/63, loss = 0.4264\n",
            "epoch 1830 / 2000, step 32/63, loss = 0.2808\n",
            "epoch 1830 / 2000, step 48/63, loss = 0.2403\n",
            "epoch 1831 / 2000, step 16/63, loss = 0.4196\n",
            "epoch 1831 / 2000, step 32/63, loss = 0.3730\n",
            "epoch 1831 / 2000, step 48/63, loss = 0.2763\n",
            "epoch 1832 / 2000, step 16/63, loss = 0.3737\n",
            "epoch 1832 / 2000, step 32/63, loss = 0.2012\n",
            "epoch 1832 / 2000, step 48/63, loss = 0.2741\n",
            "epoch 1833 / 2000, step 16/63, loss = 0.3111\n",
            "epoch 1833 / 2000, step 32/63, loss = 0.3594\n",
            "epoch 1833 / 2000, step 48/63, loss = 0.2603\n",
            "epoch 1834 / 2000, step 16/63, loss = 0.2322\n",
            "epoch 1834 / 2000, step 32/63, loss = 0.3075\n",
            "epoch 1834 / 2000, step 48/63, loss = 0.4138\n",
            "epoch 1835 / 2000, step 16/63, loss = 0.4049\n",
            "epoch 1835 / 2000, step 32/63, loss = 0.2341\n",
            "epoch 1835 / 2000, step 48/63, loss = 0.2343\n",
            "epoch 1836 / 2000, step 16/63, loss = 0.4865\n",
            "epoch 1836 / 2000, step 32/63, loss = 0.4321\n",
            "epoch 1836 / 2000, step 48/63, loss = 0.5282\n",
            "epoch 1837 / 2000, step 16/63, loss = 0.3457\n",
            "epoch 1837 / 2000, step 32/63, loss = 0.3899\n",
            "epoch 1837 / 2000, step 48/63, loss = 0.3819\n",
            "epoch 1838 / 2000, step 16/63, loss = 0.2553\n",
            "epoch 1838 / 2000, step 32/63, loss = 0.3155\n",
            "epoch 1838 / 2000, step 48/63, loss = 0.3463\n",
            "epoch 1839 / 2000, step 16/63, loss = 0.2576\n",
            "epoch 1839 / 2000, step 32/63, loss = 0.3476\n",
            "epoch 1839 / 2000, step 48/63, loss = 0.2106\n",
            "epoch 1840 / 2000, step 16/63, loss = 0.2654\n",
            "epoch 1840 / 2000, step 32/63, loss = 0.2828\n",
            "epoch 1840 / 2000, step 48/63, loss = 0.3262\n",
            "epoch 1841 / 2000, step 16/63, loss = 0.6124\n",
            "epoch 1841 / 2000, step 32/63, loss = 0.5621\n",
            "epoch 1841 / 2000, step 48/63, loss = 0.3229\n",
            "epoch 1842 / 2000, step 16/63, loss = 0.3247\n",
            "epoch 1842 / 2000, step 32/63, loss = 0.4451\n",
            "epoch 1842 / 2000, step 48/63, loss = 0.2866\n",
            "epoch 1843 / 2000, step 16/63, loss = 0.2587\n",
            "epoch 1843 / 2000, step 32/63, loss = 0.3564\n",
            "epoch 1843 / 2000, step 48/63, loss = 0.4119\n",
            "epoch 1844 / 2000, step 16/63, loss = 0.3519\n",
            "epoch 1844 / 2000, step 32/63, loss = 0.2789\n",
            "epoch 1844 / 2000, step 48/63, loss = 0.2588\n",
            "epoch 1845 / 2000, step 16/63, loss = 0.3519\n",
            "epoch 1845 / 2000, step 32/63, loss = 0.2980\n",
            "epoch 1845 / 2000, step 48/63, loss = 0.5241\n",
            "epoch 1846 / 2000, step 16/63, loss = 0.2619\n",
            "epoch 1846 / 2000, step 32/63, loss = 0.5484\n",
            "epoch 1846 / 2000, step 48/63, loss = 0.3067\n",
            "epoch 1847 / 2000, step 16/63, loss = 0.2137\n",
            "epoch 1847 / 2000, step 32/63, loss = 0.4611\n",
            "epoch 1847 / 2000, step 48/63, loss = 0.2020\n",
            "epoch 1848 / 2000, step 16/63, loss = 0.6200\n",
            "epoch 1848 / 2000, step 32/63, loss = 0.2556\n",
            "epoch 1848 / 2000, step 48/63, loss = 0.2863\n",
            "epoch 1849 / 2000, step 16/63, loss = 0.4334\n",
            "epoch 1849 / 2000, step 32/63, loss = 0.2940\n",
            "epoch 1849 / 2000, step 48/63, loss = 0.3705\n",
            "epoch 1850 / 2000, step 16/63, loss = 0.3511\n",
            "epoch 1850 / 2000, step 32/63, loss = 0.2380\n",
            "epoch 1850 / 2000, step 48/63, loss = 0.2504\n",
            "epoch 1851 / 2000, step 16/63, loss = 0.4034\n",
            "epoch 1851 / 2000, step 32/63, loss = 0.2397\n",
            "epoch 1851 / 2000, step 48/63, loss = 0.5323\n",
            "epoch 1852 / 2000, step 16/63, loss = 0.3577\n",
            "epoch 1852 / 2000, step 32/63, loss = 0.3424\n",
            "epoch 1852 / 2000, step 48/63, loss = 0.2784\n",
            "epoch 1853 / 2000, step 16/63, loss = 0.2766\n",
            "epoch 1853 / 2000, step 32/63, loss = 0.8966\n",
            "epoch 1853 / 2000, step 48/63, loss = 0.5272\n",
            "epoch 1854 / 2000, step 16/63, loss = 0.2415\n",
            "epoch 1854 / 2000, step 32/63, loss = 0.4529\n",
            "epoch 1854 / 2000, step 48/63, loss = 0.2878\n",
            "epoch 1855 / 2000, step 16/63, loss = 0.2016\n",
            "epoch 1855 / 2000, step 32/63, loss = 0.2882\n",
            "epoch 1855 / 2000, step 48/63, loss = 0.2638\n",
            "epoch 1856 / 2000, step 16/63, loss = 0.4355\n",
            "epoch 1856 / 2000, step 32/63, loss = 0.5479\n",
            "epoch 1856 / 2000, step 48/63, loss = 0.2939\n",
            "epoch 1857 / 2000, step 16/63, loss = 0.2747\n",
            "epoch 1857 / 2000, step 32/63, loss = 0.1841\n",
            "epoch 1857 / 2000, step 48/63, loss = 0.3596\n",
            "epoch 1858 / 2000, step 16/63, loss = 0.3507\n",
            "epoch 1858 / 2000, step 32/63, loss = 0.3030\n",
            "epoch 1858 / 2000, step 48/63, loss = 0.3011\n",
            "epoch 1859 / 2000, step 16/63, loss = 0.3386\n",
            "epoch 1859 / 2000, step 32/63, loss = 0.4750\n",
            "epoch 1859 / 2000, step 48/63, loss = 0.4477\n",
            "epoch 1860 / 2000, step 16/63, loss = 0.1453\n",
            "epoch 1860 / 2000, step 32/63, loss = 0.2790\n",
            "epoch 1860 / 2000, step 48/63, loss = 0.4198\n",
            "epoch 1861 / 2000, step 16/63, loss = 0.2845\n",
            "epoch 1861 / 2000, step 32/63, loss = 0.2806\n",
            "epoch 1861 / 2000, step 48/63, loss = 0.3071\n",
            "epoch 1862 / 2000, step 16/63, loss = 0.5727\n",
            "epoch 1862 / 2000, step 32/63, loss = 0.1430\n",
            "epoch 1862 / 2000, step 48/63, loss = 0.3874\n",
            "epoch 1863 / 2000, step 16/63, loss = 0.3046\n",
            "epoch 1863 / 2000, step 32/63, loss = 0.4022\n",
            "epoch 1863 / 2000, step 48/63, loss = 0.3487\n",
            "epoch 1864 / 2000, step 16/63, loss = 0.2754\n",
            "epoch 1864 / 2000, step 32/63, loss = 0.2776\n",
            "epoch 1864 / 2000, step 48/63, loss = 0.2634\n",
            "epoch 1865 / 2000, step 16/63, loss = 0.2368\n",
            "epoch 1865 / 2000, step 32/63, loss = 0.2043\n",
            "epoch 1865 / 2000, step 48/63, loss = 0.4039\n",
            "epoch 1866 / 2000, step 16/63, loss = 0.3113\n",
            "epoch 1866 / 2000, step 32/63, loss = 0.3045\n",
            "epoch 1866 / 2000, step 48/63, loss = 0.8685\n",
            "epoch 1867 / 2000, step 16/63, loss = 0.3262\n",
            "epoch 1867 / 2000, step 32/63, loss = 0.2056\n",
            "epoch 1867 / 2000, step 48/63, loss = 0.2619\n",
            "epoch 1868 / 2000, step 16/63, loss = 0.4897\n",
            "epoch 1868 / 2000, step 32/63, loss = 0.4817\n",
            "epoch 1868 / 2000, step 48/63, loss = 0.3683\n",
            "epoch 1869 / 2000, step 16/63, loss = 0.5697\n",
            "epoch 1869 / 2000, step 32/63, loss = 0.1915\n",
            "epoch 1869 / 2000, step 48/63, loss = 0.2648\n",
            "epoch 1870 / 2000, step 16/63, loss = 0.4280\n",
            "epoch 1870 / 2000, step 32/63, loss = 0.4154\n",
            "epoch 1870 / 2000, step 48/63, loss = 0.4968\n",
            "epoch 1871 / 2000, step 16/63, loss = 0.2465\n",
            "epoch 1871 / 2000, step 32/63, loss = 0.3354\n",
            "epoch 1871 / 2000, step 48/63, loss = 0.2798\n",
            "epoch 1872 / 2000, step 16/63, loss = 0.5172\n",
            "epoch 1872 / 2000, step 32/63, loss = 0.2504\n",
            "epoch 1872 / 2000, step 48/63, loss = 0.3452\n",
            "epoch 1873 / 2000, step 16/63, loss = 0.2939\n",
            "epoch 1873 / 2000, step 32/63, loss = 0.3019\n",
            "epoch 1873 / 2000, step 48/63, loss = 0.3570\n",
            "epoch 1874 / 2000, step 16/63, loss = 0.4189\n",
            "epoch 1874 / 2000, step 32/63, loss = 0.3049\n",
            "epoch 1874 / 2000, step 48/63, loss = 0.2764\n",
            "epoch 1875 / 2000, step 16/63, loss = 0.4647\n",
            "epoch 1875 / 2000, step 32/63, loss = 0.6059\n",
            "epoch 1875 / 2000, step 48/63, loss = 0.3107\n",
            "epoch 1876 / 2000, step 16/63, loss = 0.3940\n",
            "epoch 1876 / 2000, step 32/63, loss = 0.1988\n",
            "epoch 1876 / 2000, step 48/63, loss = 0.2712\n",
            "epoch 1877 / 2000, step 16/63, loss = 0.4680\n",
            "epoch 1877 / 2000, step 32/63, loss = 0.3076\n",
            "epoch 1877 / 2000, step 48/63, loss = 0.2174\n",
            "epoch 1878 / 2000, step 16/63, loss = 0.3460\n",
            "epoch 1878 / 2000, step 32/63, loss = 0.4041\n",
            "epoch 1878 / 2000, step 48/63, loss = 0.3031\n",
            "epoch 1879 / 2000, step 16/63, loss = 0.1480\n",
            "epoch 1879 / 2000, step 32/63, loss = 0.3124\n",
            "epoch 1879 / 2000, step 48/63, loss = 0.3242\n",
            "epoch 1880 / 2000, step 16/63, loss = 0.2587\n",
            "epoch 1880 / 2000, step 32/63, loss = 0.2188\n",
            "epoch 1880 / 2000, step 48/63, loss = 0.5266\n",
            "epoch 1881 / 2000, step 16/63, loss = 0.3235\n",
            "epoch 1881 / 2000, step 32/63, loss = 0.2382\n",
            "epoch 1881 / 2000, step 48/63, loss = 0.1709\n",
            "epoch 1882 / 2000, step 16/63, loss = 0.3165\n",
            "epoch 1882 / 2000, step 32/63, loss = 0.2763\n",
            "epoch 1882 / 2000, step 48/63, loss = 0.2023\n",
            "epoch 1883 / 2000, step 16/63, loss = 0.3573\n",
            "epoch 1883 / 2000, step 32/63, loss = 0.3084\n",
            "epoch 1883 / 2000, step 48/63, loss = 0.4159\n",
            "epoch 1884 / 2000, step 16/63, loss = 0.2630\n",
            "epoch 1884 / 2000, step 32/63, loss = 0.3913\n",
            "epoch 1884 / 2000, step 48/63, loss = 0.3127\n",
            "epoch 1885 / 2000, step 16/63, loss = 0.3861\n",
            "epoch 1885 / 2000, step 32/63, loss = 0.2563\n",
            "epoch 1885 / 2000, step 48/63, loss = 0.7028\n",
            "epoch 1886 / 2000, step 16/63, loss = 0.2374\n",
            "epoch 1886 / 2000, step 32/63, loss = 0.2793\n",
            "epoch 1886 / 2000, step 48/63, loss = 0.5793\n",
            "epoch 1887 / 2000, step 16/63, loss = 0.2413\n",
            "epoch 1887 / 2000, step 32/63, loss = 0.2034\n",
            "epoch 1887 / 2000, step 48/63, loss = 0.3716\n",
            "epoch 1888 / 2000, step 16/63, loss = 0.2579\n",
            "epoch 1888 / 2000, step 32/63, loss = 0.3966\n",
            "epoch 1888 / 2000, step 48/63, loss = 0.3435\n",
            "epoch 1889 / 2000, step 16/63, loss = 0.3412\n",
            "epoch 1889 / 2000, step 32/63, loss = 0.5926\n",
            "epoch 1889 / 2000, step 48/63, loss = 0.5120\n",
            "epoch 1890 / 2000, step 16/63, loss = 0.3396\n",
            "epoch 1890 / 2000, step 32/63, loss = 0.6708\n",
            "epoch 1890 / 2000, step 48/63, loss = 0.2423\n",
            "epoch 1891 / 2000, step 16/63, loss = 0.2679\n",
            "epoch 1891 / 2000, step 32/63, loss = 0.3077\n",
            "epoch 1891 / 2000, step 48/63, loss = 0.2973\n",
            "epoch 1892 / 2000, step 16/63, loss = 0.5087\n",
            "epoch 1892 / 2000, step 32/63, loss = 0.5319\n",
            "epoch 1892 / 2000, step 48/63, loss = 0.3106\n",
            "epoch 1893 / 2000, step 16/63, loss = 0.2743\n",
            "epoch 1893 / 2000, step 32/63, loss = 0.3775\n",
            "epoch 1893 / 2000, step 48/63, loss = 0.3811\n",
            "epoch 1894 / 2000, step 16/63, loss = 0.3491\n",
            "epoch 1894 / 2000, step 32/63, loss = 0.2441\n",
            "epoch 1894 / 2000, step 48/63, loss = 0.4385\n",
            "epoch 1895 / 2000, step 16/63, loss = 0.2393\n",
            "epoch 1895 / 2000, step 32/63, loss = 0.3202\n",
            "epoch 1895 / 2000, step 48/63, loss = 0.3736\n",
            "epoch 1896 / 2000, step 16/63, loss = 0.1591\n",
            "epoch 1896 / 2000, step 32/63, loss = 0.2558\n",
            "epoch 1896 / 2000, step 48/63, loss = 0.3248\n",
            "epoch 1897 / 2000, step 16/63, loss = 0.6010\n",
            "epoch 1897 / 2000, step 32/63, loss = 0.4565\n",
            "epoch 1897 / 2000, step 48/63, loss = 0.4497\n",
            "epoch 1898 / 2000, step 16/63, loss = 0.3225\n",
            "epoch 1898 / 2000, step 32/63, loss = 0.4925\n",
            "epoch 1898 / 2000, step 48/63, loss = 0.2692\n",
            "epoch 1899 / 2000, step 16/63, loss = 0.6040\n",
            "epoch 1899 / 2000, step 32/63, loss = 0.4053\n",
            "epoch 1899 / 2000, step 48/63, loss = 0.5331\n",
            "epoch 1900 / 2000, step 16/63, loss = 0.3039\n",
            "epoch 1900 / 2000, step 32/63, loss = 0.3186\n",
            "epoch 1900 / 2000, step 48/63, loss = 0.1917\n",
            "epoch 1901 / 2000, step 16/63, loss = 0.4184\n",
            "epoch 1901 / 2000, step 32/63, loss = 0.3690\n",
            "epoch 1901 / 2000, step 48/63, loss = 0.3956\n",
            "epoch 1902 / 2000, step 16/63, loss = 0.3145\n",
            "epoch 1902 / 2000, step 32/63, loss = 0.4033\n",
            "epoch 1902 / 2000, step 48/63, loss = 0.4764\n",
            "epoch 1903 / 2000, step 16/63, loss = 0.4338\n",
            "epoch 1903 / 2000, step 32/63, loss = 0.5303\n",
            "epoch 1903 / 2000, step 48/63, loss = 0.4966\n",
            "epoch 1904 / 2000, step 16/63, loss = 0.3897\n",
            "epoch 1904 / 2000, step 32/63, loss = 0.1447\n",
            "epoch 1904 / 2000, step 48/63, loss = 0.2906\n",
            "epoch 1905 / 2000, step 16/63, loss = 0.3094\n",
            "epoch 1905 / 2000, step 32/63, loss = 0.3595\n",
            "epoch 1905 / 2000, step 48/63, loss = 0.3275\n",
            "epoch 1906 / 2000, step 16/63, loss = 0.3945\n",
            "epoch 1906 / 2000, step 32/63, loss = 0.3911\n",
            "epoch 1906 / 2000, step 48/63, loss = 0.3229\n",
            "epoch 1907 / 2000, step 16/63, loss = 0.5809\n",
            "epoch 1907 / 2000, step 32/63, loss = 0.1645\n",
            "epoch 1907 / 2000, step 48/63, loss = 0.3499\n",
            "epoch 1908 / 2000, step 16/63, loss = 0.7089\n",
            "epoch 1908 / 2000, step 32/63, loss = 0.3932\n",
            "epoch 1908 / 2000, step 48/63, loss = 0.1365\n",
            "epoch 1909 / 2000, step 16/63, loss = 0.2271\n",
            "epoch 1909 / 2000, step 32/63, loss = 0.3685\n",
            "epoch 1909 / 2000, step 48/63, loss = 0.4739\n",
            "epoch 1910 / 2000, step 16/63, loss = 0.1513\n",
            "epoch 1910 / 2000, step 32/63, loss = 0.4792\n",
            "epoch 1910 / 2000, step 48/63, loss = 0.5430\n",
            "epoch 1911 / 2000, step 16/63, loss = 0.4085\n",
            "epoch 1911 / 2000, step 32/63, loss = 0.7266\n",
            "epoch 1911 / 2000, step 48/63, loss = 0.2178\n",
            "epoch 1912 / 2000, step 16/63, loss = 0.4722\n",
            "epoch 1912 / 2000, step 32/63, loss = 0.5064\n",
            "epoch 1912 / 2000, step 48/63, loss = 0.3104\n",
            "epoch 1913 / 2000, step 16/63, loss = 0.2640\n",
            "epoch 1913 / 2000, step 32/63, loss = 0.5805\n",
            "epoch 1913 / 2000, step 48/63, loss = 0.3590\n",
            "epoch 1914 / 2000, step 16/63, loss = 0.3122\n",
            "epoch 1914 / 2000, step 32/63, loss = 0.6990\n",
            "epoch 1914 / 2000, step 48/63, loss = 0.2711\n",
            "epoch 1915 / 2000, step 16/63, loss = 0.2860\n",
            "epoch 1915 / 2000, step 32/63, loss = 0.4346\n",
            "epoch 1915 / 2000, step 48/63, loss = 0.4077\n",
            "epoch 1916 / 2000, step 16/63, loss = 0.4712\n",
            "epoch 1916 / 2000, step 32/63, loss = 0.5545\n",
            "epoch 1916 / 2000, step 48/63, loss = 0.5521\n",
            "epoch 1917 / 2000, step 16/63, loss = 0.4972\n",
            "epoch 1917 / 2000, step 32/63, loss = 0.1941\n",
            "epoch 1917 / 2000, step 48/63, loss = 0.3734\n",
            "epoch 1918 / 2000, step 16/63, loss = 0.3049\n",
            "epoch 1918 / 2000, step 32/63, loss = 0.3730\n",
            "epoch 1918 / 2000, step 48/63, loss = 0.3144\n",
            "epoch 1919 / 2000, step 16/63, loss = 0.5691\n",
            "epoch 1919 / 2000, step 32/63, loss = 0.3332\n",
            "epoch 1919 / 2000, step 48/63, loss = 0.3752\n",
            "epoch 1920 / 2000, step 16/63, loss = 0.5715\n",
            "epoch 1920 / 2000, step 32/63, loss = 0.2335\n",
            "epoch 1920 / 2000, step 48/63, loss = 0.2489\n",
            "epoch 1921 / 2000, step 16/63, loss = 0.5364\n",
            "epoch 1921 / 2000, step 32/63, loss = 0.3518\n",
            "epoch 1921 / 2000, step 48/63, loss = 0.4168\n",
            "epoch 1922 / 2000, step 16/63, loss = 0.4337\n",
            "epoch 1922 / 2000, step 32/63, loss = 0.3213\n",
            "epoch 1922 / 2000, step 48/63, loss = 0.4039\n",
            "epoch 1923 / 2000, step 16/63, loss = 0.3388\n",
            "epoch 1923 / 2000, step 32/63, loss = 0.2237\n",
            "epoch 1923 / 2000, step 48/63, loss = 0.2089\n",
            "epoch 1924 / 2000, step 16/63, loss = 0.3995\n",
            "epoch 1924 / 2000, step 32/63, loss = 0.2401\n",
            "epoch 1924 / 2000, step 48/63, loss = 0.3652\n",
            "epoch 1925 / 2000, step 16/63, loss = 0.1907\n",
            "epoch 1925 / 2000, step 32/63, loss = 0.1699\n",
            "epoch 1925 / 2000, step 48/63, loss = 0.5012\n",
            "epoch 1926 / 2000, step 16/63, loss = 0.5821\n",
            "epoch 1926 / 2000, step 32/63, loss = 0.3664\n",
            "epoch 1926 / 2000, step 48/63, loss = 0.3470\n",
            "epoch 1927 / 2000, step 16/63, loss = 0.1247\n",
            "epoch 1927 / 2000, step 32/63, loss = 0.4740\n",
            "epoch 1927 / 2000, step 48/63, loss = 0.3457\n",
            "epoch 1928 / 2000, step 16/63, loss = 0.3376\n",
            "epoch 1928 / 2000, step 32/63, loss = 0.2337\n",
            "epoch 1928 / 2000, step 48/63, loss = 0.2600\n",
            "epoch 1929 / 2000, step 16/63, loss = 0.2717\n",
            "epoch 1929 / 2000, step 32/63, loss = 0.4586\n",
            "epoch 1929 / 2000, step 48/63, loss = 0.2160\n",
            "epoch 1930 / 2000, step 16/63, loss = 0.5641\n",
            "epoch 1930 / 2000, step 32/63, loss = 0.3015\n",
            "epoch 1930 / 2000, step 48/63, loss = 0.2053\n",
            "epoch 1931 / 2000, step 16/63, loss = 0.3997\n",
            "epoch 1931 / 2000, step 32/63, loss = 0.4489\n",
            "epoch 1931 / 2000, step 48/63, loss = 0.2790\n",
            "epoch 1932 / 2000, step 16/63, loss = 0.5113\n",
            "epoch 1932 / 2000, step 32/63, loss = 0.2586\n",
            "epoch 1932 / 2000, step 48/63, loss = 0.6195\n",
            "epoch 1933 / 2000, step 16/63, loss = 0.3619\n",
            "epoch 1933 / 2000, step 32/63, loss = 0.4279\n",
            "epoch 1933 / 2000, step 48/63, loss = 0.2107\n",
            "epoch 1934 / 2000, step 16/63, loss = 0.2508\n",
            "epoch 1934 / 2000, step 32/63, loss = 0.3061\n",
            "epoch 1934 / 2000, step 48/63, loss = 0.3452\n",
            "epoch 1935 / 2000, step 16/63, loss = 0.2190\n",
            "epoch 1935 / 2000, step 32/63, loss = 0.1452\n",
            "epoch 1935 / 2000, step 48/63, loss = 0.3134\n",
            "epoch 1936 / 2000, step 16/63, loss = 0.2191\n",
            "epoch 1936 / 2000, step 32/63, loss = 0.3927\n",
            "epoch 1936 / 2000, step 48/63, loss = 0.3166\n",
            "epoch 1937 / 2000, step 16/63, loss = 0.3086\n",
            "epoch 1937 / 2000, step 32/63, loss = 0.2501\n",
            "epoch 1937 / 2000, step 48/63, loss = 0.2932\n",
            "epoch 1938 / 2000, step 16/63, loss = 0.2062\n",
            "epoch 1938 / 2000, step 32/63, loss = 0.4036\n",
            "epoch 1938 / 2000, step 48/63, loss = 0.3574\n",
            "epoch 1939 / 2000, step 16/63, loss = 0.2332\n",
            "epoch 1939 / 2000, step 32/63, loss = 0.2908\n",
            "epoch 1939 / 2000, step 48/63, loss = 0.3240\n",
            "epoch 1940 / 2000, step 16/63, loss = 0.3789\n",
            "epoch 1940 / 2000, step 32/63, loss = 0.4015\n",
            "epoch 1940 / 2000, step 48/63, loss = 0.6525\n",
            "epoch 1941 / 2000, step 16/63, loss = 0.4764\n",
            "epoch 1941 / 2000, step 32/63, loss = 0.3635\n",
            "epoch 1941 / 2000, step 48/63, loss = 0.3928\n",
            "epoch 1942 / 2000, step 16/63, loss = 0.2564\n",
            "epoch 1942 / 2000, step 32/63, loss = 0.5617\n",
            "epoch 1942 / 2000, step 48/63, loss = 0.3472\n",
            "epoch 1943 / 2000, step 16/63, loss = 0.3177\n",
            "epoch 1943 / 2000, step 32/63, loss = 0.2636\n",
            "epoch 1943 / 2000, step 48/63, loss = 0.4351\n",
            "epoch 1944 / 2000, step 16/63, loss = 0.3109\n",
            "epoch 1944 / 2000, step 32/63, loss = 0.2454\n",
            "epoch 1944 / 2000, step 48/63, loss = 0.3980\n",
            "epoch 1945 / 2000, step 16/63, loss = 0.3553\n",
            "epoch 1945 / 2000, step 32/63, loss = 0.2451\n",
            "epoch 1945 / 2000, step 48/63, loss = 0.2934\n",
            "epoch 1946 / 2000, step 16/63, loss = 0.3077\n",
            "epoch 1946 / 2000, step 32/63, loss = 0.3203\n",
            "epoch 1946 / 2000, step 48/63, loss = 0.2021\n",
            "epoch 1947 / 2000, step 16/63, loss = 0.2634\n",
            "epoch 1947 / 2000, step 32/63, loss = 0.4396\n",
            "epoch 1947 / 2000, step 48/63, loss = 0.6768\n",
            "epoch 1948 / 2000, step 16/63, loss = 0.4667\n",
            "epoch 1948 / 2000, step 32/63, loss = 0.1511\n",
            "epoch 1948 / 2000, step 48/63, loss = 0.5188\n",
            "epoch 1949 / 2000, step 16/63, loss = 0.2859\n",
            "epoch 1949 / 2000, step 32/63, loss = 0.3559\n",
            "epoch 1949 / 2000, step 48/63, loss = 0.2948\n",
            "epoch 1950 / 2000, step 16/63, loss = 0.4128\n",
            "epoch 1950 / 2000, step 32/63, loss = 0.3099\n",
            "epoch 1950 / 2000, step 48/63, loss = 0.3746\n",
            "epoch 1951 / 2000, step 16/63, loss = 0.4777\n",
            "epoch 1951 / 2000, step 32/63, loss = 0.3439\n",
            "epoch 1951 / 2000, step 48/63, loss = 0.2319\n",
            "epoch 1952 / 2000, step 16/63, loss = 0.3215\n",
            "epoch 1952 / 2000, step 32/63, loss = 0.3180\n",
            "epoch 1952 / 2000, step 48/63, loss = 0.1252\n",
            "epoch 1953 / 2000, step 16/63, loss = 0.2597\n",
            "epoch 1953 / 2000, step 32/63, loss = 0.7032\n",
            "epoch 1953 / 2000, step 48/63, loss = 0.3082\n",
            "epoch 1954 / 2000, step 16/63, loss = 0.4653\n",
            "epoch 1954 / 2000, step 32/63, loss = 0.3343\n",
            "epoch 1954 / 2000, step 48/63, loss = 0.3319\n",
            "epoch 1955 / 2000, step 16/63, loss = 0.2121\n",
            "epoch 1955 / 2000, step 32/63, loss = 0.3567\n",
            "epoch 1955 / 2000, step 48/63, loss = 0.2662\n",
            "epoch 1956 / 2000, step 16/63, loss = 0.3729\n",
            "epoch 1956 / 2000, step 32/63, loss = 0.2771\n",
            "epoch 1956 / 2000, step 48/63, loss = 0.4284\n",
            "epoch 1957 / 2000, step 16/63, loss = 0.5358\n",
            "epoch 1957 / 2000, step 32/63, loss = 0.3067\n",
            "epoch 1957 / 2000, step 48/63, loss = 0.5065\n",
            "epoch 1958 / 2000, step 16/63, loss = 0.2183\n",
            "epoch 1958 / 2000, step 32/63, loss = 0.2624\n",
            "epoch 1958 / 2000, step 48/63, loss = 0.3399\n",
            "epoch 1959 / 2000, step 16/63, loss = 0.1797\n",
            "epoch 1959 / 2000, step 32/63, loss = 0.4226\n",
            "epoch 1959 / 2000, step 48/63, loss = 0.6038\n",
            "epoch 1960 / 2000, step 16/63, loss = 0.5158\n",
            "epoch 1960 / 2000, step 32/63, loss = 0.4809\n",
            "epoch 1960 / 2000, step 48/63, loss = 0.2330\n",
            "epoch 1961 / 2000, step 16/63, loss = 0.3583\n",
            "epoch 1961 / 2000, step 32/63, loss = 0.1963\n",
            "epoch 1961 / 2000, step 48/63, loss = 0.2405\n",
            "epoch 1962 / 2000, step 16/63, loss = 0.5437\n",
            "epoch 1962 / 2000, step 32/63, loss = 0.2562\n",
            "epoch 1962 / 2000, step 48/63, loss = 0.5605\n",
            "epoch 1963 / 2000, step 16/63, loss = 0.3155\n",
            "epoch 1963 / 2000, step 32/63, loss = 0.3272\n",
            "epoch 1963 / 2000, step 48/63, loss = 0.2100\n",
            "epoch 1964 / 2000, step 16/63, loss = 0.1919\n",
            "epoch 1964 / 2000, step 32/63, loss = 0.3313\n",
            "epoch 1964 / 2000, step 48/63, loss = 0.2867\n",
            "epoch 1965 / 2000, step 16/63, loss = 0.3501\n",
            "epoch 1965 / 2000, step 32/63, loss = 0.6028\n",
            "epoch 1965 / 2000, step 48/63, loss = 0.4429\n",
            "epoch 1966 / 2000, step 16/63, loss = 0.3117\n",
            "epoch 1966 / 2000, step 32/63, loss = 0.3467\n",
            "epoch 1966 / 2000, step 48/63, loss = 0.3005\n",
            "epoch 1967 / 2000, step 16/63, loss = 0.4157\n",
            "epoch 1967 / 2000, step 32/63, loss = 0.2413\n",
            "epoch 1967 / 2000, step 48/63, loss = 0.6121\n",
            "epoch 1968 / 2000, step 16/63, loss = 0.3391\n",
            "epoch 1968 / 2000, step 32/63, loss = 0.3261\n",
            "epoch 1968 / 2000, step 48/63, loss = 0.3400\n",
            "epoch 1969 / 2000, step 16/63, loss = 0.4372\n",
            "epoch 1969 / 2000, step 32/63, loss = 0.4092\n",
            "epoch 1969 / 2000, step 48/63, loss = 0.3663\n",
            "epoch 1970 / 2000, step 16/63, loss = 0.3403\n",
            "epoch 1970 / 2000, step 32/63, loss = 0.3461\n",
            "epoch 1970 / 2000, step 48/63, loss = 0.4592\n",
            "epoch 1971 / 2000, step 16/63, loss = 0.3592\n",
            "epoch 1971 / 2000, step 32/63, loss = 0.3108\n",
            "epoch 1971 / 2000, step 48/63, loss = 0.2160\n",
            "epoch 1972 / 2000, step 16/63, loss = 0.4060\n",
            "epoch 1972 / 2000, step 32/63, loss = 0.1934\n",
            "epoch 1972 / 2000, step 48/63, loss = 0.2436\n",
            "epoch 1973 / 2000, step 16/63, loss = 0.1959\n",
            "epoch 1973 / 2000, step 32/63, loss = 0.3193\n",
            "epoch 1973 / 2000, step 48/63, loss = 0.2211\n",
            "epoch 1974 / 2000, step 16/63, loss = 0.2304\n",
            "epoch 1974 / 2000, step 32/63, loss = 0.3727\n",
            "epoch 1974 / 2000, step 48/63, loss = 0.2411\n",
            "epoch 1975 / 2000, step 16/63, loss = 0.3459\n",
            "epoch 1975 / 2000, step 32/63, loss = 0.2203\n",
            "epoch 1975 / 2000, step 48/63, loss = 0.2095\n",
            "epoch 1976 / 2000, step 16/63, loss = 0.3440\n",
            "epoch 1976 / 2000, step 32/63, loss = 0.3144\n",
            "epoch 1976 / 2000, step 48/63, loss = 0.1965\n",
            "epoch 1977 / 2000, step 16/63, loss = 0.5107\n",
            "epoch 1977 / 2000, step 32/63, loss = 0.4144\n",
            "epoch 1977 / 2000, step 48/63, loss = 0.2126\n",
            "epoch 1978 / 2000, step 16/63, loss = 0.2595\n",
            "epoch 1978 / 2000, step 32/63, loss = 0.5946\n",
            "epoch 1978 / 2000, step 48/63, loss = 0.2763\n",
            "epoch 1979 / 2000, step 16/63, loss = 0.1841\n",
            "epoch 1979 / 2000, step 32/63, loss = 0.2676\n",
            "epoch 1979 / 2000, step 48/63, loss = 0.3322\n",
            "epoch 1980 / 2000, step 16/63, loss = 0.2949\n",
            "epoch 1980 / 2000, step 32/63, loss = 0.4658\n",
            "epoch 1980 / 2000, step 48/63, loss = 0.1567\n",
            "epoch 1981 / 2000, step 16/63, loss = 0.3924\n",
            "epoch 1981 / 2000, step 32/63, loss = 0.3058\n",
            "epoch 1981 / 2000, step 48/63, loss = 0.3615\n",
            "epoch 1982 / 2000, step 16/63, loss = 0.3619\n",
            "epoch 1982 / 2000, step 32/63, loss = 0.4455\n",
            "epoch 1982 / 2000, step 48/63, loss = 0.1199\n",
            "epoch 1983 / 2000, step 16/63, loss = 0.4199\n",
            "epoch 1983 / 2000, step 32/63, loss = 0.5694\n",
            "epoch 1983 / 2000, step 48/63, loss = 0.1962\n",
            "epoch 1984 / 2000, step 16/63, loss = 0.4024\n",
            "epoch 1984 / 2000, step 32/63, loss = 0.2301\n",
            "epoch 1984 / 2000, step 48/63, loss = 0.1411\n",
            "epoch 1985 / 2000, step 16/63, loss = 0.2824\n",
            "epoch 1985 / 2000, step 32/63, loss = 0.3311\n",
            "epoch 1985 / 2000, step 48/63, loss = 0.4872\n",
            "epoch 1986 / 2000, step 16/63, loss = 0.2514\n",
            "epoch 1986 / 2000, step 32/63, loss = 0.1509\n",
            "epoch 1986 / 2000, step 48/63, loss = 0.2485\n",
            "epoch 1987 / 2000, step 16/63, loss = 0.2211\n",
            "epoch 1987 / 2000, step 32/63, loss = 0.3221\n",
            "epoch 1987 / 2000, step 48/63, loss = 0.3204\n",
            "epoch 1988 / 2000, step 16/63, loss = 0.1470\n",
            "epoch 1988 / 2000, step 32/63, loss = 0.1476\n",
            "epoch 1988 / 2000, step 48/63, loss = 0.3850\n",
            "epoch 1989 / 2000, step 16/63, loss = 0.2848\n",
            "epoch 1989 / 2000, step 32/63, loss = 0.5180\n",
            "epoch 1989 / 2000, step 48/63, loss = 0.5566\n",
            "epoch 1990 / 2000, step 16/63, loss = 0.4519\n",
            "epoch 1990 / 2000, step 32/63, loss = 0.2539\n",
            "epoch 1990 / 2000, step 48/63, loss = 0.3027\n",
            "epoch 1991 / 2000, step 16/63, loss = 0.7226\n",
            "epoch 1991 / 2000, step 32/63, loss = 0.3464\n",
            "epoch 1991 / 2000, step 48/63, loss = 0.3781\n",
            "epoch 1992 / 2000, step 16/63, loss = 0.4154\n",
            "epoch 1992 / 2000, step 32/63, loss = 0.2443\n",
            "epoch 1992 / 2000, step 48/63, loss = 0.5376\n",
            "epoch 1993 / 2000, step 16/63, loss = 0.4590\n",
            "epoch 1993 / 2000, step 32/63, loss = 0.2191\n",
            "epoch 1993 / 2000, step 48/63, loss = 0.4512\n",
            "epoch 1994 / 2000, step 16/63, loss = 0.3097\n",
            "epoch 1994 / 2000, step 32/63, loss = 0.2599\n",
            "epoch 1994 / 2000, step 48/63, loss = 0.3500\n",
            "epoch 1995 / 2000, step 16/63, loss = 0.3874\n",
            "epoch 1995 / 2000, step 32/63, loss = 0.2511\n",
            "epoch 1995 / 2000, step 48/63, loss = 0.2702\n",
            "epoch 1996 / 2000, step 16/63, loss = 0.4366\n",
            "epoch 1996 / 2000, step 32/63, loss = 0.4564\n",
            "epoch 1996 / 2000, step 48/63, loss = 0.1933\n",
            "epoch 1997 / 2000, step 16/63, loss = 0.2505\n",
            "epoch 1997 / 2000, step 32/63, loss = 0.2700\n",
            "epoch 1997 / 2000, step 48/63, loss = 0.3361\n",
            "epoch 1998 / 2000, step 16/63, loss = 0.2620\n",
            "epoch 1998 / 2000, step 32/63, loss = 0.2331\n",
            "epoch 1998 / 2000, step 48/63, loss = 0.5022\n",
            "epoch 1999 / 2000, step 16/63, loss = 0.3062\n",
            "epoch 1999 / 2000, step 32/63, loss = 0.3850\n",
            "epoch 1999 / 2000, step 48/63, loss = 0.4395\n",
            "epoch 2000 / 2000, step 16/63, loss = 0.3042\n",
            "epoch 2000 / 2000, step 32/63, loss = 0.3100\n",
            "epoch 2000 / 2000, step 48/63, loss = 0.3067\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  for i, (features, labels) in enumerate(train_loader):\n",
        "    features = features.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    #forward path\n",
        "    outputs = model(features) # Подаем признаки на вход нейросети\n",
        "    loss = L(outputs, labels)\n",
        "\n",
        "    #backward path\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      running_loss += loss.item()\n",
        "    if (i+1) % 16 == 0:\n",
        "      print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "  history[\"Train loss\"].append(running_loss / n_total_steps)\n",
        "  history[f\"Train {NN_ver}\"].append(i+1)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBpHSfuK3GWL",
        "outputId": "e972da92-5c23-4f6b-c44c-5a8132d0fdc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.6146, -0.2288, -1.1286],\n",
              "         [-3.5769, -1.3647,  3.7695],\n",
              "         [-1.8111, -1.6287,  2.5756],\n",
              "         [-2.0069,  2.0943, -0.9092],\n",
              "         [-1.5287,  2.2253, -1.2406],\n",
              "         [ 0.5029,  1.4524, -2.7506],\n",
              "         [-0.3118,  1.0272, -1.5921],\n",
              "         [-1.1663,  1.3441, -0.2815],\n",
              "         [-2.9111, -2.8622,  5.4149],\n",
              "         [-1.2543,  1.4602, -0.7382],\n",
              "         [-0.5493,  0.6219, -0.4637],\n",
              "         [-2.7356, -1.9694,  3.9402],\n",
              "         [-1.3170, -0.3812,  0.8545],\n",
              "         [ 0.5691, -0.4759, -0.4230],\n",
              "         [ 1.9140, -1.1405, -1.3453],\n",
              "         [-1.8372,  1.3235, -0.2582],\n",
              "         [ 1.2663,  0.1964, -1.7387],\n",
              "         [ 1.2049, -1.7441,  0.2769],\n",
              "         [ 0.5073, -2.1937,  2.0266],\n",
              "         [-0.6939,  0.9788, -1.0202]], grad_fn=<AddmmBackward0>),\n",
              " tensor([0, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ],
      "source": [
        "outputs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga6qUKpY04F1"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff5AdQL005sM",
        "outputId": "d884187f-a900-4b47-d44b-7fafc112d604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.3111,  2.0096,  1.5338],\n",
            "        [-0.8801,  1.9278, -2.0527],\n",
            "        [-3.1167,  1.4564,  0.5668],\n",
            "        [ 1.7410, -0.5324, -1.4594],\n",
            "        [ 3.1781, -0.2994, -2.7597],\n",
            "        [ 2.1277, -0.3406, -1.8258],\n",
            "        [ 1.8288, -0.6046, -0.5174],\n",
            "        [ 3.1815, -1.5787, -1.4026],\n",
            "        [-3.8541,  2.7347,  0.1575],\n",
            "        [ 0.4417,  0.5555, -1.4621],\n",
            "        [-3.9400,  1.9501,  1.1540],\n",
            "        [ 0.9050,  0.1836, -1.6920],\n",
            "        [ 0.3646,  0.2019, -0.8639],\n",
            "        [-1.5123,  1.1950, -0.6957],\n",
            "        [ 1.5050, -0.2874, -1.4704],\n",
            "        [-1.3858,  1.4421, -0.6480],\n",
            "        [ 1.2564, -0.6821, -1.5043],\n",
            "        [ 1.5029, -0.7531, -0.6043],\n",
            "        [ 1.0108, -0.0637, -1.3739],\n",
            "        [-2.1623,  0.3707,  1.1837]])\n",
            "tensor([[-1.6277,  1.5558, -0.4897],\n",
            "        [-2.9205,  2.7112, -0.3972],\n",
            "        [-2.3750,  0.6563,  0.6802],\n",
            "        [ 2.6478, -0.5686, -2.4182],\n",
            "        [-0.8417, -1.5666,  1.4812],\n",
            "        [-2.2685,  1.4116,  0.3568],\n",
            "        [ 2.4108,  0.6535, -3.1386],\n",
            "        [-0.2024,  0.5257, -0.0658],\n",
            "        [ 0.5453,  0.3409, -1.9252],\n",
            "        [ 0.1273, -1.8219,  1.4335],\n",
            "        [ 1.2671,  0.2624, -1.7096],\n",
            "        [ 2.3327, -0.3085, -1.9090],\n",
            "        [-2.6930,  2.3653, -0.1232],\n",
            "        [ 2.5266, -1.5730, -1.3469],\n",
            "        [-1.4903,  1.8675, -1.4715],\n",
            "        [-2.5855,  1.0490,  0.3903],\n",
            "        [-1.9736,  0.6800,  0.5853],\n",
            "        [-0.1679, -0.7735,  0.2175],\n",
            "        [-1.8383,  1.5613, -0.6313],\n",
            "        [-1.4273,  1.4894, -0.6526]])\n",
            "tensor([[-2.8242,  1.8811,  0.0047],\n",
            "        [ 1.8959,  0.0138, -2.5963],\n",
            "        [-4.0093,  2.0906,  0.4533],\n",
            "        [ 3.1709, -0.6580, -2.3458],\n",
            "        [-1.2613, -0.5557,  1.5693],\n",
            "        [-2.2186,  0.7136,  0.5916],\n",
            "        [ 0.3569, -1.2838,  0.2516],\n",
            "        [-3.8557,  2.0419,  0.5747],\n",
            "        [ 0.1676,  0.0530, -0.4417],\n",
            "        [-2.2692,  1.8040, -0.3891],\n",
            "        [ 1.8539, -0.7003, -2.3101],\n",
            "        [ 3.2542, -0.6633, -2.9201],\n",
            "        [ 0.3455, -0.1048, -0.4546],\n",
            "        [ 0.0044,  0.3155, -1.1567],\n",
            "        [ 2.8079, -0.7252, -2.1523],\n",
            "        [-3.1876,  0.9861,  1.6413],\n",
            "        [ 3.7176, -1.5771, -1.7514],\n",
            "        [-1.1947,  1.9073, -1.4245],\n",
            "        [ 1.8052, -1.0530, -0.2370],\n",
            "        [-3.1420,  2.0937,  0.0416]])\n",
            "tensor([[-0.1048, -0.4955, -0.1953],\n",
            "        [ 0.5264, -0.7158, -0.6668],\n",
            "        [ 0.6662, -0.9624, -0.1592],\n",
            "        [-3.4843,  1.6898,  0.9896],\n",
            "        [ 0.7239,  0.0931, -1.3840],\n",
            "        [-2.0155,  1.1231,  0.4303],\n",
            "        [ 1.3020,  0.6311, -2.4603],\n",
            "        [-4.0211,  2.0853,  0.7800],\n",
            "        [-0.7572,  1.3801, -0.4769],\n",
            "        [ 1.2661, -0.1002, -1.7595],\n",
            "        [-3.4278,  2.0003,  0.5750],\n",
            "        [ 2.8393, -0.0544, -2.3868],\n",
            "        [ 1.0041, -0.0343, -1.1605],\n",
            "        [ 0.7947,  0.5133, -1.5407],\n",
            "        [-2.2880,  1.5829, -0.1928],\n",
            "        [ 0.1328,  0.2084, -0.8453],\n",
            "        [-2.3366,  1.0461,  0.7309],\n",
            "        [ 1.1785, -0.7815, -0.3655],\n",
            "        [ 1.0518, -0.4537, -0.8408],\n",
            "        [ 0.8024,  0.1012, -1.2942]])\n",
            "tensor([[-3.2590e+00,  1.9626e+00, -7.9078e-04],\n",
            "        [-2.7901e+00,  2.0821e+00,  1.3666e-01],\n",
            "        [ 2.1151e+00, -5.2485e-01, -2.4236e+00],\n",
            "        [ 3.6163e-01, -1.1075e+00,  9.2454e-01],\n",
            "        [ 1.4087e+00,  4.1735e-01, -1.9265e+00],\n",
            "        [-4.1009e-01,  1.8784e-01, -3.9166e-01],\n",
            "        [ 2.1421e+00, -1.0189e+00, -1.4532e+00],\n",
            "        [ 1.6956e+00, -1.8613e+00, -1.7950e-01],\n",
            "        [ 9.1399e-01,  3.0204e-01, -1.4318e+00],\n",
            "        [ 9.2015e-01, -1.3465e+00,  4.6519e-01],\n",
            "        [-4.1767e+00,  2.4721e+00,  1.0343e+00],\n",
            "        [-2.4589e-01,  7.6890e-01, -1.1250e+00],\n",
            "        [ 1.1996e+00,  9.9718e-01, -2.9587e+00],\n",
            "        [-2.4968e+00,  1.6166e+00,  3.5246e-02],\n",
            "        [-6.8446e-01, -2.4752e-01,  2.8553e-01],\n",
            "        [ 1.6101e+00,  9.6177e-01, -3.0320e+00],\n",
            "        [-2.2902e+00,  1.7298e+00, -2.6884e-01],\n",
            "        [ 3.5650e+00, -1.3815e+00, -2.0358e+00],\n",
            "        [ 1.8406e+00, -1.8440e+00, -2.8955e-01],\n",
            "        [-3.7176e+00,  2.2228e+00,  6.7418e-01]])\n",
            "tensor([[-0.5237, -0.5257,  0.9100],\n",
            "        [ 0.7207,  0.8016, -1.5635],\n",
            "        [-1.3691,  1.5430, -0.9743],\n",
            "        [-2.6088,  3.3298, -1.2717],\n",
            "        [-3.8483,  1.1682,  1.4767],\n",
            "        [ 0.1236, -0.7886, -0.0332],\n",
            "        [ 0.7951, -2.0794,  0.6329],\n",
            "        [ 2.0270, -1.2901, -0.8329],\n",
            "        [-2.5239,  1.2232,  0.7057],\n",
            "        [ 1.3362,  1.4827, -3.4275],\n",
            "        [-2.8641, -1.4672,  2.9581],\n",
            "        [ 0.6347,  0.4779, -1.4253],\n",
            "        [ 1.1352,  0.3644, -1.3742],\n",
            "        [ 0.9368,  0.0241, -1.8043],\n",
            "        [-0.8257, -1.5405,  1.9888],\n",
            "        [ 2.9139, -0.2955, -2.8126],\n",
            "        [ 1.3235, -0.7490, -1.0987],\n",
            "        [ 0.0701, -1.2783,  0.3750],\n",
            "        [-3.9207,  2.1151,  1.2369],\n",
            "        [ 2.7372, -0.6964, -2.1705]])\n",
            "tensor([[ 2.7553, -1.9103, -0.9104],\n",
            "        [ 2.6656, -1.0570, -1.6531],\n",
            "        [ 0.1916,  0.0290, -0.4263],\n",
            "        [-0.7192,  0.4666, -0.6118],\n",
            "        [-3.1808,  1.4342,  0.6216],\n",
            "        [ 1.2001, -2.3030,  0.4432],\n",
            "        [ 0.5454, -1.5637,  0.7675],\n",
            "        [ 0.1427, -1.3972,  0.9700],\n",
            "        [-3.8259,  1.2955,  1.4954],\n",
            "        [-0.0460,  0.2333, -0.3787],\n",
            "        [-2.5974,  1.1416,  1.0257],\n",
            "        [ 3.3579, -0.8757, -2.9915],\n",
            "        [ 2.0173, -1.2883, -1.4766],\n",
            "        [ 1.7079,  0.3397, -2.4147],\n",
            "        [-1.7116,  0.8345,  0.0209],\n",
            "        [-2.8629,  3.1089, -1.1619],\n",
            "        [ 1.4205, -0.5544, -1.3502],\n",
            "        [ 3.1290, -1.1152, -2.1352],\n",
            "        [-4.0315,  2.5018,  0.5397],\n",
            "        [-2.5301,  0.7034,  1.0401]])\n",
            "tensor([[-3.3287,  1.4495,  0.7946],\n",
            "        [ 2.0985, -0.1522, -2.1787],\n",
            "        [ 2.7651, -0.3872, -2.6925],\n",
            "        [ 2.2693, -0.8253, -2.1534],\n",
            "        [-3.4053,  2.3538, -0.1512],\n",
            "        [-3.6637,  3.6843, -1.2109],\n",
            "        [ 1.7459, -0.6899, -1.2112],\n",
            "        [ 3.7186, -0.8705, -2.2969],\n",
            "        [ 0.2944,  0.8994, -1.3214],\n",
            "        [ 1.0420, -0.6471, -1.0221],\n",
            "        [-1.1800, -0.1553,  0.7710],\n",
            "        [-0.2778, -0.1380,  0.1327],\n",
            "        [-1.3826,  1.0247, -0.3600],\n",
            "        [ 0.1871, -0.3862, -0.3281],\n",
            "        [-1.8623,  1.4068, -0.0842],\n",
            "        [ 0.2834, -0.2215, -0.6028],\n",
            "        [ 1.4613, -1.1570, -0.1203],\n",
            "        [-4.2690,  2.2476,  0.6824],\n",
            "        [ 1.5864,  0.6735, -2.3514],\n",
            "        [-3.5070,  1.6822,  0.9085]])\n",
            "tensor([[ 2.7426, -0.6853, -2.1736],\n",
            "        [-0.1323, -1.0443,  0.2949],\n",
            "        [-2.7623,  2.3425, -0.6794],\n",
            "        [ 2.6187,  0.5539, -3.3233],\n",
            "        [ 1.0407, -0.1235, -1.3690],\n",
            "        [-1.5665,  2.0471, -1.0470],\n",
            "        [ 0.3406,  1.6490, -2.3034],\n",
            "        [-1.1576, -1.1576,  1.3557],\n",
            "        [-3.9575,  2.3028,  0.6745],\n",
            "        [-1.1712, -2.3411,  2.8520],\n",
            "        [-3.5010,  1.2476,  1.3235],\n",
            "        [ 0.9085, -0.3223, -0.6607],\n",
            "        [ 2.2283, -1.5865, -0.7648],\n",
            "        [ 1.2325,  0.1720, -1.4995],\n",
            "        [-4.5982,  1.2541,  2.4585],\n",
            "        [-3.2688,  1.8252,  0.3402],\n",
            "        [ 2.0009, -0.8308, -1.5902],\n",
            "        [ 2.4047,  0.0615, -2.2973],\n",
            "        [ 2.0890,  0.3351, -2.7194],\n",
            "        [ 0.1765,  0.2642, -1.3171]])\n",
            "tensor([[ 0.7260, -0.3299, -0.9110],\n",
            "        [-3.5064,  1.9277,  0.4603],\n",
            "        [ 3.0059, -1.6708, -1.5284],\n",
            "        [ 0.0085,  2.0164, -2.2841],\n",
            "        [ 0.8003,  0.0494, -0.7005],\n",
            "        [ 0.9673, -0.9900, -0.5622],\n",
            "        [-0.3120,  0.1828, -0.6841],\n",
            "        [ 3.5566, -0.6377, -3.0128],\n",
            "        [ 3.4875, -0.4950, -2.5084],\n",
            "        [ 1.2862, -1.3687, -0.2571],\n",
            "        [-4.0196,  2.8963, -0.1773],\n",
            "        [-2.9771,  1.3690,  0.6342],\n",
            "        [-2.0146,  0.9896,  0.7432],\n",
            "        [ 3.0987,  0.2140, -3.2272],\n",
            "        [-2.9642,  1.9669,  0.2140],\n",
            "        [ 1.0775,  0.0864, -1.5581],\n",
            "        [-2.5929,  1.0549,  0.9563],\n",
            "        [-2.7945,  1.5515,  0.3121],\n",
            "        [ 0.6463,  0.5377, -1.5040],\n",
            "        [ 0.0690,  0.9322, -1.2268]])\n",
            "tensor([[ 2.1555, -1.2370, -1.4017],\n",
            "        [ 1.7054, -1.1369, -0.8987],\n",
            "        [-2.5986,  0.0949,  1.8940],\n",
            "        [ 1.2867, -0.9476, -0.3527],\n",
            "        [ 1.0504, -0.8824, -0.6236],\n",
            "        [-3.5288,  1.0184,  2.0761],\n",
            "        [ 1.1184, -1.0273, -0.1752],\n",
            "        [-3.3061,  2.1538, -0.0212],\n",
            "        [-3.5003,  2.0648,  0.3269],\n",
            "        [ 1.8013, -0.5083, -1.9164],\n",
            "        [-4.2156,  1.7772,  1.4256],\n",
            "        [ 1.3562, -1.4508, -0.1289],\n",
            "        [ 2.3156, -0.1423, -2.2217],\n",
            "        [ 1.8158, -1.8450, -0.8013],\n",
            "        [-0.3862, -0.0205,  0.0749],\n",
            "        [ 0.1514,  0.0844, -0.6883],\n",
            "        [ 0.9006,  0.4040, -1.7810],\n",
            "        [ 2.3966,  0.6549, -3.1735],\n",
            "        [-2.8347,  1.1103,  1.0293],\n",
            "        [-4.0073,  1.6104,  1.6949]])\n",
            "tensor([[-0.6208,  0.4689, -0.1922],\n",
            "        [ 2.6139, -0.5804, -1.8724],\n",
            "        [ 1.4220, -0.3628, -1.8063],\n",
            "        [-2.5253,  0.4869,  0.6238],\n",
            "        [ 1.1800, -0.2822, -1.6275],\n",
            "        [ 1.2117, -0.1140, -1.6399],\n",
            "        [-1.5200,  1.6717, -1.0551],\n",
            "        [-2.0125,  1.6465, -0.5017],\n",
            "        [ 2.3612, -2.1810, -0.0876],\n",
            "        [-4.9524,  2.0834,  2.0188],\n",
            "        [-0.3717, -0.3634,  0.1853],\n",
            "        [ 1.6898, -0.8637, -0.4725],\n",
            "        [ 1.1030, -1.0806, -0.5387],\n",
            "        [-2.9004,  1.7373,  0.2076],\n",
            "        [ 0.2257, -0.5331,  0.0908],\n",
            "        [-4.1318,  2.4580,  0.9025],\n",
            "        [ 0.3123, -0.6977, -0.5033],\n",
            "        [-0.1530, -1.5358,  1.2720],\n",
            "        [-0.0439,  0.5993, -1.4689],\n",
            "        [ 0.7223, -0.7826, -0.5657]])\n",
            "tensor([[-4.8256,  2.0878,  1.9079],\n",
            "        [-1.5104, -1.6916,  1.9291],\n",
            "        [-4.0265,  2.0407,  0.7540],\n",
            "        [ 1.5158, -0.0607, -2.2266],\n",
            "        [-1.3883,  0.9057, -0.0535],\n",
            "        [ 1.9421,  0.5264, -2.7830],\n",
            "        [ 1.2909, -0.5426, -0.4637],\n",
            "        [ 1.5948, -0.2829, -1.3450],\n",
            "        [-1.3016,  1.1866, -0.2563],\n",
            "        [-2.2926,  0.8966,  0.0851],\n",
            "        [ 0.8117,  0.5157, -1.2959],\n",
            "        [ 1.5024, -0.3476, -1.1094],\n",
            "        [-0.7725,  1.1362, -0.7345],\n",
            "        [-1.0901,  1.0612, -0.6462],\n",
            "        [ 1.7399, -1.5144,  0.1243],\n",
            "        [ 4.0652, -1.2949, -2.6367],\n",
            "        [ 1.6152,  0.4896, -2.0741],\n",
            "        [ 0.1104, -0.4476, -0.0714],\n",
            "        [ 0.8569, -0.5376, -0.3225],\n",
            "        [ 0.3023,  0.1918, -0.8600]])\n",
            "tensor([[-2.8378,  2.5733, -0.7332],\n",
            "        [-2.8225,  1.7329,  0.5463],\n",
            "        [ 0.4719,  0.3146, -0.9962],\n",
            "        [ 0.9220, -0.9127, -0.4481],\n",
            "        [-0.2428, -0.9253,  0.8701],\n",
            "        [ 1.1599,  0.3126, -2.1232],\n",
            "        [ 0.8953, -0.0541, -1.2034],\n",
            "        [ 0.8425, -1.4722,  0.4356],\n",
            "        [-2.8503,  1.2469,  0.9264],\n",
            "        [ 1.7791, -2.3504,  0.5323],\n",
            "        [-0.6253,  1.7984, -2.0520],\n",
            "        [ 0.1148, -0.2520, -0.5637],\n",
            "        [ 0.4479,  1.0176, -1.8837],\n",
            "        [-1.8905,  1.4348, -0.1259],\n",
            "        [ 2.1456, -1.2494, -1.3204],\n",
            "        [-2.3278,  2.0803, -0.6709],\n",
            "        [ 0.8285,  0.2099, -1.2694],\n",
            "        [-2.1254,  2.0866, -0.9406],\n",
            "        [ 1.2653, -0.8806, -0.6160],\n",
            "        [ 2.1448, -0.0918, -2.0456]])\n",
            "tensor([[-2.3342,  1.4087, -0.0397],\n",
            "        [ 1.1110, -0.3344, -0.5612],\n",
            "        [ 1.9722, -0.4480, -1.9277],\n",
            "        [ 2.2139,  0.0348, -2.6950],\n",
            "        [-1.6820,  1.4987, -0.6354],\n",
            "        [ 0.1559, -2.0307,  1.4699],\n",
            "        [-3.7577,  1.8086,  0.8952],\n",
            "        [ 0.6861, -0.3859, -0.5548],\n",
            "        [-1.0690,  0.3408,  0.3651],\n",
            "        [ 2.9825, -0.9442, -1.6738],\n",
            "        [ 1.7809, -1.1215, -0.8668],\n",
            "        [-3.4733,  3.0512, -0.5276],\n",
            "        [-2.5957,  1.6641,  0.2165],\n",
            "        [-2.7695,  1.6099,  0.6490],\n",
            "        [ 0.1043, -1.0581,  0.6642],\n",
            "        [ 0.7691, -0.9248, -0.3370],\n",
            "        [-3.6192,  1.4002,  0.8210],\n",
            "        [ 0.4472, -0.5181, -0.3687],\n",
            "        [ 1.2591, -1.4653, -0.2634],\n",
            "        [-1.5700,  1.9086, -0.9116]])\n",
            "tensor([[-0.6889,  1.7395, -2.1216],\n",
            "        [-0.1529,  0.3639, -0.7181],\n",
            "        [ 1.3122, -1.1211, -0.3361],\n",
            "        [-2.7503,  1.4664,  0.9584],\n",
            "        [ 2.5216, -0.2342, -2.5368],\n",
            "        [ 1.0477,  0.4732, -1.8968],\n",
            "        [ 1.4771, -1.3693, -0.5586],\n",
            "        [ 1.2500,  0.1653, -1.3252],\n",
            "        [ 1.2816,  0.6113, -2.5534],\n",
            "        [ 1.7109,  0.8482, -3.2997],\n",
            "        [ 1.8591, -0.7200, -1.4050],\n",
            "        [ 2.2583, -0.8046, -2.0525],\n",
            "        [-4.3839,  1.3392,  2.3751],\n",
            "        [-3.7165,  2.1236,  0.6587],\n",
            "        [-3.2755,  1.1428,  1.3247],\n",
            "        [-3.2733,  0.4690,  2.1774],\n",
            "        [ 0.3825,  0.2258, -1.4938],\n",
            "        [ 2.6240, -1.5832, -1.1363],\n",
            "        [ 2.3356,  0.7610, -3.1840],\n",
            "        [-1.9124,  1.3771, -0.3322]])\n",
            "tensor([[-2.8195,  2.6845, -0.9693],\n",
            "        [ 0.6484, -0.9555,  0.2920],\n",
            "        [ 0.3839, -1.0973,  0.9015],\n",
            "        [ 1.0008,  0.5938, -1.3047],\n",
            "        [-1.3563,  1.9533, -1.2705],\n",
            "        [ 0.4876, -0.6673, -0.5230],\n",
            "        [-0.2461, -0.9450,  0.9997],\n",
            "        [ 2.5419, -1.2462, -1.5916],\n",
            "        [-2.8812,  1.4846,  0.4863],\n",
            "        [-1.9320,  1.4023, -0.3704],\n",
            "        [ 2.8983, -1.0858, -1.9050],\n",
            "        [-1.6733,  1.6404, -0.5512],\n",
            "        [-3.6839,  1.0644,  1.6572],\n",
            "        [-3.6814,  1.5954,  0.7878],\n",
            "        [ 2.3129, -0.4692, -2.5231],\n",
            "        [ 2.6512, -0.8141, -2.5698],\n",
            "        [-3.4256,  1.7637,  0.9745],\n",
            "        [-2.3172,  1.4833,  0.1925],\n",
            "        [ 1.5172,  0.6188, -2.0762],\n",
            "        [ 1.2462,  0.7268, -2.5177]])\n",
            "tensor([[-2.4962,  1.3227, -0.0536],\n",
            "        [-1.9730,  1.9601, -0.7794],\n",
            "        [ 0.7783, -0.3754, -0.5920],\n",
            "        [ 1.3881, -0.4962, -1.7212],\n",
            "        [-1.5857, -0.0413,  1.5388],\n",
            "        [-3.1292,  2.1910,  0.0577],\n",
            "        [-2.7825,  2.4322, -0.6567],\n",
            "        [ 1.5072, -0.3754, -1.3674],\n",
            "        [-2.0741,  0.9598,  0.6572],\n",
            "        [ 0.8001,  0.8402, -2.7622],\n",
            "        [ 0.2246, -0.1246, -0.8620],\n",
            "        [ 0.7525,  0.3191, -1.9352],\n",
            "        [ 0.3014, -0.6872, -0.2811],\n",
            "        [ 1.6300, -2.4922,  0.4104],\n",
            "        [ 1.2227, -0.3195, -1.5926],\n",
            "        [-2.4375,  2.2383, -0.9885],\n",
            "        [-2.1230,  1.1090,  0.4261],\n",
            "        [ 1.6748, -1.0260, -1.0091],\n",
            "        [-1.1476,  1.0120, -0.6765],\n",
            "        [ 2.2430, -0.9468, -1.3248]])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  for features, labels in test_loader:\n",
        "    features = features.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(features)\n",
        "    print(outputs)\n",
        "    #value, index\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    n_samples += labels.shape[0]\n",
        "    n_correct += (predictions == labels).sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJhrmwxPV60j"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ledys3Jj-6Or",
        "outputId": "0232eea2-cb38-4d8f-f94f-582dcc2f05ca"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 64.72222222222223 %\n"
          ]
        }
      ],
      "source": [
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'accuracy = {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Графики"
      ],
      "metadata": {
        "id": "12sjEA2hKgIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_history(history):\n",
        "    \"\"\"Plot train history.\n",
        "\n",
        "    Args:\n",
        "        history (dict): Dict of lists with train history.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(17, 9))\n",
        "\n",
        "    ax[0].plot(history['Train loss'], c='r')\n",
        "    #ax[0].plot(history['Val loss'], c='g')\n",
        "    ax[0].set_title('Loss')\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].set_yscale('log')\n",
        "    ax[0].legend(['Train'])\n",
        "\n",
        "    ax[1].plot(history[f'Train {NN_ver}'], c='r')\n",
        "    #ax[1].plot(history[f'Val {NN_ver}'], c='g')\n",
        "    ax[1].set_title(NN_ver)\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[1].set_ylabel(NN_ver)\n",
        "    ax[1].set_yscale('log')\n",
        "    ax[1].legend(['Train'])\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7mpd-xTCKh_V"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_history(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "KegIXkwcSsY8",
        "outputId": "66d92f11-58b5-498f-bce1-f09dc0e7e11a"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1224x648 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'accuracy = {acc} %')\n",
        "print(f'batch size = {batch_size} %')\n",
        "print(f'learning rate = {lr} %')\n",
        "print(f'numer of epochs = {num_epochs} %')\n",
        "print('В свёрточной нейронной сети количество каналов: 20>40>80>160'\n",
        "'структура нейросети: conv1d + relu + batchNormalise1d(1998, 1996, 1994'\n",
        "                      'conv1d + relu + batchNormalise1d(996 и stride=2, 994, 992)'\n",
        "                      'conv1d + relu + batchNormalise1d(495 и stride=2, 493, 491)'\n",
        "                      'Flatten'\n",
        "                      'Linear(491,32)'\n",
        "                      'Dropout(p=0.5)'\n",
        "                      'Sigmoid'\n",
        "                      'Linear(32,3')\n",
        "print(f'В одном файле {num_samples} графиков по {point_number} точек')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWHeJXOyV9Au",
        "outputId": "dd28f1de-e3d8-41c7-8fa8-5486dd90c943"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 64.72222222222223 %\n",
            "batch size = 20 %\n",
            "learning rate = 5e-05 %\n",
            "numer of epochs = 2000 %\n",
            "В свёрточной нейронной сети количество каналов: 20>40>80>160структура нейросети: conv1d + relu + batchNormalise1d(1998, 1996, 1994conv1d + relu + batchNormalise1d(996 и stride=2, 994, 992)conv1d + relu + batchNormalise1d(495 и stride=2, 493, 491)FlattenLinear(491,32)Dropout(p=0.5)SigmoidLinear(32,3\n",
            "В одном файле 20 графиков по 1000 точек\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jHRIGOVWV_tn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MwSUjvtElsBR",
        "P4vWtCezUnKu",
        "QG0rieAzwscd",
        "OsQ4OoSKxrmR",
        "ga6qUKpY04F1"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}